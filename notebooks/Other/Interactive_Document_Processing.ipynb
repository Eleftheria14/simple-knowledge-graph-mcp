{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Document Processing Workflow\n",
    "\n",
    "This notebook provides an improved user experience for the document processing phase of GraphRAG MCP, addressing key friction points:\n",
    "\n",
    "- ‚úÖ **Real-time progress feedback** during processing\n",
    "- ‚úÖ **Incremental processing** - process documents one by one\n",
    "- ‚úÖ **Error handling** with recovery options\n",
    "- ‚úÖ **Interactive workflow** with visual feedback\n",
    "- ‚úÖ **Processing analytics** and performance insights\n",
    "\n",
    "## üéØ What You'll Accomplish\n",
    "\n",
    "Transform your document collection into a searchable knowledge graph with:\n",
    "- Real-time progress tracking\n",
    "- Individual document status monitoring\n",
    "- Error recovery and retry mechanisms\n",
    "- Performance analytics and optimization tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import GraphRAG MCP components\n",
    "from graphrag_mcp.core.analyzer import AdvancedAnalyzer\n",
    "from graphrag_mcp.core.citation_manager import CitationTracker\n",
    "from graphrag_mcp.core.document_processor import DocumentProcessor\n",
    "from graphrag_mcp.core.graphiti_engine import GraphitiKnowledgeGraph\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÅ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration and Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project configuration\n",
    "PROJECT_NAME = \"my-research\"  # Change this to your project name\n",
    "TEMPLATE = \"academic\"\n",
    "DOCUMENTS_FOLDER = \"../examples\"  # Change to your documents folder\n",
    "\n",
    "# Processing configuration\n",
    "MAX_CONCURRENT_DOCS = 3  # Process up to 3 documents simultaneously\n",
    "RETRY_ATTEMPTS = 3\n",
    "TIMEOUT_SECONDS = 300  # 5 minutes per document\n",
    "\n",
    "print(\"üèóÔ∏è  Project Configuration:\")\n",
    "print(f\"   Project Name: {PROJECT_NAME}\")\n",
    "print(f\"   Template: {TEMPLATE}\")\n",
    "print(f\"   Documents Folder: {DOCUMENTS_FOLDER}\")\n",
    "print(f\"   Max Concurrent: {MAX_CONCURRENT_DOCS}\")\n",
    "print(f\"   Retry Attempts: {RETRY_ATTEMPTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Document Discovery and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentStatus:\n",
    "    \"\"\"Track processing status for each document\"\"\"\n",
    "    path: Path\n",
    "    name: str\n",
    "    size_mb: float\n",
    "    status: str = \"pending\"  # pending, processing, completed, failed\n",
    "    start_time: datetime | None = None\n",
    "    end_time: datetime | None = None\n",
    "    error_message: str | None = None\n",
    "    entities_found: int = 0\n",
    "    citations_found: int = 0\n",
    "    processing_time: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def processing_speed(self) -> float:\n",
    "        \"\"\"Pages per minute estimate\"\"\"\n",
    "        if self.processing_time > 0:\n",
    "            return (self.size_mb * 10) / (self.processing_time / 60)  # Rough estimate\n",
    "        return 0.0\n",
    "\n",
    "def discover_documents(folder_path: str) -> list[DocumentStatus]:\n",
    "    \"\"\"Discover and validate PDF documents in the specified folder\"\"\"\n",
    "    documents = []\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    if not folder.exists():\n",
    "        print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "        return documents\n",
    "\n",
    "    print(f\"üîç Scanning folder: {folder_path}\")\n",
    "\n",
    "    for pdf_file in folder.glob(\"**/*.pdf\"):\n",
    "        try:\n",
    "            size_mb = pdf_file.stat().st_size / (1024 * 1024)\n",
    "            doc_status = DocumentStatus(\n",
    "                path=pdf_file,\n",
    "                name=pdf_file.name,\n",
    "                size_mb=round(size_mb, 2)\n",
    "            )\n",
    "            documents.append(doc_status)\n",
    "            print(f\"   üìÑ {pdf_file.name} ({size_mb:.2f} MB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error reading {pdf_file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\nüìä Found {len(documents)} PDF documents\")\n",
    "    print(f\"   Total size: {sum(doc.size_mb for doc in documents):.2f} MB\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Discover documents\n",
    "documents = discover_documents(DOCUMENTS_FOLDER)\n",
    "\n",
    "if not documents:\n",
    "    print(\"\\n‚ùå No documents found. Please check your DOCUMENTS_FOLDER path.\")\n",
    "else:\n",
    "    # Display documents table\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'Document': doc.name,\n",
    "            'Size (MB)': doc.size_mb,\n",
    "            'Status': doc.status,\n",
    "            'Est. Time (min)': round(doc.size_mb * 2, 1)  # Rough estimate\n",
    "        } for doc in documents\n",
    "    ])\n",
    "    print(\"\\nüìã Document Processing Queue:\")\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Initialize Processing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize core components\n",
    "print(\"üîß Initializing processing components...\")\n",
    "\n",
    "try:\n",
    "    # Document processor\n",
    "    doc_processor = DocumentProcessor()\n",
    "    print(\"   ‚úÖ Document Processor initialized\")\n",
    "\n",
    "    # Advanced analyzer\n",
    "    analyzer = AdvancedAnalyzer()\n",
    "    print(\"   ‚úÖ Advanced Analyzer initialized\")\n",
    "\n",
    "    # Graphiti knowledge graph\n",
    "    graphiti_engine = GraphitiKnowledgeGraph()\n",
    "    print(\"   ‚úÖ Graphiti Knowledge Graph initialized\")\n",
    "\n",
    "    # Citation tracker\n",
    "    citation_tracker = CitationTracker()\n",
    "    print(\"   ‚úÖ Citation Tracker initialized\")\n",
    "\n",
    "    print(\"\\nüöÄ All components ready for processing!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Component initialization failed: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"   - Ensure Ollama is running: ollama serve\")\n",
    "    print(\"   - Check Neo4j is running: docker ps\")\n",
    "    print(\"   - Verify models are installed: ollama list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Interactive Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_single_document(doc_status: DocumentStatus) -> bool:\n",
    "    \"\"\"Process a single document with detailed tracking\"\"\"\n",
    "    doc_status.status = \"processing\"\n",
    "    doc_status.start_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Step 1: Basic document processing\n",
    "        print(f\"üìÑ Processing: {doc_status.name}\")\n",
    "        doc_data = doc_processor.process_document(str(doc_status.path))\n",
    "\n",
    "        # Step 2: Advanced analysis\n",
    "        print(f\"üîç Analyzing: {doc_status.name}\")\n",
    "        corpus_doc = analyzer.analyze_for_corpus(str(doc_status.path))\n",
    "\n",
    "        # Step 3: Citation tracking\n",
    "        print(f\"üìö Extracting citations: {doc_status.name}\")\n",
    "        # Add citation tracking logic here\n",
    "\n",
    "        # Step 4: Add to knowledge graph\n",
    "        print(f\"üï∏Ô∏è  Adding to graph: {doc_status.name}\")\n",
    "        success = await graphiti_engine.add_document(\n",
    "            document_content=corpus_doc.content,\n",
    "            document_id=f\"{PROJECT_NAME}_{doc_status.path.stem}\",\n",
    "            metadata={\n",
    "                \"title\": corpus_doc.title,\n",
    "                \"project\": PROJECT_NAME,\n",
    "                \"template\": TEMPLATE,\n",
    "                \"filename\": doc_status.name,\n",
    "                \"entities\": corpus_doc.entities,\n",
    "                \"processing_date\": datetime.now().isoformat(),\n",
    "                **corpus_doc.metadata\n",
    "            },\n",
    "            source_description=f\"{TEMPLATE} document from {PROJECT_NAME} project\"\n",
    "        )\n",
    "\n",
    "        # Update status\n",
    "        doc_status.entities_found = len(corpus_doc.entities)\n",
    "        doc_status.citations_found = len(getattr(corpus_doc, 'citations', []))\n",
    "        doc_status.status = \"completed\" if success else \"failed\"\n",
    "\n",
    "        print(f\"‚úÖ Completed: {doc_status.name} ({doc_status.entities_found} entities, {doc_status.citations_found} citations)\")\n",
    "        return success\n",
    "\n",
    "    except Exception as e:\n",
    "        doc_status.status = \"failed\"\n",
    "        doc_status.error_message = str(e)\n",
    "        print(f\"‚ùå Failed: {doc_status.name} - {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        doc_status.end_time = datetime.now()\n",
    "        if doc_status.start_time:\n",
    "            doc_status.processing_time = (doc_status.end_time - doc_status.start_time).total_seconds()\n",
    "\n",
    "async def process_documents_with_progress(documents: list[DocumentStatus]):\n",
    "    \"\"\"Process documents with real-time progress tracking\"\"\"\n",
    "    if not documents:\n",
    "        print(\"No documents to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üöÄ Starting processing of {len(documents)} documents...\")\n",
    "    print(f\"‚öôÔ∏è  Processing up to {MAX_CONCURRENT_DOCS} documents simultaneously\")\n",
    "\n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(total=len(documents), desc=\"Processing Documents\", unit=\"doc\")\n",
    "\n",
    "    # Process documents with concurrency control\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_DOCS)\n",
    "\n",
    "    async def process_with_semaphore(doc_status):\n",
    "        async with semaphore:\n",
    "            success = await process_single_document(doc_status)\n",
    "            progress_bar.update(1)\n",
    "            return success\n",
    "\n",
    "    # Start processing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process all documents\n",
    "    tasks = [process_with_semaphore(doc) for doc in documents]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Summary\n",
    "    total_time = time.time() - start_time\n",
    "    successful = sum(1 for doc in documents if doc.status == \"completed\")\n",
    "    failed = sum(1 for doc in documents if doc.status == \"failed\")\n",
    "\n",
    "    print(\"\\nüìä Processing Complete:\")\n",
    "    print(f\"   ‚úÖ Successful: {successful}/{len(documents)}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}/{len(documents)}\")\n",
    "    print(f\"   ‚è±Ô∏è  Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"   ‚ö° Average: {total_time/len(documents):.1f} seconds per document\")\n",
    "\n",
    "# Run the processing\n",
    "if documents:\n",
    "    await process_documents_with_progress(documents)\n",
    "else:\n",
    "    print(\"‚ùå No documents to process. Please check the DOCUMENTS_FOLDER path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Processing Analytics and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "if documents:\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Document': doc.name,\n",
    "            'Status': doc.status,\n",
    "            'Size (MB)': doc.size_mb,\n",
    "            'Processing Time (s)': doc.processing_time,\n",
    "            'Entities Found': doc.entities_found,\n",
    "            'Citations Found': doc.citations_found,\n",
    "            'Speed (MB/min)': doc.processing_speed if doc.processing_time > 0 else 0,\n",
    "            'Error': doc.error_message if doc.error_message else ''\n",
    "        } for doc in documents\n",
    "    ])\n",
    "\n",
    "    print(\"üìã Detailed Processing Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Summary statistics\n",
    "    completed_docs = results_df[results_df['Status'] == 'completed']\n",
    "\n",
    "    if not completed_docs.empty:\n",
    "        print(\"\\nüìà Performance Statistics:\")\n",
    "        print(f\"   Average processing time: {completed_docs['Processing Time (s)'].mean():.1f} seconds\")\n",
    "        print(f\"   Fastest document: {completed_docs['Processing Time (s)'].min():.1f} seconds\")\n",
    "        print(f\"   Slowest document: {completed_docs['Processing Time (s)'].max():.1f} seconds\")\n",
    "        print(f\"   Total entities extracted: {completed_docs['Entities Found'].sum()}\")\n",
    "        print(f\"   Total citations found: {completed_docs['Citations Found'].sum()}\")\n",
    "        print(f\"   Average entities per document: {completed_docs['Entities Found'].mean():.1f}\")\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # Processing time by document size\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.scatter(completed_docs['Size (MB)'], completed_docs['Processing Time (s)'], alpha=0.7)\n",
    "        plt.xlabel('Document Size (MB)')\n",
    "        plt.ylabel('Processing Time (seconds)')\n",
    "        plt.title('Processing Time vs Document Size')\n",
    "\n",
    "        # Entities found per document\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.bar(range(len(completed_docs)), completed_docs['Entities Found'])\n",
    "        plt.xlabel('Document Index')\n",
    "        plt.ylabel('Entities Found')\n",
    "        plt.title('Entities Extracted per Document')\n",
    "\n",
    "        # Processing speed distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist(completed_docs['Processing Time (s)'], bins=10, alpha=0.7)\n",
    "        plt.xlabel('Processing Time (seconds)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Processing Time Distribution')\n",
    "\n",
    "        # Status summary\n",
    "        plt.subplot(2, 2, 4)\n",
    "        status_counts = results_df['Status'].value_counts()\n",
    "        plt.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%')\n",
    "        plt.title('Processing Status Summary')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Failed documents analysis\n",
    "    failed_docs = results_df[results_df['Status'] == 'failed']\n",
    "    if not failed_docs.empty:\n",
    "        print(\"\\n‚ùå Failed Documents Analysis:\")\n",
    "        for _, row in failed_docs.iterrows():\n",
    "            print(f\"   üìÑ {row['Document']}: {row['Error']}\")\n",
    "\n",
    "else:\n",
    "    print(\"No processing results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Error Recovery and Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_documents() -> list[DocumentStatus]:\n",
    "    \"\"\"Get list of failed documents for retry\"\"\"\n",
    "    return [doc for doc in documents if doc.status == \"failed\"]\n",
    "\n",
    "def reset_document_for_retry(doc_status: DocumentStatus):\n",
    "    \"\"\"Reset document status for retry\"\"\"\n",
    "    doc_status.status = \"pending\"\n",
    "    doc_status.start_time = None\n",
    "    doc_status.end_time = None\n",
    "    doc_status.error_message = None\n",
    "    doc_status.processing_time = 0.0\n",
    "\n",
    "# Check for failed documents\n",
    "failed_docs = get_failed_documents()\n",
    "\n",
    "if failed_docs:\n",
    "    print(f\"‚ùå Found {len(failed_docs)} failed documents:\")\n",
    "    for doc in failed_docs:\n",
    "        print(f\"   üìÑ {doc.name}: {doc.error_message}\")\n",
    "\n",
    "    # Option to retry failed documents\n",
    "    retry_choice = input(\"\\nüîÑ Would you like to retry failed documents? (y/n): \")\n",
    "\n",
    "    if retry_choice.lower() == 'y':\n",
    "        print(\"\\nüîÑ Retrying failed documents...\")\n",
    "\n",
    "        # Reset failed documents\n",
    "        for doc in failed_docs:\n",
    "            reset_document_for_retry(doc)\n",
    "\n",
    "        # Retry processing\n",
    "        await process_documents_with_progress(failed_docs)\n",
    "\n",
    "        # Show updated results\n",
    "        still_failed = get_failed_documents()\n",
    "        if still_failed:\n",
    "            print(f\"\\n‚ö†Ô∏è  {len(still_failed)} documents still failed after retry.\")\n",
    "            print(\"\\nüîß Troubleshooting suggestions:\")\n",
    "            print(\"   - Check if files are corrupted or password-protected\")\n",
    "            print(\"   - Verify Ollama service is stable\")\n",
    "            print(\"   - Try processing individually with smaller batch sizes\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All documents processed successfully after retry!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ No failed documents to retry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Knowledge Graph Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check knowledge graph status\n",
    "try:\n",
    "    print(\"üï∏Ô∏è  Knowledge Graph Status:\")\n",
    "\n",
    "    # Get basic statistics from Graphiti\n",
    "    # Note: This would need to be implemented in the GraphitiKnowledgeGraph class\n",
    "    print(f\"   üìä Project: {PROJECT_NAME}\")\n",
    "    print(f\"   üìÑ Documents processed: {len([doc for doc in documents if doc.status == 'completed'])}\")\n",
    "    print(f\"   üîó Total entities: {sum(doc.entities_found for doc in documents)}\")\n",
    "    print(f\"   üìö Total citations: {sum(doc.citations_found for doc in documents)}\")\n",
    "\n",
    "    # Knowledge graph readiness\n",
    "    successful_docs = [doc for doc in documents if doc.status == \"completed\"]\n",
    "    if successful_docs:\n",
    "        print(\"\\n‚úÖ Knowledge graph ready for MCP server!\")\n",
    "        print(f\"   üöÄ Next step: graphrag-mcp serve {PROJECT_NAME} --transport stdio\")\n",
    "        print(\"   üîå Connect to Claude Desktop for interactive research\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Knowledge graph not ready - no documents processed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking knowledge graph status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Next Steps:\")\n",
    "print(\"\\n1. üöÄ Start MCP Server:\")\n",
    "print(f\"   graphrag-mcp serve {PROJECT_NAME} --transport stdio\")\n",
    "\n",
    "print(\"\\n2. üîå Connect to Claude Desktop:\")\n",
    "print(\"   Add this to ~/.config/claude-desktop/config.json:\")\n",
    "print('   \"mcpServers\": {\"\"\"')\n",
    "print(f'     \"{PROJECT_NAME}\": {{')\n",
    "print('       \"command\": \"graphrag-mcp\",')\n",
    "print(f'       \"args\": [\"serve\", \"{PROJECT_NAME}\", \"--transport\", \"stdio\"]')\n",
    "print('     }')\n",
    "print('   }\"\"\"')\n",
    "\n",
    "print(\"\\n3. üí¨ Start Research Chat:\")\n",
    "print(\"   Conversational Mode:\")\n",
    "print('   - \"Ask knowledge graph: What are the main themes?\"')\n",
    "print('   - \"Explore topic: machine learning applications\"')\n",
    "print('   - \"Find connections between concepts\"')\n",
    "\n",
    "print(\"\\n   Literature Review Mode:\")\n",
    "print('   - \"Get facts with citations about transformers\"')\n",
    "print('   - \"Verify claim with sources: [your claim]\"')\n",
    "print('   - \"Generate bibliography in APA style\"')\n",
    "\n",
    "print(\"\\n4. üìà Performance Optimization:\")\n",
    "if documents:\n",
    "    avg_time = sum(doc.processing_time for doc in documents if doc.processing_time > 0) / len([doc for doc in documents if doc.processing_time > 0])\n",
    "    if avg_time > 120:  # 2 minutes\n",
    "        print(\"   ‚ö†Ô∏è  Processing time is high. Consider:\")\n",
    "        print(\"     - Reducing document size before processing\")\n",
    "        print(\"     - Using a more powerful GPU/CPU\")\n",
    "        print(\"     - Processing in smaller batches\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Processing performance is good\")\n",
    "\n",
    "print(\"\\n5. üìä Quality Assurance:\")\n",
    "if documents:\n",
    "    avg_entities = sum(doc.entities_found for doc in documents) / len([doc for doc in documents if doc.entities_found > 0])\n",
    "    if avg_entities < 10:\n",
    "        print(\"   ‚ö†Ô∏è  Low entity extraction. Consider:\")\n",
    "        print(\"     - Checking document quality (OCR, formatting)\")\n",
    "        print(\"     - Adjusting entity extraction templates\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Good entity extraction ({avg_entities:.1f} per document)\")\n",
    "\n",
    "print(\"\\nüéâ Your GraphRAG MCP system is ready for interactive research!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}