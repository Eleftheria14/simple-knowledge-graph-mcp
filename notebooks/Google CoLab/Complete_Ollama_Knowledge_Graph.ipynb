{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Complete Ollama + Knowledge Graph System\n",
    "\n",
    "**All-in-one notebook: Ollama setup + Knowledge graph processing**\n",
    "\n",
    "This notebook:\n",
    "- Installs and starts Ollama in Colab\n",
    "- Downloads required models (llama3.1:8b, nomic-embed-text)\n",
    "- Processes one research paper into a knowledge graph\n",
    "- Creates embeddings and vector store\n",
    "- Shows comprehensive results\n",
    "\n",
    "**Requirements:** Enable GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration: Real vs Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration: Choose your data source\n# Set USE_SAMPLE_DATA = True to test with fake data (fast, no PDF needed)\n# Set USE_SAMPLE_DATA = False to process real PDF papers (requires PDF upload)\n\nUSE_SAMPLE_DATA = True  # Change to False for real PDF processing\n\nif USE_SAMPLE_DATA:\n    print(\"üé≠ DEMO MODE: Using sample data\")\n    print(\"   ‚ö° Fast testing without PDF upload\")\n    print(\"   üß™ Pre-extracted entities and content\")\n    print(\"   üöÄ Perfect for testing the knowledge graph system\")\n    print(\"   üìã Still uses Ollama for processing and embeddings\")\n    print(\"\")\n    print(\"üí° To process real PDFs:\")\n    print(\"   1. Set USE_SAMPLE_DATA = False\")\n    print(\"   2. Wait for Ollama setup (10-15 minutes)\")\n    print(\"   3. Upload your own PDF file\")\nelse:\n    print(\"üìÑ REAL DATA MODE: Processing actual PDFs\")\n    print(\"   üìã Full Ollama setup required\")\n    print(\"   üß† Uses LLM for entity extraction\")\n    print(\"   ‚è±Ô∏è Takes 15-20 minutes total (setup + processing)\")\n    print(\"\")\n    print(\"üí° For quick testing:\")\n    print(\"   1. Set USE_SAMPLE_DATA = True\")\n    print(\"   2. Still gets full Ollama + LLM experience\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Google Colab and GPU status\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "    \n",
    "    # Check GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected!\")\n",
    "        print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "        if not USE_SAMPLE_DATA:\n",
    "            print(\"   GPU is REQUIRED for real data processing!\")\n",
    "else:\n",
    "    print(\"üè† Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"üì¶ Installing core dependencies...\")\n    !pip install -q langchain langchain-ollama langchain-chroma\n    !pip install -q chromadb>=0.4.0\n    !pip install -q networkx\n    !pip install -q yfiles_jupyter_graphs\n    \n    if not USE_SAMPLE_DATA:\n        print(\"üì¶ Installing PDF processing dependencies...\")\n        !pip install -q pdfplumber\n    \n    print(\"‚úÖ Dependencies installed!\")\nelse:\n    print(\"üè† Using local environment\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Install Ollama"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"üöÄ Installing Ollama in Colab...\")\n    print(\"‚è±Ô∏è This takes about 2-3 minutes...\")\n    \n    # Download and install Ollama\n    !curl -fsSL https://ollama.ai/install.sh | sh\n    \n    print(\"‚úÖ Ollama installed!\")\n    \nelse:\n    print(\"üè† Assuming local Ollama is running\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Start Ollama Server"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    import subprocess\n    import time\n    import threading\n    import os\n    \n    print(\"üöÄ Starting Ollama server...\")\n    \n    # Function to run Ollama serve in background\n    def run_ollama_serve():\n        os.system(\"ollama serve > /dev/null 2>&1 &\")\n    \n    # Start Ollama in a separate thread\n    ollama_thread = threading.Thread(target=run_ollama_serve, daemon=True)\n    ollama_thread.start()\n    \n    # Wait for server to start\n    print(\"‚è≥ Waiting for server to start...\")\n    time.sleep(10)\n    \n    # Test if server is running\n    try:\n        result = !curl -s http://localhost:11434/api/version\n        if result:\n            print(\"‚úÖ Ollama server is running!\")\n            print(f\"   Version info: {result[0] if result else 'N/A'}\")\n        else:\n            print(\"‚ùå Server not responding\")\n    except:\n        print(\"‚ùå Failed to check server status\")\n        \nelse:\n    print(\"üè† Assuming local Ollama server is running\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Download Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"üì• Downloading models (this takes 5-10 minutes)...\")\n    print(\"‚òï Perfect time for a coffee break!\")\n    print(\"\")\n    \n    # Download LLM model\n    print(\"üß† Downloading llama3.1:8b (main LLM)...\")\n    !ollama pull llama3.1:8b\n    \n    print(\"\")\n    print(\"üî§ Downloading nomic-embed-text (embeddings)...\")\n    !ollama pull nomic-embed-text\n    \n    print(\"\")\n    print(\"‚úÖ All models downloaded and ready!\")\n    \nelse:\n    print(\"üè† Check local models with: ollama list\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Test Ollama Connection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test basic LLM functionality\ntry:\n    from langchain_ollama import ChatOllama\n    \n    print(\"üß™ Testing LLM connection...\")\n    \n    # Create LLM instance\n    llm = ChatOllama(\n        model=\"llama3.1:8b\",\n        temperature=0.1\n    )\n    \n    # Simple test\n    response = llm.invoke(\"Say 'Hello from Colab!' and nothing else.\")\n    print(f\"‚úÖ LLM Response: {response.content}\")\n    \n    # Test embeddings\n    from langchain_ollama import OllamaEmbeddings\n    \n    print(\"üî§ Testing embeddings...\")\n    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n    \n    test_embedding = embeddings.embed_query(\"This is a test.\")\n    print(f\"‚úÖ Embedding created: {len(test_embedding)} dimensions\")\n    \n    print(\"\")\n    print(\"üéâ SUCCESS! Ollama is working perfectly in Colab!\")\n    print(\"üöÄ Ready to process research papers!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Test failed: {e}\")\n    print(\"üí° You may need to restart runtime and try again\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif USE_SAMPLE_DATA:\n    print(\"üé≠ Loading sample paper data...\")\n    \n    # Use built-in sample data (no download needed)\n    SAMPLE_PAPER_DATA = {\n        \"title\": \"Machine Learning for Drug Discovery: A Comprehensive Review\",\n        \"content\": \"\"\"Machine Learning for Drug Discovery: A Comprehensive Review\n\nAuthors: Dr. Sarah Chen (MIT), Prof. Michael Torres (Stanford), Dr. Lisa Wang (UC Berkeley)\n\nAbstract:\nThis comprehensive review examines the application of machine learning techniques to drug discovery processes. \nWe analyze various computational approaches including deep learning, graph neural networks, and transformer \narchitectures for molecular property prediction and drug-target interaction modeling.\n\nMethods:\nWe conducted a systematic review of machine learning applications in drug discovery, focusing on:\n\n1. Molecular Property Prediction\n- Graph Convolutional Networks (GCNs) for molecular representation\n- Transformer models adapted for SMILES sequences\n- Recurrent Neural Networks for sequential molecular data\n\n2. Drug-Target Interaction Prediction\n- Matrix factorization techniques\n- Deep neural networks with protein sequence embeddings\n- Graph-based approaches combining molecular and protein structures\n\nTechnologies and Tools:\n- Deep Learning: TensorFlow, PyTorch, Keras\n- Cheminformatics: RDKit, OpenEye, ChemAxon\n- Graph Processing: DGL, PyTorch Geometric, NetworkX\n\nConclusions:\nMachine learning has fundamentally transformed drug discovery by enabling more efficient exploration of chemical \nand biological space. Future success will depend on continued collaboration between computational scientists, \nmedicinal chemists, and clinical researchers.\"\"\",\n        \"pages\": 12,\n        \"char_count\": 1234\n    }\n    \n    # Use sample data for natural discovery\n    paper_path = \"sample_data\"  # Placeholder\n    paper_title = SAMPLE_PAPER_DATA[\"title\"]\n    text_content = SAMPLE_PAPER_DATA[\"content\"]\n    \n    print(f\"‚úÖ Sample data loaded!\")\n    print(f\"üì∞ Title: {paper_title}\")\n    print(f\"üìä Content length: {len(text_content):,} characters\")\n    print(f\"üìÑ Simulated pages: {SAMPLE_PAPER_DATA['pages']}\")\n    print(f\"üåø Ready for natural knowledge discovery\")\n    \nelif IN_COLAB:\n    print(\"üì§ Choose how to load your PDF:\")\n    print(\"   1Ô∏è‚É£ Upload file using file picker\")\n    print(\"   2Ô∏è‚É£ Use file already in Colab storage\")\n    print(\"\")\n    \n    # Check for existing PDFs in current directory\n    existing_pdfs = [f for f in os.listdir('.') if f.endswith('.pdf')]\n    \n    if existing_pdfs:\n        print(f\"üìÅ Found {len(existing_pdfs)} PDF(s) in current directory:\")\n        for i, pdf in enumerate(existing_pdfs, 1):\n            file_size = os.path.getsize(pdf) / (1024*1024)  # MB\n            print(f\"   {i}. {pdf} ({file_size:.1f} MB)\")\n        print(\"\")\n        \n        choice = input(\"Type filename to use existing PDF, or press Enter to upload new file: \").strip()\n        \n        if choice and choice in existing_pdfs:\n            paper_path = choice\n            print(f\"‚úÖ Using existing file: {paper_path}\")\n        else:\n            print(\"üì§ Upload a new PDF file...\")\n            from google.colab import files\n            uploaded = files.upload()\n            \n            # Get the first PDF\n            paper_path = None\n            for filename in uploaded.keys():\n                if filename.endswith('.pdf'):\n                    paper_path = filename\n                    break\n    else:\n        print(\"üìÅ No existing PDFs found in current directory\")\n        print(\"üì§ Upload a PDF file...\")\n        from google.colab import files\n        uploaded = files.upload()\n        \n        # Get the first PDF\n        paper_path = None\n        for filename in uploaded.keys():\n            if filename.endswith('.pdf'):\n                paper_path = filename\n                break\n    \n    if paper_path:\n        file_size = os.path.getsize(paper_path) / (1024*1024)  # MB\n        print(f\"‚úÖ Paper selected: {paper_path} ({file_size:.1f} MB)\")\n        \n        # Show file details\n        print(f\"üìÅ File location: /content/{paper_path}\")\n        print(f\"üìä File size: {file_size:.1f} MB\")\n    else:\n        print(\"‚ùå No PDF file found! Please upload a PDF.\")\n        \nelse:\n    # Use local example\n    paper_path = '../../examples/d4sc03921a.pdf'\n    if os.path.exists(paper_path):\n        print(f\"‚úÖ Using local paper: {paper_path}\")\n    else:\n        print(f\"‚ùå Local paper not found: {paper_path}\")\n        paper_path = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Extract Text from PDF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAMPLE_DATA:\n",
    "    print(\"üé≠ Using sample text content (already loaded)\")\n",
    "    print(f\"‚úÖ Text content ready!\")\n",
    "    print(f\"üì∞ Title: {paper_title}\")\n",
    "    print(f\"üìä Content length: {len(text_content):,} characters\")\n",
    "    print(f\"üìÑ Sample paper simulates {SAMPLE_PAPER_DATA['pages']} pages\")\n",
    "    \n",
    "elif paper_path:\n",
    "    import pdfplumber\n",
    "    \n",
    "    print(f\"üìÑ Extracting text from: {paper_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract text\n",
    "        with pdfplumber.open(paper_path) as pdf:\n",
    "            text_content = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_content += page_text + \"\\n\\n\"\n",
    "        \n",
    "        # Get paper title (first substantial line)\n",
    "        lines = text_content.split('\\n')\n",
    "        paper_title = \"Unknown Title\"\n",
    "        for line in lines:\n",
    "            if len(line.strip()) > 20 and not line.strip().isdigit():\n",
    "                paper_title = line.strip()[:100]\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÖ Text extracted successfully!\")\n",
    "        print(f\"üì∞ Title: {paper_title}\")\n",
    "        print(f\"üìä Content length: {len(text_content):,} characters\")\n",
    "        print(f\"üìÑ Pages processed: {len(pdf.pages)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to extract text: {e}\")\n",
    "        text_content = None\n",
    "        paper_title = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No paper to process\")\n",
    "    text_content = None\n",
    "    paper_title = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Natural Paper Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_SAMPLE_DATA:\n    print(\"üé≠ Using sample paper content for natural analysis\")\n    print(f\"‚úÖ Sample paper loaded!\")\n    \n    # Use the complete sample paper content directly for analysis\n    paper_content = text_content\n    paper_title_final = paper_title\n    \n    # For demo mode, create a sample natural analysis\n    complete_analysis = \"\"\"This paper provides a comprehensive review of machine learning applications in drug discovery. The research examines how computational approaches, particularly deep learning and graph neural networks, are transforming pharmaceutical research.\n\nThe paper covers three main areas: molecular property prediction using Graph Convolutional Networks and transformer models, drug-target interaction prediction through deep neural networks and matrix factorization, and virtual screening using generative models and reinforcement learning.\n\nKey findings include the effectiveness of graph-based approaches for molecular representation, the importance of transformer architectures for SMILES sequences, and the potential of generative adversarial networks for novel molecule design. The work highlights major datasets like ChEMBL, PubChem, and ZINC, along with important technologies including TensorFlow, PyTorch, and RDKit.\n\nThe research concludes that machine learning has fundamentally transformed drug discovery by enabling more efficient exploration of chemical and biological space, though challenges remain in data quality, model interpretability, and regulatory acceptance.\"\"\"\n    \n    print(f\"üìä Content length: {len(text_content):,} characters\")\n    print(f\"üìù Analysis length: {len(complete_analysis):,} characters\")\n    print(f\"üìÑ Ready for natural knowledge graph creation\")\n    \nelif text_content:\n    from langchain_ollama import ChatOllama\n    from langchain_core.prompts import ChatPromptTemplate\n    \n    print(\"üß† Analyzing complete paper with LLM...\")\n    print(\"‚è±Ô∏è This analyzes the ENTIRE paper content without predefined categories...\")\n    \n    # Create LLM\n    llm = ChatOllama(\n        model=\"llama3.1:8b\",\n        temperature=0.1\n    )\n    \n    # Simple, open-ended analysis prompt\n    prompt_text = '''You are an expert research analyst. Read this COMPLETE research paper and provide a comprehensive, natural analysis.\n\nCOMPLETE PAPER CONTENT:\n{content}\n\nAnalyze this paper thoroughly and naturally. Don't force it into categories - just understand it completely and tell me:\n\n1. What is this paper about?\n2. What are the main ideas, findings, and contributions?\n3. What methods, approaches, or techniques are used?\n4. What's important or interesting about this work?\n5. What are the key concepts, technologies, or data mentioned?\n\nProvide a thorough, natural analysis - not a structured format. Just understand the paper completely and explain it comprehensively.'''\n    \n    prompt = ChatPromptTemplate.from_template(prompt_text)\n    \n    try:\n        # Process paper in chunks if too long, then synthesize\n        max_chars = 25000  # Conservative limit for analysis\n        \n        if len(text_content) > max_chars:\n            print(f\"üìÑ Paper is long ({len(text_content):,} chars), analyzing in sections...\")\n            \n            # Split into logical sections\n            sections = []\n            chunk_size = max_chars\n            \n            for i in range(0, len(text_content), chunk_size):\n                section = text_content[i:i+chunk_size]\n                sections.append(section)\n            \n            print(f\"üîÑ Analyzing {len(sections)} sections...\")\n            \n            section_analyses = []\n            for i, section in enumerate(sections, 1):\n                print(f\"   Analyzing section {i}/{len(sections)}...\")\n                \n                chain = prompt | llm\n                result = chain.invoke({\n                    \"content\": section\n                })\n                \n                section_analyses.append(result.content)\n            \n            # Now synthesize all sections into final analysis\n            if section_analyses:\n                print(\"üîÑ Synthesizing complete paper understanding...\")\n                \n                synthesis_prompt = '''You have analyzed different sections of a research paper. Now synthesize these section analyses into one comprehensive understanding of the complete paper.\n\nSECTION ANALYSES:\n{sections}\n\nProvide a complete, unified analysis of the entire paper. What is this research really about? What are the key insights across the whole work?'''\n                \n                synthesis_chain = ChatPromptTemplate.from_template(synthesis_prompt) | llm\n                synthesis_result = synthesis_chain.invoke({\n                    \"sections\": \"\\n\\n---SECTION---\\n\\n\".join(section_analyses)\n                })\n                \n                complete_analysis = synthesis_result.content\n            else:\n                print(\"‚ùå No section analyses completed\")\n                complete_analysis = None\n                \n        else:\n            print(f\"üìÑ Analyzing complete paper ({len(text_content):,} chars)...\")\n            \n            # Process entire paper at once\n            chain = prompt | llm\n            result = chain.invoke({\n                \"content\": text_content\n            })\n            \n            complete_analysis = result.content\n        \n        if complete_analysis:\n            print(\"‚úÖ Complete paper analysis finished!\")\n            print(f\"\\nüìä PAPER ANALYSIS:\")\n            print(f\"üìÑ Title: {paper_title}\")\n            print(f\"üìù Analysis length: {len(complete_analysis):,} characters\")\n            print(f\"üîç Analysis preview: {complete_analysis[:200]}...\")\n            \n            # Store the results\n            paper_content = text_content\n            paper_title_final = paper_title\n            \n        else:\n            print(\"‚ùå Paper analysis failed\")\n            complete_analysis = None\n            \n    except Exception as e:\n        print(f\"‚ùå Paper analysis failed: {e}\")\n        complete_analysis = None\n        \nelse:\n    print(\"‚ùå No text content to analyze\")\n    complete_analysis = None\n    paper_content = None\n    paper_title_final = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10: Create Embeddings and Vector Store"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if paper_content and complete_analysis:\n    from langchain_ollama import OllamaEmbeddings\n    from langchain_chroma import Chroma\n    from langchain_core.documents import Document\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    \n    print(\"üî§ Creating embeddings and vector store from complete paper...\")\n    print(\"‚è±Ô∏è This takes 2-3 minutes...\")\n    \n    # Create embeddings model\n    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n    \n    # Split text into chunks for embeddings\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    \n    chunks = text_splitter.split_text(paper_content)\n    print(f\"üìÑ Created {len(chunks)} text chunks from complete paper\")\n    \n    # Create documents with metadata from analysis\n    documents = []\n    for i, chunk in enumerate(chunks):\n        metadata = {\n            'paper_title': paper_title_final,\n            'chunk_id': f\"chunk_{i}\",\n            'chunk_index': i,\n            'total_chunks': len(chunks),\n            'analysis_preview': complete_analysis[:500] if complete_analysis else '',\n            'has_analysis': bool(complete_analysis)\n        }\n        \n        doc = Document(page_content=chunk, metadata=metadata)\n        documents.append(doc)\n    \n    # Also add the complete analysis as a document\n    if complete_analysis:\n        analysis_doc = Document(\n            page_content=complete_analysis,\n            metadata={\n                'paper_title': paper_title_final,\n                'chunk_id': 'complete_analysis',\n                'chunk_index': -1,\n                'total_chunks': len(chunks),\n                'is_analysis': True\n            }\n        )\n        documents.append(analysis_doc)\n    \n    # Create vector store\n    persist_directory = \"/tmp/chroma_paper_complete\"\n    \n    print(\"üóÑÔ∏è Creating vector store with ChromaDB...\")\n    vector_store = Chroma(\n        embedding_function=embeddings,\n        persist_directory=persist_directory\n    )\n    \n    # Add documents to vector store\n    document_ids = vector_store.add_documents(documents)\n    \n    print(f\"‚úÖ Vector store created from complete paper!\")\n    print(f\"   üìù {len(documents)} documents added (including analysis)\")\n    print(f\"   üî§ Embeddings created with nomic-embed-text\")\n    print(f\"   üóÑÔ∏è Stored in ChromaDB at {persist_directory}\")\n    \n    # Test semantic search on complete paper\n    print(\"\\nüîç Testing semantic search on complete paper...\")\n    query = \"What are the main findings and contributions?\"\n    results = vector_store.similarity_search(query, k=3)\n    \n    print(f\"Query: '{query}'\")\n    print(f\"Found {len(results)} relevant chunks:\")\n    for i, result in enumerate(results, 1):\n        is_analysis = result.metadata.get('is_analysis', False)\n        content_type = \"LLM Analysis\" if is_analysis else \"Paper Content\"\n        print(f\"  {i}. [{content_type}] {result.page_content[:100]}...\")\n    \nelse:\n    print(\"‚ùå No paper content to process - skipping vector store creation\")\n    vector_store = None\n    documents = []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 11: Create Natural Knowledge Graph"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if paper_content and complete_analysis:\n    from langchain_ollama import ChatOllama\n    from langchain_core.prompts import ChatPromptTemplate\n    import networkx as nx\n    import json\n    \n    print(\"üï∏Ô∏è Creating natural knowledge graph from paper content...\")\n    print(\"‚è±Ô∏è Let the LLM discover natural relationships...\")\n    \n    # Use LLM to discover natural connections in the content\n    llm = ChatOllama(\n        model=\"llama3.1:8b\",\n        temperature=0.1\n    )\n    \n    graph_prompt = '''You are analyzing this research paper to discover natural relationships and connections.\n\nPAPER CONTENT:\n{content}\n\nLLM ANALYSIS:\n{analysis}\n\nLook at this content naturally and identify:\n1. Key concepts, ideas, and topics that emerge from the paper\n2. Natural relationships and connections between these concepts\n3. Important terms, methods, findings that relate to each other\n\nReturn a JSON with nodes and edges that represent the natural structure you see:\n\n{\n  \"nodes\": [\n    {\"id\": \"concept_name\", \"label\": \"Natural concept from paper\", \"importance\": \"high/medium/low\"},\n    ...\n  ],\n  \"edges\": [\n    {\"source\": \"concept1\", \"target\": \"concept2\", \"relationship\": \"natural relationship you observe\"},\n    ...\n  ]\n}\n\nDiscover what's naturally connected in this research - don't force categories. Let the content reveal its own structure.\n\nJSON:'''\n    \n    try:\n        print(\"üîç Discovering natural connections in the paper...\")\n        \n        # Let LLM discover natural graph structure\n        prompt = ChatPromptTemplate.from_template(graph_prompt)\n        chain = prompt | llm\n        result = chain.invoke({\n            \"content\": paper_content[:15000],  # First part of content\n            \"analysis\": complete_analysis[:5000] if complete_analysis else \"\"\n        })\n        \n        # Extract JSON from response\n        response_text = result.content\n        json_start = response_text.find('{')\n        json_end = response_text.rfind('}') + 1\n        \n        if json_start != -1 and json_end != -1:\n            json_str = response_text[json_start:json_end]\n            graph_data = json.loads(json_str)\n            \n            # Create NetworkX graph from discovered structure\n            G = nx.Graph()\n            \n            # Add nodes with natural attributes\n            nodes_added = set()\n            for node in graph_data.get('nodes', []):\n                node_id = node.get('id', '')\n                if node_id and node_id not in nodes_added:\n                    G.add_node(\n                        node_id,\n                        label=node.get('label', node_id),\n                        importance=node.get('importance', 'medium'),\n                        type='natural_concept'\n                    )\n                    nodes_added.add(node_id)\n            \n            # Add edges with natural relationships\n            for edge in graph_data.get('edges', []):\n                source = edge.get('source', '')\n                target = edge.get('target', '')\n                relationship = edge.get('relationship', 'related_to')\n                \n                if source in nodes_added and target in nodes_added:\n                    G.add_edge(source, target, relationship=relationship)\n            \n            print(f\"‚úÖ Natural knowledge graph discovered!\")\n            print(f\"   üîó Nodes: {G.number_of_nodes()}\")\n            print(f\"   üìä Edges: {G.number_of_edges()}\")\n            print(f\"   üåø Structure emerged naturally from content\")\n            \n            # Show discovered concepts\n            print(f\"\\nüåø Naturally discovered concepts:\")\n            for node in G.nodes():\n                importance = G.nodes[node].get('importance', 'medium')\n                label = G.nodes[node].get('label', node)\n                print(f\"   ‚Ä¢ {node}: {label} ({importance} importance)\")\n            \n        else:\n            print(\"‚ùå Could not parse natural graph structure\")\n            print(\"üîÑ Creating simple content-based graph...\")\n            \n            # Fallback: simple content representation\n            G = nx.Graph()\n            G.add_node(paper_title_final or \"Research Paper\", type='paper')\n            G.add_node(\"Paper Content\", type='content')\n            G.add_node(\"LLM Analysis\", type='analysis')\n            G.add_edge(paper_title_final or \"Research Paper\", \"Paper Content\", relationship='contains')\n            G.add_edge(\"Paper Content\", \"LLM Analysis\", relationship='analyzed_to_produce')\n    \n    except Exception as e:\n        print(f\"‚ùå Natural graph discovery failed: {e}\")\n        print(\"üîÑ Creating simple representation...\")\n        \n        # Simple fallback\n        G = nx.Graph()\n        G.add_node(paper_title_final or \"Research Paper\", type='paper')\n        G.add_node(\"Paper Content\", type='content')\n        if complete_analysis:\n            G.add_node(\"LLM Analysis\", type='analysis')\n            G.add_edge(paper_title_final or \"Research Paper\", \"Paper Content\", relationship='contains')\n            G.add_edge(\"Paper Content\", \"LLM Analysis\", relationship='produces')\n    \n    # Store for visualization\n    knowledge_graph = {\n        'graph': G,\n        'paper_content': paper_content,\n        'complete_analysis': complete_analysis,\n        'paper_title': paper_title_final,\n        'stats': {\n            'nodes': G.number_of_nodes(),\n            'edges': G.number_of_edges(),\n            'discovery_method': 'natural_llm_discovery'\n        }\n    }\n    \nelse:\n    print(\"‚ùå No paper content to build graph from\")\n    knowledge_graph = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 12: Interactive Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if knowledge_graph and knowledge_graph['graph'].number_of_nodes() > 0:\n    print(\"üìä Creating interactive yFiles visualization of natural knowledge graph...\")\n    \n    try:\n        from yfiles_jupyter_graphs import GraphWidget\n        import networkx as nx\n        \n        G = knowledge_graph['graph']\n        \n        print(f\"üéÆ Building interactive graph with {G.number_of_nodes()} naturally discovered nodes...\")\n        \n        # Create yFiles widget\n        widget = GraphWidget(graph=G)\n        \n        # Configure node styling based on natural attributes\n        def configure_node_style(node):\n            node_data = G.nodes[node]\n            importance = node_data.get('importance', 'medium')\n            node_type = node_data.get('type', 'natural_concept')\n            \n            # Natural color scheme based on importance and type\n            if node_type == 'paper':\n                color = '#1f4e79'  # Deep blue for main paper\n                size = 50\n            elif node_type == 'analysis':\n                color = '#7b68ee'  # Medium slate blue for analysis\n                size = 40\n            elif importance == 'high':\n                color = '#e74c3c'  # Red for high importance\n                size = 45\n            elif importance == 'medium':\n                color = '#3498db'  # Blue for medium importance\n                size = 35\n            else:  # low importance\n                color = '#95a5a6'  # Gray for low importance\n                size = 25\n            \n            label = node_data.get('label', node)\n            return {\n                'color': color,\n                'size': size,\n                'label': label[:40] + \"...\" if len(label) > 40 else label\n            }\n        \n        # Apply node styling\n        widget.set_node_styles_mapping(configure_node_style)\n        \n        # Configure edge styling\n        def configure_edge_style(edge):\n            return {\n                'color': '#bdc3c7',\n                'thickness': 2,\n                'style': 'solid'\n            }\n        \n        widget.set_edge_styles_mapping(configure_edge_style)\n        \n        # Use organic layout for natural structure\n        widget.set_layout('organic')\n        \n        # Enable overview and navigation\n        widget.overview_enabled = True\n        widget.context_start_with = 'clean-slate'\n        \n        print(\"‚úÖ Interactive natural knowledge graph created!\")\n        print(\"üéÆ Controls:\")\n        print(\"   ‚Ä¢ Drag nodes to rearrange\")\n        print(\"   ‚Ä¢ Zoom with mouse wheel\")\n        print(\"   ‚Ä¢ Click nodes to highlight connections\")\n        print(\"   ‚Ä¢ Use overview panel for navigation\")\n        print(\"\")\n        \n        # Show the widget\n        display(widget)\n        \n        # Show natural relationships discovered\n        print(\"üåø Natural relationships discovered:\")\n        for edge in G.edges(data=True):\n            source, target, data = edge\n            relationship = data.get('relationship', 'connected to')\n            print(f\"   ‚Ä¢ {source} {relationship} {target}\")\n        \n    except ImportError:\n        print(\"‚ùå yfiles_jupyter_graphs not available\")\n        print(\"üí° Install with: pip install yfiles_jupyter_graphs\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error creating yFiles visualization: {e}\")\n        print(\"üìä Falling back to summary\")\n    \n    # Print comprehensive summary\n    print(f\"\\nüìä NATURAL KNOWLEDGE GRAPH SUMMARY:\")\n    print(f\"   üìÑ Paper: {knowledge_graph.get('paper_title', 'Unknown')}\")\n    print(f\"   üåø Discovery method: {knowledge_graph['stats'].get('discovery_method', 'natural')}\")\n    print(f\"   üîó Naturally discovered nodes: {knowledge_graph['stats']['nodes']}\")\n    print(f\"   üìä Natural relationships: {knowledge_graph['stats']['edges']}\")\n    print(f\"   üî§ Vector store documents: {len(documents) if 'documents' in locals() else 0}\")\n    print(f\"   üóÑÔ∏è Vector store: {'‚úÖ Created' if 'vector_store' in locals() and vector_store else '‚ùå Not created'}\")\n    \n    # Show analysis preview\n    analysis = knowledge_graph.get('complete_analysis', '')\n    if analysis:\n        print(f\"\\nüìù LLM ANALYSIS PREVIEW:\")\n        print(f\"   {analysis[:300]}...\")\n    \nelse:\n    print(\"‚ùå No natural knowledge graph to visualize\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# üíæ Save Natural Analysis and Knowledge Graph\n\nif complete_analysis and knowledge_graph:\n    import json\n    import pickle\n    from datetime import datetime\n    \n    print(\"üíæ Saving natural analysis and knowledge graph...\")\n    \n    # Create timestamp for unique filenames\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    paper_name = (paper_title_final or 'unknown_paper')[:30].replace(\" \", \"_\").replace(\"/\", \"_\")\n    base_filename = f\"{paper_name}_{timestamp}\"\n    \n    # 1. Save complete natural analysis as text file\n    analysis_file = f\"{base_filename}_analysis.txt\"\n    with open(analysis_file, 'w', encoding='utf-8') as f:\n        f.write(f\"# Natural Analysis of: {paper_title_final}\\n\\n\")\n        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n        f.write(complete_analysis)\n    print(f\"‚úÖ Natural analysis saved: {analysis_file}\")\n    \n    # 2. Save graph as GraphML (standard format, works with many tools)\n    graph_file = f\"{base_filename}_graph.graphml\"\n    import networkx as nx\n    nx.write_graphml(knowledge_graph['graph'], graph_file)\n    print(f\"‚úÖ Graph saved: {graph_file}\")\n    \n    # 3. Save complete knowledge graph as pickle (Python objects)\n    kg_file = f\"{base_filename}_knowledge_graph.pkl\"\n    with open(kg_file, 'wb') as f:\n        pickle.dump(knowledge_graph, f)\n    print(f\"‚úÖ Complete KG saved: {kg_file}\")\n    \n    # 4. Save paper metadata and processing info\n    metadata_file = f\"{base_filename}_metadata.json\"\n    metadata = {\n        \"title\": paper_title_final or 'Unknown',\n        \"timestamp\": timestamp,\n        \"content_length\": len(paper_content) if paper_content else 0,\n        \"analysis_length\": len(complete_analysis),\n        \"graph_nodes\": knowledge_graph['stats']['nodes'],\n        \"graph_edges\": knowledge_graph['stats']['edges'],\n        \"discovery_method\": knowledge_graph['stats'].get('discovery_method', 'natural'),\n        \"file_path\": paper_path if 'paper_path' in locals() and paper_path != \"sample_data\" else \"sample_data\",\n        \"mode\": \"sample_data\" if USE_SAMPLE_DATA else \"real_pdf\"\n    }\n    \n    if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n        # Save text content for real papers\n        text_file = f\"{base_filename}_content.txt\"\n        with open(text_file, 'w', encoding='utf-8') as f:\n            f.write(text_content)\n        metadata[\"content_file\"] = text_file\n        print(f\"‚úÖ Text content saved: {text_file}\")\n    \n    with open(metadata_file, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    print(f\"‚úÖ Metadata saved: {metadata_file}\")\n    \n    # 5. Create a comprehensive report\n    report_file = f\"{base_filename}_report.md\"\n    with open(report_file, 'w') as f:\n        f.write(f\"# Natural Knowledge Graph Report\\n\\n\")\n        f.write(f\"**Paper:** {paper_title_final or 'Unknown'}\\n\")\n        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"**Mode:** {'Sample Data' if USE_SAMPLE_DATA else 'Real PDF'}\\n\\n\")\n        \n        f.write(f\"## Natural Analysis\\n\\n\")\n        f.write(f\"{complete_analysis}\\n\\n\")\n        \n        f.write(f\"## Knowledge Graph Statistics\\n\\n\")\n        f.write(f\"- **Content Length:** {len(paper_content) if paper_content else 0:,} characters\\n\")\n        f.write(f\"- **Analysis Length:** {len(complete_analysis):,} characters\\n\")\n        f.write(f\"- **Graph Nodes:** {knowledge_graph['stats']['nodes']}\\n\")\n        f.write(f\"- **Graph Edges:** {knowledge_graph['stats']['edges']}\\n\")\n        f.write(f\"- **Discovery Method:** {knowledge_graph['stats'].get('discovery_method', 'natural')}\\n\\n\")\n        \n        # Show discovered concepts if available\n        G = knowledge_graph['graph']\n        if G.number_of_nodes() > 0:\n            f.write(f\"## Naturally Discovered Concepts\\n\\n\")\n            for node in G.nodes():\n                importance = G.nodes[node].get('importance', 'medium')\n                label = G.nodes[node].get('label', node)\n                f.write(f\"- **{node}**: {label} ({importance} importance)\\n\")\n            f.write(f\"\\n\")\n            \n            f.write(f\"## Natural Relationships\\n\\n\")\n            for edge in G.edges(data=True):\n                source, target, data = edge\n                relationship = data.get('relationship', 'connected to')\n                f.write(f\"- {source} **{relationship}** {target}\\n\")\n            f.write(f\"\\n\")\n        \n        f.write(f\"## Files Generated\\n\\n\")\n        f.write(f\"- `{analysis_file}` - Natural analysis in text format\\n\")\n        f.write(f\"- `{graph_file}` - Knowledge graph in GraphML format\\n\")\n        f.write(f\"- `{kg_file}` - Complete knowledge graph (Python pickle)\\n\")\n        f.write(f\"- `{metadata_file}` - Processing metadata\\n\")\n        if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n            f.write(f\"- `{text_file}` - Extracted text content\\n\")\n        f.write(f\"- `{report_file}` - This comprehensive report\\n\")\n    \n    print(f\"‚úÖ Comprehensive report saved: {report_file}\")\n    \n    print(f\"\\nüìä SAVED FILES SUMMARY:\")\n    print(f\"üìÅ All files saved to: /content/\")\n    print(f\"üè∑Ô∏è Base filename: {base_filename}\")\n    print(f\"üìÑ Files created:\")\n    print(f\"   ‚Ä¢ {analysis_file} (Natural analysis)\")\n    print(f\"   ‚Ä¢ {graph_file} (GraphML graph)\")\n    print(f\"   ‚Ä¢ {kg_file} (Python pickle)\")\n    print(f\"   ‚Ä¢ {metadata_file} (metadata)\")\n    if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n        print(f\"   ‚Ä¢ {text_file} (text content)\")\n    print(f\"   ‚Ä¢ {report_file} (comprehensive report)\")\n    \n    # 6. Download files option (Colab only)\n    if IN_COLAB:\n        print(f\"\\nüì• DOWNLOAD FILES:\")\n        print(f\"Right-click files in the file panel to download\")\n        print(f\"Or run this code to download all at once:\")\n        print(f\"```python\")\n        print(f\"from google.colab import files\")\n        print(f\"files.download('{analysis_file}')\")\n        print(f\"files.download('{graph_file}')\")\n        print(f\"files.download('{kg_file}')\")\n        print(f\"files.download('{metadata_file}')\")\n        if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n            print(f\"files.download('{text_file}')\")\n        print(f\"files.download('{report_file}')\")\n        print(f\"```\")\n    \n    # 7. How to reload the analysis\n    print(f\"\\nüîÑ TO RELOAD THIS ANALYSIS LATER:\")\n    print(f\"```python\")\n    print(f\"import pickle\")\n    print(f\"\")\n    print(f\"# Load natural analysis\")\n    print(f\"with open('{analysis_file}', 'r') as f:\")\n    print(f\"    complete_analysis = f.read()\")\n    print(f\"\")\n    print(f\"# Load complete knowledge graph\")\n    print(f\"with open('{kg_file}', 'rb') as f:\")\n    print(f\"    knowledge_graph = pickle.load(f)\")\n    print(f\"\")\n    print(f\"# Load graph separately (if needed)\")\n    print(f\"import networkx as nx\")\n    print(f\"graph = nx.read_graphml('{graph_file}')\")\n    print(f\"```\")\n    \nelse:\n    print(\"‚ùå No natural analysis to save\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üéâ Complete Success!\n\nIf you see results above, you have successfully created a **natural knowledge graph system** with Ollama running in Colab!\n\n### ‚úÖ What You Accomplished:\n\n**Infrastructure:**\n- ‚úÖ **Installed Ollama** in Google Colab environment\n- ‚úÖ **Downloaded models** (llama3.1:8b + nomic-embed-text)\n- ‚úÖ **Started server** successfully in background\n\n**Natural Knowledge Graph System:**\n- ‚úÖ **Processed research paper** with PDF text extraction  \n- ‚úÖ **Natural analysis** using local Ollama LLM without forced categories\n- ‚úÖ **Created embeddings** with nomic-embed-text model\n- ‚úÖ **Built vector store** with ChromaDB for semantic search\n- ‚úÖ **Discovered knowledge graph** with natural concepts and relationships\n- ‚úÖ **Interactive visualization** with yFiles organic layout\n- ‚úÖ **Saved complete results** in multiple formats\n\n### üîç Technical Stack Validated:\n\n**Local LLM Processing**: Ollama running on Colab T4 GPU  \n**Natural Analysis**: Open-ended paper understanding without categories  \n**Vector Embeddings**: Semantic search capabilities over paper content  \n**Knowledge Graph**: Naturally discovered concepts and relationships  \n**Vector Store**: ChromaDB with persistent storage  \n**Hybrid System**: Vector similarity + natural graph structure  \n**Interactive Visualization**: yFiles with organic layout and importance-based sizing\n**Complete Save System**: Text analysis, GraphML, pickle, and summary files\n\n### üöÄ Next Steps:\n- Process multiple papers for cross-paper natural connections\n- Build corpus with naturally emerging themes\n- Scale to literature collections with organic knowledge discovery\n- Use for automated literature synthesis and review generation\n\n**You've proven complete natural knowledge discovery!** üéØ\n\nEach paper creates its own unique knowledge structure based on what the LLM naturally discovers!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}