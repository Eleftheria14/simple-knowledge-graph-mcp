{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Complete Ollama + Knowledge Graph System\n",
    "\n",
    "**All-in-one notebook: Ollama setup + Knowledge graph processing**\n",
    "\n",
    "This notebook:\n",
    "- Installs and starts Ollama in Colab\n",
    "- Downloads required models (llama3.1:8b, nomic-embed-text)\n",
    "- Processes one research paper into a knowledge graph\n",
    "- Creates embeddings and vector store\n",
    "- Shows comprehensive results\n",
    "\n",
    "**Requirements:** Enable GPU runtime (Runtime â†’ Change runtime type â†’ GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration: Real vs Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration: Choose your data source\n# Set USE_SAMPLE_DATA = True to test with fake data (fast, no PDF needed)\n# Set USE_SAMPLE_DATA = False to process real PDF papers (requires PDF upload)\n\nUSE_SAMPLE_DATA = True  # Change to False for real PDF processing\n\nif USE_SAMPLE_DATA:\n    print(\"ðŸŽ­ DEMO MODE: Using sample data\")\n    print(\"   âš¡ Fast testing without PDF upload\")\n    print(\"   ðŸ§ª Pre-extracted entities and content\")\n    print(\"   ðŸš€ Perfect for testing the knowledge graph system\")\n    print(\"   ðŸ“‹ Still uses Ollama for processing and embeddings\")\n    print(\"\")\n    print(\"ðŸ’¡ To process real PDFs:\")\n    print(\"   1. Set USE_SAMPLE_DATA = False\")\n    print(\"   2. Wait for Ollama setup (10-15 minutes)\")\n    print(\"   3. Upload your own PDF file\")\nelse:\n    print(\"ðŸ“„ REAL DATA MODE: Processing actual PDFs\")\n    print(\"   ðŸ“‹ Full Ollama setup required\")\n    print(\"   ðŸ§  Uses LLM for entity extraction\")\n    print(\"   â±ï¸ Takes 15-20 minutes total (setup + processing)\")\n    print(\"\")\n    print(\"ðŸ’¡ For quick testing:\")\n    print(\"   1. Set USE_SAMPLE_DATA = True\")\n    print(\"   2. Still gets full Ollama + LLM experience\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Google Colab and GPU status\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "    \n",
    "    # Check GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No GPU detected!\")\n",
    "        print(\"   Go to Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "        if not USE_SAMPLE_DATA:\n",
    "            print(\"   GPU is REQUIRED for real data processing!\")\n",
    "else:\n",
    "    print(\"ðŸ  Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"ðŸ“¦ Installing core dependencies... â±ï¸ ~2-3 minutes\")\n    !pip install -q langchain langchain-ollama langchain-chroma\n    !pip install -q chromadb>=0.4.0\n    !pip install -q networkx\n    !pip install -q yfiles_jupyter_graphs\n    \n    # Enable custom widget manager for yFiles in Colab\n    from google.colab import output\n    output.enable_custom_widget_manager()\n    print(\"âœ… Custom widget manager enabled for interactive visualizations\")\n    \n    if not USE_SAMPLE_DATA:\n        print(\"ðŸ“¦ Installing PDF processing dependencies... â±ï¸ ~30 seconds\")\n        !pip install -q pdfplumber\n    \n    print(\"âœ… Dependencies installed!\")\nelse:\n    print(\"ðŸ  Using local environment\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Install Ollama"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"ðŸš€ Installing Ollama in Colab... â±ï¸ ~2-3 minutes\")\n    print(\"â±ï¸ This takes about 2-3 minutes...\")\n    \n    # Download and install Ollama\n    !curl -fsSL https://ollama.ai/install.sh | sh\n    \n    print(\"âœ… Ollama installed!\")\n    \nelse:\n    print(\"ðŸ  Assuming local Ollama is running\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Start Ollama Server"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    import subprocess\n    import time\n    import threading\n    import os\n    \n    print(\"ðŸš€ Starting Ollama server...\")\n    \n    # Function to run Ollama serve in background\n    def run_ollama_serve():\n        os.system(\"ollama serve > /dev/null 2>&1 &\")\n    \n    # Start Ollama in a separate thread\n    ollama_thread = threading.Thread(target=run_ollama_serve, daemon=True)\n    ollama_thread.start()\n    \n    # Wait for server to start\n    print(\"â³ Waiting for server to start...\")\n    time.sleep(10)\n    \n    # Test if server is running\n    try:\n        result = !curl -s http://localhost:11434/api/version\n        if result:\n            print(\"âœ… Ollama server is running!\")\n            print(f\"   Version info: {result[0] if result else 'N/A'}\")\n        else:\n            print(\"âŒ Server not responding\")\n    except:\n        print(\"âŒ Failed to check server status\")\n        \nelse:\n    print(\"ðŸ  Assuming local Ollama server is running\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Download Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"ðŸ“¥ Downloading models... â±ï¸ ~5-10 minutes\")\n    print(\"â˜• Perfect time for a coffee break!\")\n    print(\"\")\n    \n    # Download LLM model\n    print(\"ðŸ§  Downloading llama3.1:8b (main LLM)... â±ï¸ ~8 minutes\")\n    !ollama pull llama3.1:8b\n    \n    print(\"\")\n    print(\"ðŸ”¤ Downloading nomic-embed-text (embeddings)... â±ï¸ ~2 minutes\")\n    !ollama pull nomic-embed-text\n    \n    print(\"\")\n    print(\"âœ… All models downloaded and ready!\")\n    \nelse:\n    print(\"ðŸ  Check local models with: ollama list\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Test Ollama Connection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test basic LLM functionality\ntry:\n    from langchain_ollama import ChatOllama\n    \n    print(\"ðŸ§ª Testing LLM connection...\")\n    \n    # Create LLM instance\n    llm = ChatOllama(\n        model=\"llama3.1:8b\",\n        temperature=0.1\n    )\n    \n    # Simple test\n    response = llm.invoke(\"Say 'Hello from Colab!' and nothing else.\")\n    print(f\"âœ… LLM Response: {response.content}\")\n    \n    # Test embeddings\n    from langchain_ollama import OllamaEmbeddings\n    \n    print(\"ðŸ”¤ Testing embeddings...\")\n    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n    \n    test_embedding = embeddings.embed_query(\"This is a test.\")\n    print(f\"âœ… Embedding created: {len(test_embedding)} dimensions\")\n    \n    print(\"\")\n    print(\"ðŸŽ‰ SUCCESS! Ollama is working perfectly in Colab!\")\n    print(\"ðŸš€ Ready to process research papers!\")\n    \nexcept Exception as e:\n    print(f\"âŒ Test failed: {e}\")\n    print(\"ðŸ’¡ You may need to restart runtime and try again\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif USE_SAMPLE_DATA:\n    print(\"ðŸŽ­ Loading sample paper data...\")\n    \n    # Use built-in sample data (no download needed)\n    SAMPLE_PAPER_DATA = {\n        \"title\": \"Machine Learning for Drug Discovery: A Comprehensive Review\",\n        \"content\": \"\"\"Machine Learning for Drug Discovery: A Comprehensive Review\n\nAuthors: Dr. Sarah Chen (MIT), Prof. Michael Torres (Stanford), Dr. Lisa Wang (UC Berkeley)\n\nAbstract:\nThis comprehensive review examines the application of machine learning techniques to drug discovery processes. \nWe analyze various computational approaches including deep learning, graph neural networks, and transformer \narchitectures for molecular property prediction and drug-target interaction modeling.\n\nMethods:\nWe conducted a systematic review of machine learning applications in drug discovery, focusing on:\n\n1. Molecular Property Prediction\n- Graph Convolutional Networks (GCNs) for molecular representation\n- Transformer models adapted for SMILES sequences\n- Recurrent Neural Networks for sequential molecular data\n\n2. Drug-Target Interaction Prediction\n- Matrix factorization techniques\n- Deep neural networks with protein sequence embeddings\n- Graph-based approaches combining molecular and protein structures\n\nTechnologies and Tools:\n- Deep Learning: TensorFlow, PyTorch, Keras\n- Cheminformatics: RDKit, OpenEye, ChemAxon\n- Graph Processing: DGL, PyTorch Geometric, NetworkX\n\nConclusions:\nMachine learning has fundamentally transformed drug discovery by enabling more efficient exploration of chemical \nand biological space. Future success will depend on continued collaboration between computational scientists, \nmedicinal chemists, and clinical researchers.\"\"\",\n        \"pages\": 12,\n        \"char_count\": 1234\n    }\n    \n    # Use sample data for natural discovery\n    paper_path = \"sample_data\"  # Placeholder\n    paper_title = SAMPLE_PAPER_DATA[\"title\"]\n    text_content = SAMPLE_PAPER_DATA[\"content\"]\n    \n    print(f\"âœ… Sample data loaded!\")\n    print(f\"ðŸ“° Title: {paper_title}\")\n    print(f\"ðŸ“Š Content length: {len(text_content):,} characters\")\n    print(f\"ðŸ“„ Simulated pages: {SAMPLE_PAPER_DATA['pages']}\")\n    print(f\"ðŸŒ¿ Ready for natural knowledge discovery\")\n    \nelif IN_COLAB:\n    print(\"ðŸ“¤ Choose how to load your PDF:\")\n    print(\"   1ï¸âƒ£ Upload file using file picker\")\n    print(\"   2ï¸âƒ£ Use file already in Colab storage\")\n    print(\"\")\n    \n    # Check for existing PDFs in current directory\n    existing_pdfs = [f for f in os.listdir('.') if f.endswith('.pdf')]\n    \n    if existing_pdfs:\n        print(f\"ðŸ“ Found {len(existing_pdfs)} PDF(s) in current directory:\")\n        for i, pdf in enumerate(existing_pdfs, 1):\n            file_size = os.path.getsize(pdf) / (1024*1024)  # MB\n            print(f\"   {i}. {pdf} ({file_size:.1f} MB)\")\n        print(\"\")\n        \n        choice = input(\"Type filename to use existing PDF, or press Enter to upload new file: \").strip()\n        \n        if choice and choice in existing_pdfs:\n            paper_path = choice\n            print(f\"âœ… Using existing file: {paper_path}\")\n        else:\n            print(\"ðŸ“¤ Upload a new PDF file...\")\n            from google.colab import files\n            uploaded = files.upload()\n            \n            # Get the first PDF\n            paper_path = None\n            for filename in uploaded.keys():\n                if filename.endswith('.pdf'):\n                    paper_path = filename\n                    break\n    else:\n        print(\"ðŸ“ No existing PDFs found in current directory\")\n        print(\"ðŸ“¤ Upload a PDF file...\")\n        from google.colab import files\n        uploaded = files.upload()\n        \n        # Get the first PDF\n        paper_path = None\n        for filename in uploaded.keys():\n            if filename.endswith('.pdf'):\n                paper_path = filename\n                break\n    \n    if paper_path:\n        file_size = os.path.getsize(paper_path) / (1024*1024)  # MB\n        print(f\"âœ… Paper selected: {paper_path} ({file_size:.1f} MB)\")\n        \n        # Show file details\n        print(f\"ðŸ“ File location: /content/{paper_path}\")\n        print(f\"ðŸ“Š File size: {file_size:.1f} MB\")\n    else:\n        print(\"âŒ No PDF file found! Please upload a PDF.\")\n        \nelse:\n    # Use local example\n    paper_path = '../../examples/d4sc03921a.pdf'\n    if os.path.exists(paper_path):\n        print(f\"âœ… Using local paper: {paper_path}\")\n    else:\n        print(f\"âŒ Local paper not found: {paper_path}\")\n        paper_path = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Extract Text from PDF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAMPLE_DATA:\n",
    "    print(\"ðŸŽ­ Using sample text content (already loaded)\")\n",
    "    print(f\"âœ… Text content ready!\")\n",
    "    print(f\"ðŸ“° Title: {paper_title}\")\n",
    "    print(f\"ðŸ“Š Content length: {len(text_content):,} characters\")\n",
    "    print(f\"ðŸ“„ Sample paper simulates {SAMPLE_PAPER_DATA['pages']} pages\")\n",
    "    \n",
    "elif paper_path:\n",
    "    import pdfplumber\n",
    "    \n",
    "    print(f\"ðŸ“„ Extracting text from: {paper_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract text\n",
    "        with pdfplumber.open(paper_path) as pdf:\n",
    "            text_content = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_content += page_text + \"\\n\\n\"\n",
    "        \n",
    "        # Get paper title (first substantial line)\n",
    "        lines = text_content.split('\\n')\n",
    "        paper_title = \"Unknown Title\"\n",
    "        for line in lines:\n",
    "            if len(line.strip()) > 20 and not line.strip().isdigit():\n",
    "                paper_title = line.strip()[:100]\n",
    "                break\n",
    "        \n",
    "        print(f\"âœ… Text extracted successfully!\")\n",
    "        print(f\"ðŸ“° Title: {paper_title}\")\n",
    "        print(f\"ðŸ“Š Content length: {len(text_content):,} characters\")\n",
    "        print(f\"ðŸ“„ Pages processed: {len(pdf.pages)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to extract text: {e}\")\n",
    "        text_content = None\n",
    "        paper_title = None\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No paper to process\")\n",
    "    text_content = None\n",
    "    paper_title = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Natural Paper Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_SAMPLE_DATA:\n    print(\"ðŸŽ­ Using sample paper content for natural analysis\")\n    print(f\"âœ… Sample paper loaded!\")\n    \n    # Use the complete sample paper content directly for analysis\n    paper_content = text_content\n    paper_title_final = paper_title\n    \n    # For demo mode, create a sample natural analysis\n    complete_analysis = \"\"\"This paper provides a comprehensive review of machine learning applications in drug discovery. The research examines how computational approaches, particularly deep learning and graph neural networks, are transforming pharmaceutical research.\n\nThe paper covers three main areas: molecular property prediction using Graph Convolutional Networks and transformer models, drug-target interaction prediction through deep neural networks and matrix factorization, and virtual screening using generative models and reinforcement learning.\n\nKey findings include the effectiveness of graph-based approaches for molecular representation, the importance of transformer architectures for SMILES sequences, and the potential of generative adversarial networks for novel molecule design. The work highlights major datasets like ChEMBL, PubChem, and ZINC, along with important technologies including TensorFlow, PyTorch, and RDKit.\n\nThe research concludes that machine learning has fundamentally transformed drug discovery by enabling more efficient exploration of chemical and biological space, though challenges remain in data quality, model interpretability, and regulatory acceptance.\"\"\"\n    \n    print(f\"ðŸ“Š Content length: {len(text_content):,} characters\")\n    print(f\"ðŸ“ Analysis length: {len(complete_analysis):,} characters\")\n    print(f\"ðŸ“„ Ready for natural knowledge graph creation\")\n    \nelif text_content:\n    from langchain_ollama import ChatOllama\n    from langchain_core.prompts import ChatPromptTemplate\n    \n    print(\"ðŸ§  Analyzing complete paper with LLM... â±ï¸ ~2-5 minutes\")\n    print(\"â±ï¸ This analyzes the ENTIRE paper content without predefined categories...\")\n    \n    # Create LLM\n    llm = ChatOllama(\n        model=\"llama3.1:8b\",\n        temperature=0.1\n    )\n    \n    # Simple, open-ended analysis prompt\n    prompt_text = '''You are an expert research analyst. Read this COMPLETE research paper and provide a comprehensive, natural analysis.\n\nCOMPLETE PAPER CONTENT:\n{content}\n\nAnalyze this paper thoroughly and naturally. Don't force it into categories - just understand it completely and tell me:\n\n1. What is this paper about?\n2. What are the main ideas, findings, and contributions?\n3. What methods, approaches, or techniques are used?\n4. What's important or interesting about this work?\n5. What are the key concepts, technologies, or data mentioned?\n\nProvide a thorough, natural analysis - not a structured format. Just understand the paper completely and explain it comprehensively.'''\n    \n    prompt = ChatPromptTemplate.from_template(prompt_text)\n    \n    try:\n        # Process paper in chunks if too long, then synthesize\n        max_chars = 25000  # Conservative limit for analysis\n        \n        if len(text_content) > max_chars:\n            print(f\"ðŸ“„ Paper is long ({len(text_content):,} chars), analyzing in sections... â±ï¸ ~3-7 minutes\")\n            \n            # Split into logical sections\n            sections = []\n            chunk_size = max_chars\n            \n            for i in range(0, len(text_content), chunk_size):\n                section = text_content[i:i+chunk_size]\n                sections.append(section)\n            \n            print(f\"ðŸ”„ Analyzing {len(sections)} sections...\")\n            \n            section_analyses = []\n            for i, section in enumerate(sections, 1):\n                print(f\"   Analyzing section {i}/{len(sections)}... â±ï¸ ~1-2 minutes per section\")\n                \n                chain = prompt | llm\n                result = chain.invoke({\n                    \"content\": section\n                })\n                \n                section_analyses.append(result.content)\n            \n            # Now synthesize all sections into final analysis\n            if section_analyses:\n                print(\"ðŸ”„ Synthesizing complete paper understanding... â±ï¸ ~1-2 minutes\")\n                \n                synthesis_prompt = '''You have analyzed different sections of a research paper. Now synthesize these section analyses into one comprehensive understanding of the complete paper.\n\nSECTION ANALYSES:\n{sections}\n\nProvide a complete, unified analysis of the entire paper. What is this research really about? What are the key insights across the whole work?'''\n                \n                synthesis_chain = ChatPromptTemplate.from_template(synthesis_prompt) | llm\n                synthesis_result = synthesis_chain.invoke({\n                    \"sections\": \"\\n\\n---SECTION---\\n\\n\".join(section_analyses)\n                })\n                \n                complete_analysis = synthesis_result.content\n            else:\n                print(\"âŒ No section analyses completed\")\n                complete_analysis = None\n                \n        else:\n            print(f\"ðŸ“„ Analyzing complete paper ({len(text_content):,} chars)... â±ï¸ ~2-3 minutes\")\n            \n            # Process entire paper at once\n            chain = prompt | llm\n            result = chain.invoke({\n                \"content\": text_content\n            })\n            \n            complete_analysis = result.content\n        \n        if complete_analysis:\n            print(\"âœ… Complete paper analysis finished!\")\n            print(f\"\\nðŸ“Š PAPER ANALYSIS:\")\n            print(f\"ðŸ“„ Title: {paper_title}\")\n            print(f\"ðŸ“ Analysis length: {len(complete_analysis):,} characters\")\n            print(f\"ðŸ” Analysis preview: {complete_analysis[:200]}...\")\n            \n            # Store the results\n            paper_content = text_content\n            paper_title_final = paper_title\n            \n        else:\n            print(\"âŒ Paper analysis failed\")\n            complete_analysis = None\n            \n    except Exception as e:\n        print(f\"âŒ Paper analysis failed: {e}\")\n        complete_analysis = None\n        \nelse:\n    print(\"âŒ No text content to analyze\")\n    complete_analysis = None\n    paper_content = None\n    paper_title_final = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10: Create Embeddings and Vector Store"
  },
  {
   "cell_type": "code",
   "source": "if paper_content and complete_analysis:\n    print(\"ðŸ“š Extracting citations and preparing database links...\")\n    print(\"â±ï¸ Analyzing citation patterns and reference mapping...\")\n    \n    # Citation tracking implementation (adapted from src/citation_tracker.py)\n    import re\n    from datetime import datetime\n    \n    def extract_citations_from_paper(content, paper_title):\n        \"\"\"Extract citations with precise location tracking\"\"\"\n        \n        # Citation patterns for different formats\n        citation_patterns = {\n            # Numbered citations: [1], [1,2,3], [1-3]\n            \"numbered\": [\n                r'\\[(\\d+(?:[-,]\\s*\\d+)*)\\]',\n                r'\\((\\d+(?:[-,]\\s*\\d+)*)\\)'\n            ],\n            \n            # Author-year citations: (Smith, 2020), (Smith et al., 2020)\n            \"author_year\": [\n                r'\\(([A-Za-z]+(?:\\s+et\\s+al\\.)?(?:,\\s*\\d{4})?)\\)',\n                r'([A-Za-z]+\\s+et\\s+al\\.\\s*\\(\\d{4}\\))',\n                r'([A-Za-z]+(?:,\\s*\\d{4})?)'\n            ],\n            \n            # Superscript citations: text^1, text^1,2,3\n            \"superscript\": [\n                r'\\^(\\d+(?:[-,]\\s*\\d+)*)',\n                r'(\\d+(?:[-,]\\s*\\d+)*)\\s*(?=\\.|,|\\s)'\n            ],\n            \n            # Full author citations: Smith (2020), According to Smith (2020)\n            \"full_author\": [\n                r'([A-Za-z]+(?:\\s+et\\s+al\\.)?)\\s*\\((\\d{4})\\)',\n                r'(?:According\\s+to|As\\s+shown\\s+by)\\s+([A-Za-z]+(?:\\s+et\\s+al\\.)?)\\s*\\((\\d{4})\\)'\n            ]\n        }\n        \n        citations = []\n        \n        # Extract inline citations with locations\n        for citation_type, patterns in citation_patterns.items():\n            for pattern in patterns:\n                for match in re.finditer(pattern, content, re.IGNORECASE):\n                    # Get line number\n                    line_num = content[:match.start()].count('\\n') + 1\n                    \n                    # Get context around citation\n                    context_start = max(0, match.start() - 100)\n                    context_end = min(len(content), match.end() + 100)\n                    context = content[context_start:context_end].replace('\\n', ' ')\n                    \n                    citation_info = {\n                        \"type\": citation_type,\n                        \"text\": match.group(0),\n                        \"citation_key\": match.group(1) if match.groups() else match.group(0),\n                        \"start_position\": match.start(),\n                        \"end_position\": match.end(),\n                        \"line_number\": line_num,\n                        \"context\": context.strip(),\n                        \"section\": \"main_text\"  # Could be enhanced to detect sections\n                    }\n                    citations.append(citation_info)\n        \n        # Extract reference list\n        reference_list = []\n        \n        # Look for references section\n        ref_patterns = [\n            r'(?:References|Bibliography|Literature\\s+Cited)\\s*\\n(.*?)(?:\\n\\n|\\n[A-Z]|\\Z)',\n            r'(?:REFERENCES|BIBLIOGRAPHY)\\s*\\n(.*?)(?:\\n\\n|\\n[A-Z]|\\Z)'\n        ]\n        \n        ref_section = \"\"\n        for pattern in ref_patterns:\n            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)\n            if match:\n                ref_section = match.group(1)\n                break\n        \n        if ref_section:\n            # Split references by line and parse\n            ref_lines = ref_section.split('\\n')\n            for i, line in enumerate(ref_lines):\n                line = line.strip()\n                if len(line) > 20:  # Filter short lines\n                    # Try to extract basic info (this could be enhanced)\n                    year_match = re.search(r'(\\d{4})', line)\n                    year = year_match.group(1) if year_match else None\n                    \n                    # Try to extract title (text in quotes or after authors)\n                    title_match = re.search(r'[\"\\']([^\"\\']{10,})[\"\\']', line)\n                    title = title_match.group(1) if title_match else \"\"\n                    \n                    reference_info = {\n                        \"ref_number\": i + 1,\n                        \"full_text\": line,\n                        \"title\": title,\n                        \"year\": year,\n                        \"authors\": \"\",  # Could be enhanced\n                        \"journal\": \"\",  # Could be enhanced\n                        \"doi\": \"\",      # Could be enhanced\n                    }\n                    reference_list.append(reference_info)\n        \n        # Create paper metadata for database linking\n        paper_metadata = {\n            \"title\": paper_title,\n            \"document_id\": f\"paper_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n            \"extraction_date\": datetime.now().isoformat(),\n            \"total_citations\": len(citations),\n            \"total_references\": len(reference_list)\n        }\n        \n        return {\n            \"paper_metadata\": paper_metadata,\n            \"inline_citations\": citations,\n            \"reference_list\": reference_list,\n            \"citation_density\": len(citations) / len(content.split()) if content else 0,\n            \"database_ready\": True\n        }\n    \n    # Extract citations from the paper\n    try:\n        citation_data = extract_citations_from_paper(paper_content, paper_title_final)\n        \n        print(f\"âœ… Citation extraction completed!\")\n        print(f\"   ðŸ“Š Inline citations found: {len(citation_data['inline_citations'])}\")\n        print(f\"   ðŸ“š References found: {len(citation_data['reference_list'])}\")\n        print(f\"   ðŸ“ˆ Citation density: {citation_data['citation_density']:.4f} (citations/word)\")\n        print(f\"   ðŸ”— Database document ID: {citation_data['paper_metadata']['document_id']}\")\n        \n        # Show sample citations\n        if citation_data['inline_citations']:\n            print(f\"\\nðŸ“ Sample inline citations:\")\n            for i, citation in enumerate(citation_data['inline_citations'][:3], 1):\n                print(f\"   {i}. [{citation['type']}] '{citation['text']}' at line {citation['line_number']}\")\n                print(f\"      Context: ...{citation['context'][:80]}...\")\n        \n        # Show sample references\n        if citation_data['reference_list']:\n            print(f\"\\nðŸ“š Sample references:\")\n            for i, ref in enumerate(citation_data['reference_list'][:3], 1):\n                print(f\"   {i}. {ref['full_text'][:100]}...\")\n                if ref['year']:\n                    print(f\"      Year: {ref['year']}\")\n                if ref['title']:\n                    print(f\"      Title: {ref['title'][:60]}...\")\n        \n        # Prepare database-ready structure\n        database_entry = {\n            \"document_id\": citation_data['paper_metadata']['document_id'],\n            \"paper_title\": paper_title_final,\n            \"content\": paper_content,\n            \"analysis\": complete_analysis,\n            \"citations\": citation_data['inline_citations'],\n            \"references\": citation_data['reference_list'],\n            \"metadata\": {\n                \"extraction_date\": citation_data['paper_metadata']['extraction_date'],\n                \"citation_count\": citation_data['paper_metadata']['total_citations'],\n                \"reference_count\": citation_data['paper_metadata']['total_references'],\n                \"citation_density\": citation_data['citation_density'],\n                \"mode\": \"sample_data\" if USE_SAMPLE_DATA else \"real_pdf\"\n            }\n        }\n        \n        print(f\"\\nðŸ—„ï¸ DATABASE INTEGRATION READY:\")\n        print(f\"   ðŸ“„ Document prepared for database storage\")\n        print(f\"   ðŸ”— Citations linked to precise locations\")\n        print(f\"   ðŸ“Š Metadata includes extraction statistics\")\n        print(f\"   ðŸ’¾ Ready for literature corpus integration\")\n        \n        # Note about database connection\n        print(f\"\\nðŸ’¡ DATABASE CONNECTION:\")\n        print(f\"   To actually store in database, you would:\")\n        print(f\"   1. Set up PostgreSQL database (see docs/database_setup_instructions.md)\")\n        print(f\"   2. Use CitationDatabaseManager from the main codebase\")\n        print(f\"   3. Call store_paper_with_citations(database_entry)\")\n        print(f\"   4. Enable cross-paper citation linking for literature discovery\")\n        \n    except Exception as e:\n        print(f\"âŒ Citation extraction failed: {e}\")\n        citation_data = None\n        database_entry = None\n        \nelse:\n    print(\"âŒ No paper content to extract citations from\")\n    citation_data = None\n    database_entry = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 11: Create Embeddings and Vector Store",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if paper_content and complete_analysis:\n    from langchain_ollama import OllamaEmbeddings\n    from langchain_chroma import Chroma\n    from langchain_core.documents import Document\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    \n    print(\"ðŸ”¤ Creating embeddings and vector store from complete paper...\")\n    print(\"â±ï¸ This takes 2-3 minutes...\")\n    \n    # Create embeddings model\n    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n    \n    # Split text into chunks for embeddings\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    \n    chunks = text_splitter.split_text(paper_content)\n    print(f\"ðŸ“„ Created {len(chunks)} text chunks from complete paper\")\n    \n    # Create documents with metadata from analysis\n    documents = []\n    for i, chunk in enumerate(chunks):\n        metadata = {\n            'paper_title': paper_title_final,\n            'chunk_id': f\"chunk_{i}\",\n            'chunk_index': i,\n            'total_chunks': len(chunks),\n            'analysis_preview': complete_analysis[:500] if complete_analysis else '',\n            'has_analysis': bool(complete_analysis)\n        }\n        \n        doc = Document(page_content=chunk, metadata=metadata)\n        documents.append(doc)\n    \n    # Also add the complete analysis as a document\n    if complete_analysis:\n        analysis_doc = Document(\n            page_content=complete_analysis,\n            metadata={\n                'paper_title': paper_title_final,\n                'chunk_id': 'complete_analysis',\n                'chunk_index': -1,\n                'total_chunks': len(chunks),\n                'is_analysis': True\n            }\n        )\n        documents.append(analysis_doc)\n    \n    # Create vector store\n    persist_directory = \"/tmp/chroma_paper_complete\"\n    \n    print(\"ðŸ—„ï¸ Creating vector store with ChromaDB...\")\n    vector_store = Chroma(\n        embedding_function=embeddings,\n        persist_directory=persist_directory\n    )\n    \n    # Add documents to vector store\n    document_ids = vector_store.add_documents(documents)\n    \n    print(f\"âœ… Vector store created from complete paper!\")\n    print(f\"   ðŸ“ {len(documents)} documents added (including analysis)\")\n    print(f\"   ðŸ”¤ Embeddings created with nomic-embed-text\")\n    print(f\"   ðŸ—„ï¸ Stored in ChromaDB at {persist_directory}\")\n    \n    # Test semantic search on complete paper\n    print(\"\\nðŸ” Testing semantic search on complete paper...\")\n    query = \"What are the main findings and contributions?\"\n    results = vector_store.similarity_search(query, k=3)\n    \n    print(f\"Query: '{query}'\")\n    print(f\"Found {len(results)} relevant chunks:\")\n    for i, result in enumerate(results, 1):\n        is_analysis = result.metadata.get('is_analysis', False)\n        content_type = \"LLM Analysis\" if is_analysis else \"Paper Content\"\n        print(f\"  {i}. [{content_type}] {result.page_content[:100]}...\")\n    \nelse:\n    print(\"âŒ No paper content to process - skipping vector store creation\")\n    vector_store = None\n    documents = []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 12: Create Natural Knowledge Graph"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if paper_content and complete_analysis:\n    from langchain_ollama import ChatOllama\n    from langchain_core.prompts import ChatPromptTemplate\n    import networkx as nx\n    import json\n    \n    print(\"ðŸ•¸ï¸ Creating natural knowledge graph from paper content...\")\n    print(\"â±ï¸ Let the LLM discover natural relationships...\")\n    \n    # Use LLM to discover natural connections in the content\n    llm = ChatOllama(\n        model=\"llama3.1:8b\",\n        temperature=0.1\n    )\n    \n    graph_prompt = '''You are analyzing this research paper to discover natural relationships and connections.\n\nPAPER CONTENT:\n{content}\n\nLLM ANALYSIS:\n{analysis}\n\nLook at this content naturally and identify:\n1. Key concepts, ideas, and topics that emerge from the paper\n2. Natural relationships and connections between these concepts\n3. Important terms, methods, findings that relate to each other\n\nReturn a JSON with nodes and edges that represent the natural structure you see:\n\n{{\n  \"nodes\": [\n    {{\"id\": \"concept_name\", \"label\": \"Natural concept from paper\", \"importance\": \"high/medium/low\"}},\n    ...\n  ],\n  \"edges\": [\n    {{\"source\": \"concept1\", \"target\": \"concept2\", \"relationship\": \"natural relationship you observe\"}},\n    ...\n  ]\n}}\n\nDiscover what's naturally connected in this research - don't force categories. Let the content reveal its own structure.\n\nJSON:'''\n    \n    try:\n        print(\"ðŸ” Discovering natural connections in the paper...\")\n        \n        # Let LLM discover natural graph structure\n        prompt = ChatPromptTemplate.from_template(graph_prompt)\n        chain = prompt | llm\n        result = chain.invoke({\n            \"content\": paper_content[:15000],  # First part of content\n            \"analysis\": complete_analysis[:5000] if complete_analysis else \"\"\n        })\n        \n        # Extract JSON from response\n        response_text = result.content\n        json_start = response_text.find('{')\n        json_end = response_text.rfind('}') + 1\n        \n        if json_start != -1 and json_end != -1:\n            json_str = response_text[json_start:json_end]\n            graph_data = json.loads(json_str)\n            \n            # Create NetworkX graph from discovered structure\n            G = nx.Graph()\n            \n            # Add nodes with natural attributes\n            nodes_added = set()\n            for node in graph_data.get('nodes', []):\n                node_id = node.get('id', '')\n                if node_id and node_id not in nodes_added:\n                    G.add_node(\n                        node_id,\n                        label=node.get('label', node_id),\n                        importance=node.get('importance', 'medium'),\n                        type='natural_concept'\n                    )\n                    nodes_added.add(node_id)\n            \n            # Add edges with natural relationships\n            for edge in graph_data.get('edges', []):\n                source = edge.get('source', '')\n                target = edge.get('target', '')\n                relationship = edge.get('relationship', 'related_to')\n                \n                if source in nodes_added and target in nodes_added:\n                    G.add_edge(source, target, relationship=relationship)\n            \n            print(f\"âœ… Natural knowledge graph discovered!\")\n            print(f\"   ðŸ”— Nodes: {G.number_of_nodes()}\")\n            print(f\"   ðŸ“Š Edges: {G.number_of_edges()}\")\n            print(f\"   ðŸŒ¿ Structure emerged naturally from content\")\n            \n            # Show discovered concepts\n            print(f\"\\nðŸŒ¿ Naturally discovered concepts:\")\n            for node in G.nodes():\n                importance = G.nodes[node].get('importance', 'medium')\n                label = G.nodes[node].get('label', node)\n                print(f\"   â€¢ {node}: {label} ({importance} importance)\")\n            \n        else:\n            print(\"âŒ Could not parse natural graph structure\")\n            print(\"ðŸ”„ Creating simple content-based graph...\")\n            \n            # Fallback: simple content representation\n            G = nx.Graph()\n            G.add_node(paper_title_final or \"Research Paper\", type='paper')\n            G.add_node(\"Paper Content\", type='content')\n            G.add_node(\"LLM Analysis\", type='analysis')\n            G.add_edge(paper_title_final or \"Research Paper\", \"Paper Content\", relationship='contains')\n            G.add_edge(\"Paper Content\", \"LLM Analysis\", relationship='analyzed_to_produce')\n    \n    except Exception as e:\n        print(f\"âŒ Natural graph discovery failed: {e}\")\n        print(\"ðŸ”„ Creating simple representation...\")\n        \n        # Simple fallback\n        G = nx.Graph()\n        G.add_node(paper_title_final or \"Research Paper\", type='paper')\n        G.add_node(\"Paper Content\", type='content')\n        if complete_analysis:\n            G.add_node(\"LLM Analysis\", type='analysis')\n            G.add_edge(paper_title_final or \"Research Paper\", \"Paper Content\", relationship='contains')\n            G.add_edge(\"Paper Content\", \"LLM Analysis\", relationship='produces')\n    \n    # Store for visualization\n    knowledge_graph = {\n        'graph': G,\n        'paper_content': paper_content,\n        'complete_analysis': complete_analysis,\n        'paper_title': paper_title_final,\n        'stats': {\n            'nodes': G.number_of_nodes(),\n            'edges': G.number_of_edges(),\n            'discovery_method': 'natural_llm_discovery'\n        }\n    }\n    \nelse:\n    print(\"âŒ No paper content to build graph from\")\n    knowledge_graph = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 13: Interactive Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if knowledge_graph and knowledge_graph['graph'].number_of_nodes() > 0:\n    print(\"ðŸ“Š Creating interactive yFiles visualization of natural knowledge graph... â±ï¸ ~30 seconds\")\n    \n    try:\n        from yfiles_jupyter_graphs import GraphWidget\n        import networkx as nx\n        \n        G = knowledge_graph['graph']\n        \n        print(f\"ðŸŽ® Building interactive graph with {G.number_of_nodes()} naturally discovered nodes...\")\n        \n        # Create yFiles widget with explicit size and configuration\n        widget = GraphWidget(\n            graph=G,\n            layout={\"width\": \"100%\", \"height\": \"600px\"}  # Explicit sizing\n        )\n        \n        # Configure node styling using correct yFiles API\n        def node_color_mapping(node):\n            \"\"\"Map node to color based on properties\"\"\"\n            # Access node properties through the yFiles node dictionary\n            properties = node.get('properties', {})\n            importance = properties.get('importance', 'medium')\n            node_type = properties.get('type', 'natural_concept')\n            \n            # Natural color scheme based on importance and type\n            if node_type == 'paper':\n                return '#1f4e79'  # Deep blue for main paper\n            elif node_type == 'analysis':\n                return '#7b68ee'  # Medium slate blue for analysis\n            elif importance == 'high':\n                return '#e74c3c'  # Red for high importance\n            elif importance == 'medium':\n                return '#3498db'  # Blue for medium importance\n            else:  # low importance\n                return '#95a5a6'  # Gray for low importance\n        \n        def node_size_mapping(node):\n            \"\"\"Map node to size based on properties\"\"\"\n            properties = node.get('properties', {})\n            importance = properties.get('importance', 'medium')\n            node_type = properties.get('type', 'natural_concept')\n            \n            if node_type == 'paper':\n                return 50\n            elif node_type == 'analysis':\n                return 40\n            elif importance == 'high':\n                return 45\n            elif importance == 'medium':\n                return 35\n            else:  # low importance\n                return 25\n        \n        def node_label_mapping(node):\n            \"\"\"Map node to display label\"\"\"\n            properties = node.get('properties', {})\n            label = properties.get('label', properties.get('yf_label', 'Unknown'))\n            return label[:40] + \"...\" if len(label) > 40 else label\n        \n        def edge_color_mapping(edge):\n            \"\"\"Simple edge color mapping\"\"\"\n            return '#bdc3c7'\n        \n        def edge_thickness_mapping(edge):\n            \"\"\"Simple edge thickness mapping\"\"\"\n            return 2\n        \n        # Apply mappings using correct yFiles API\n        widget.node_color_mapping = node_color_mapping\n        widget.node_size_mapping = node_size_mapping\n        widget.node_label_mapping = node_label_mapping\n        widget.edge_color_mapping = edge_color_mapping\n        widget.edge_thickness_mapping = edge_thickness_mapping\n        \n        # Configure layout and interaction with explicit settings\n        widget.graph_layout = 'organic'\n        widget.overview_enabled = True\n        \n        # Additional widget configuration for Colab compatibility\n        widget.context_start_with = 'empty'  # Start with clean slate\n        widget.sidebar_enabled = True\n        \n        print(\"âœ… Interactive natural knowledge graph created!\")\n        print(\"ðŸŽ® Controls:\")\n        print(\"   â€¢ Drag nodes to rearrange\")\n        print(\"   â€¢ Zoom with mouse wheel\") \n        print(\"   â€¢ Click nodes to highlight connections\")\n        print(\"   â€¢ Use overview panel for navigation\")\n        print(\"\")\n        \n        # Force widget initialization and display\n        try:\n            # Enable custom widgets first\n            from google.colab import output\n            output.enable_custom_widget_manager()\n            \n            # Display the widget with explicit call\n            display(widget)\n            \n            # Additional initialization for stubborn widgets\n            import time\n            time.sleep(1)  # Give widget time to initialize\n            \n            print(\"ðŸ“± yFiles widget should appear above â¬†ï¸\")\n            print(\"\")\n            print(\"ðŸ’¡ If you see a black square:\")\n            print(\"   1. Wait 5-10 seconds for widget to load\")\n            print(\"   2. Try scrolling up and down\")\n            print(\"   3. Click on the widget area\")\n            print(\"   4. Refresh the cell output if needed\")\n            \n        except Exception as display_error:\n            print(f\"âš ï¸ Widget display issue: {display_error}\")\n            print(\"ðŸ“Š Using text-based visualization instead\")\n        \n        # Show natural relationships discovered\n        print(\"ðŸŒ¿ Natural relationships discovered:\")\n        for edge in G.edges(data=True):\n            source, target, data = edge\n            relationship = data.get('relationship', 'connected to')\n            print(f\"   â€¢ {source} {relationship} {target}\")\n        \n    except ImportError:\n        print(\"âŒ yfiles_jupyter_graphs not available\")\n        print(\"ðŸ’¡ Install with: pip install yfiles_jupyter_graphs\")\n        \n        # Set G for text display\n        G = knowledge_graph['graph']\n        \n    except Exception as e:\n        print(f\"âŒ yFiles visualization failed: {e}\")\n        print(\"ðŸ’¡ Try restarting the runtime and running all cells again\")\n        \n        # Set G for text display\n        G = knowledge_graph['graph']\n    \n    # Enhanced text-based summary (always shown for reliability)\n    print(f\"\\nðŸ“Š DETAILED GRAPH STRUCTURE:\")\n    G = knowledge_graph['graph']\n    print(f\"   ðŸ”— Total Nodes: {G.number_of_nodes()}\")\n    print(f\"   ðŸ“Š Total Edges: {G.number_of_edges()}\")\n    print(f\"   ðŸŒ¿ Discovery Method: Natural LLM analysis\")\n    \n    print(f\"\\nðŸŒ¿ All Discovered Concepts:\")\n    for i, node in enumerate(G.nodes(), 1):\n        importance = G.nodes[node].get('importance', 'medium')\n        label = G.nodes[node].get('label', node)\n        node_type = G.nodes[node].get('type', 'natural_concept')\n        importance_emoji = \"ðŸ”´\" if importance == 'high' else \"ðŸ”µ\" if importance == 'medium' else \"âšª\"\n        print(f\"   {i:2d}. {importance_emoji} {node}\")\n        print(f\"       ðŸ“ {label}\")\n        print(f\"       ðŸ·ï¸ Type: {node_type}, Importance: {importance}\")\n    \n    print(f\"\\nðŸ”— All Natural Relationships:\")\n    for i, edge in enumerate(G.edges(data=True), 1):\n        source, target, data = edge\n        relationship = data.get('relationship', 'connected to')\n        print(f\"   {i:2d}. {source}\")\n        print(f\"       âž¡ï¸ [{relationship}] âž¡ï¸\")\n        print(f\"       {target}\")\n    \n    # Print comprehensive summary\n    print(f\"\\nðŸ“Š NATURAL KNOWLEDGE GRAPH SUMMARY:\")\n    print(f\"   ðŸ“„ Paper: {knowledge_graph.get('paper_title', 'Unknown')}\")\n    print(f\"   ðŸŒ¿ Discovery method: {knowledge_graph['stats'].get('discovery_method', 'natural')}\")\n    print(f\"   ðŸ”— Naturally discovered nodes: {knowledge_graph['stats']['nodes']}\")\n    print(f\"   ðŸ“Š Natural relationships: {knowledge_graph['stats']['edges']}\")\n    print(f\"   ðŸ”¤ Vector store documents: {len(documents) if 'documents' in locals() else 0}\")\n    print(f\"   ðŸ—„ï¸ Vector store: {'âœ… Created' if 'vector_store' in locals() and vector_store else 'âŒ Not created'}\")\n    \n    # Show analysis preview\n    analysis = knowledge_graph.get('complete_analysis', '')\n    if analysis:\n        print(f\"\\nðŸ“ LLM ANALYSIS PREVIEW:\")\n        print(f\"   {analysis[:300]}...\")\n    \nelse:\n    print(\"âŒ No natural knowledge graph to visualize\")"
  },
  {
   "cell_type": "code",
   "source": "# ðŸ”§ WIDGET TROUBLESHOOTING - Run this cell if you see a black square\n\nif 'knowledge_graph' in locals() and knowledge_graph and knowledge_graph['graph'].number_of_nodes() > 0:\n    print(\"ðŸ”§ Attempting to fix yFiles widget display...\")\n    \n    try:\n        # Force enable custom widgets multiple times\n        from google.colab import output\n        output.enable_custom_widget_manager()\n        \n        # Re-import yFiles\n        from yfiles_jupyter_graphs import GraphWidget\n        import time\n        \n        G = knowledge_graph['graph']\n        print(f\"ðŸ“Š Recreating widget for {G.number_of_nodes()} nodes...\")\n        \n        # Create a minimal widget first\n        simple_widget = GraphWidget(graph=G)\n        \n        # Basic configuration\n        simple_widget.graph_layout = 'circular'  # Try simpler layout first\n        simple_widget.overview_enabled = False   # Disable complex features\n        \n        print(\"ðŸŽ® Displaying simplified widget...\")\n        display(simple_widget)\n        \n        time.sleep(2)  # Wait for initialization\n        \n        print(\"âœ… Simplified widget created!\")\n        print(\"\")\n        print(\"ðŸ’¡ Troubleshooting tips:\")\n        print(\"   1. The widget should appear above this text\")\n        print(\"   2. If still black, try clicking on it\")\n        print(\"   3. Try zooming out (Ctrl + scroll down)\")\n        print(\"   4. Wait 10-15 seconds for full loading\")\n        print(\"   5. If nothing works, restart runtime and run all cells again\")\n        \n    except Exception as e:\n        print(f\"âŒ Widget troubleshooting failed: {e}\")\n        print(\"ðŸ’¡ The text-based visualization below shows your complete knowledge graph structure\")\n\nelse:\n    print(\"âŒ No knowledge graph available - run previous cells first\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "if knowledge_graph and knowledge_graph['graph'].number_of_nodes() > 0:\n    print(\"ðŸ“Š Creating interactive yFiles visualization of natural knowledge graph... â±ï¸ ~30 seconds\")\n    \n    try:\n        from yfiles_jupyter_graphs import GraphWidget\n        import networkx as nx\n        \n        G = knowledge_graph['graph']\n        \n        print(f\"ðŸŽ® Building interactive graph with {G.number_of_nodes()} naturally discovered nodes...\")\n        \n        # Create yFiles widget (without invalid layout parameter)\n        widget = GraphWidget(graph=G)\n        \n        # Configure node styling using correct yFiles API\n        def node_color_mapping(node):\n            \"\"\"Map node to color based on properties\"\"\"\n            # Access node properties through the yFiles node dictionary\n            properties = node.get('properties', {})\n            importance = properties.get('importance', 'medium')\n            node_type = properties.get('type', 'natural_concept')\n            \n            # Natural color scheme based on importance and type\n            if node_type == 'paper':\n                return '#1f4e79'  # Deep blue for main paper\n            elif node_type == 'analysis':\n                return '#7b68ee'  # Medium slate blue for analysis\n            elif importance == 'high':\n                return '#e74c3c'  # Red for high importance\n            elif importance == 'medium':\n                return '#3498db'  # Blue for medium importance\n            else:  # low importance\n                return '#95a5a6'  # Gray for low importance\n        \n        def node_size_mapping(node):\n            \"\"\"Map node to size based on properties\"\"\"\n            properties = node.get('properties', {})\n            importance = properties.get('importance', 'medium')\n            node_type = properties.get('type', 'natural_concept')\n            \n            if node_type == 'paper':\n                return 50\n            elif node_type == 'analysis':\n                return 40\n            elif importance == 'high':\n                return 45\n            elif importance == 'medium':\n                return 35\n            else:  # low importance\n                return 25\n        \n        def node_label_mapping(node):\n            \"\"\"Map node to display label\"\"\"\n            properties = node.get('properties', {})\n            label = properties.get('label', properties.get('yf_label', 'Unknown'))\n            return label[:40] + \"...\" if len(label) > 40 else label\n        \n        def edge_color_mapping(edge):\n            \"\"\"Simple edge color mapping\"\"\"\n            return '#bdc3c7'\n        \n        def edge_thickness_mapping(edge):\n            \"\"\"Simple edge thickness mapping\"\"\"\n            return 2\n        \n        # Apply mappings using correct yFiles API\n        widget.node_color_mapping = node_color_mapping\n        widget.node_size_mapping = node_size_mapping\n        widget.node_label_mapping = node_label_mapping\n        widget.edge_color_mapping = edge_color_mapping\n        widget.edge_thickness_mapping = edge_thickness_mapping\n        \n        # Configure layout and interaction (set as properties, not constructor args)\n        widget.graph_layout = 'organic'\n        widget.overview_enabled = True\n        widget.context_start_with = 'empty'\n        widget.sidebar_enabled = True\n        \n        print(\"âœ… Interactive natural knowledge graph created!\")\n        print(\"ðŸŽ® Controls:\")\n        print(\"   â€¢ Drag nodes to rearrange\")\n        print(\"   â€¢ Zoom with mouse wheel\") \n        print(\"   â€¢ Click nodes to highlight connections\")\n        print(\"   â€¢ Use overview panel for navigation\")\n        print(\"\")\n        \n        # Force widget initialization and display\n        try:\n            # Enable custom widgets first\n            from google.colab import output\n            output.enable_custom_widget_manager()\n            \n            # Display the widget with explicit call\n            display(widget)\n            \n            # Additional initialization for stubborn widgets\n            import time\n            time.sleep(1)  # Give widget time to initialize\n            \n            print(\"ðŸ“± yFiles widget should appear above â¬†ï¸\")\n            print(\"\")\n            print(\"ðŸ’¡ If you see a black square:\")\n            print(\"   1. Wait 5-10 seconds for widget to load\")\n            print(\"   2. Try scrolling up and down\")\n            print(\"   3. Click on the widget area\")\n            print(\"   4. Run the troubleshooting cell below if needed\")\n            \n        except Exception as display_error:\n            print(f\"âš ï¸ Widget display issue: {display_error}\")\n            print(\"ðŸ“Š Using text-based visualization instead\")\n        \n        # Show natural relationships discovered\n        print(\"ðŸŒ¿ Natural relationships discovered:\")\n        for edge in G.edges(data=True):\n            source, target, data = edge\n            relationship = data.get('relationship', 'connected to')\n            print(f\"   â€¢ {source} {relationship} {target}\")\n        \n    except ImportError:\n        print(\"âŒ yfiles_jupyter_graphs not available\")\n        print(\"ðŸ’¡ Install with: pip install yfiles_jupyter_graphs\")\n        \n        # Set G for text display\n        G = knowledge_graph['graph']\n        \n    except Exception as e:\n        print(f\"âŒ yFiles visualization failed: {e}\")\n        print(\"ðŸ’¡ Try restarting the runtime and running all cells again\")\n        \n        # Set G for text display\n        G = knowledge_graph['graph']\n    \n    # Enhanced text-based summary (always shown for reliability)\n    print(f\"\\nðŸ“Š DETAILED GRAPH STRUCTURE:\")\n    G = knowledge_graph['graph']\n    print(f\"   ðŸ”— Total Nodes: {G.number_of_nodes()}\")\n    print(f\"   ðŸ“Š Total Edges: {G.number_of_edges()}\")\n    print(f\"   ðŸŒ¿ Discovery Method: Natural LLM analysis\")\n    \n    print(f\"\\nðŸŒ¿ All Discovered Concepts:\")\n    for i, node in enumerate(G.nodes(), 1):\n        importance = G.nodes[node].get('importance', 'medium')\n        label = G.nodes[node].get('label', node)\n        node_type = G.nodes[node].get('type', 'natural_concept')\n        importance_emoji = \"ðŸ”´\" if importance == 'high' else \"ðŸ”µ\" if importance == 'medium' else \"âšª\"\n        print(f\"   {i:2d}. {importance_emoji} {node}\")\n        print(f\"       ðŸ“ {label}\")\n        print(f\"       ðŸ·ï¸ Type: {node_type}, Importance: {importance}\")\n    \n    print(f\"\\nðŸ”— All Natural Relationships:\")\n    for i, edge in enumerate(G.edges(data=True), 1):\n        source, target, data = edge\n        relationship = data.get('relationship', 'connected to')\n        print(f\"   {i:2d}. {source}\")\n        print(f\"       âž¡ï¸ [{relationship}] âž¡ï¸\")\n        print(f\"       {target}\")\n    \n    # Print comprehensive summary\n    print(f\"\\nðŸ“Š NATURAL KNOWLEDGE GRAPH SUMMARY:\")\n    print(f\"   ðŸ“„ Paper: {knowledge_graph.get('paper_title', 'Unknown')}\")\n    print(f\"   ðŸŒ¿ Discovery method: {knowledge_graph['stats'].get('discovery_method', 'natural')}\")\n    print(f\"   ðŸ”— Naturally discovered nodes: {knowledge_graph['stats']['nodes']}\")\n    print(f\"   ðŸ“Š Natural relationships: {knowledge_graph['stats']['edges']}\")\n    print(f\"   ðŸ”¤ Vector store documents: {len(documents) if 'documents' in locals() else 0}\")\n    print(f\"   ðŸ—„ï¸ Vector store: {'âœ… Created' if 'vector_store' in locals() and vector_store else 'âŒ Not created'}\")\n    \n    # Show analysis preview\n    analysis = knowledge_graph.get('complete_analysis', '')\n    if analysis:\n        print(f\"\\nðŸ“ LLM ANALYSIS PREVIEW:\")\n        print(f\"   {analysis[:300]}...\")\n    \nelse:\n    print(\"âŒ No natural knowledge graph to visualize\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ðŸ’¾ Save Complete Analysis, Knowledge Graph, and Citations\n\nif complete_analysis and knowledge_graph:\n    import json\n    import pickle\n    from datetime import datetime\n    \n    print(\"ðŸ’¾ Saving natural analysis, knowledge graph, and citation data...\")\n    \n    # Create timestamp for unique filenames\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    paper_name = (paper_title_final or 'unknown_paper')[:30].replace(\" \", \"_\").replace(\"/\", \"_\")\n    base_filename = f\"{paper_name}_{timestamp}\"\n    \n    # 1. Save complete natural analysis as text file\n    analysis_file = f\"{base_filename}_analysis.txt\"\n    with open(analysis_file, 'w', encoding='utf-8') as f:\n        f.write(f\"# Natural Analysis of: {paper_title_final}\\n\\n\")\n        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n        f.write(complete_analysis)\n    print(f\"âœ… Natural analysis saved: {analysis_file}\")\n    \n    # 2. Save citation data with database links\n    if 'citation_data' in locals() and citation_data:\n        citation_file = f\"{base_filename}_citations.json\"\n        with open(citation_file, 'w', encoding='utf-8') as f:\n            json.dump(citation_data, f, indent=2, default=str)\n        print(f\"âœ… Citation data saved: {citation_file}\")\n        \n        # Save database-ready entry\n        if 'database_entry' in locals() and database_entry:\n            db_file = f\"{base_filename}_database_entry.json\"\n            with open(db_file, 'w', encoding='utf-8') as f:\n                json.dump(database_entry, f, indent=2, default=str)\n            print(f\"âœ… Database entry saved: {db_file}\")\n    \n    # 3. Save graph as GraphML (standard format, works with many tools)\n    graph_file = f\"{base_filename}_graph.graphml\"\n    import networkx as nx\n    nx.write_graphml(knowledge_graph['graph'], graph_file)\n    print(f\"âœ… Graph saved: {graph_file}\")\n    \n    # 4. Save complete knowledge graph as pickle (Python objects)\n    kg_file = f\"{base_filename}_knowledge_graph.pkl\"\n    with open(kg_file, 'wb') as f:\n        pickle.dump(knowledge_graph, f)\n    print(f\"âœ… Complete KG saved: {kg_file}\")\n    \n    # 5. Save paper metadata and processing info\n    metadata_file = f\"{base_filename}_metadata.json\"\n    metadata = {\n        \"title\": paper_title_final or 'Unknown',\n        \"timestamp\": timestamp,\n        \"content_length\": len(paper_content) if paper_content else 0,\n        \"analysis_length\": len(complete_analysis),\n        \"graph_nodes\": knowledge_graph['stats']['nodes'],\n        \"graph_edges\": knowledge_graph['stats']['edges'],\n        \"discovery_method\": knowledge_graph['stats'].get('discovery_method', 'natural'),\n        \"file_path\": paper_path if 'paper_path' in locals() and paper_path != \"sample_data\" else \"sample_data\",\n        \"mode\": \"sample_data\" if USE_SAMPLE_DATA else \"real_pdf\",\n        \"citations_extracted\": 'citation_data' in locals() and citation_data is not None,\n        \"citation_count\": len(citation_data['inline_citations']) if 'citation_data' in locals() and citation_data else 0,\n        \"reference_count\": len(citation_data['reference_list']) if 'citation_data' in locals() and citation_data else 0,\n        \"database_ready\": 'database_entry' in locals() and database_entry is not None\n    }\n    \n    if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n        # Save text content for real papers\n        text_file = f\"{base_filename}_content.txt\"\n        with open(text_file, 'w', encoding='utf-8') as f:\n            f.write(text_content)\n        metadata[\"content_file\"] = text_file\n        print(f\"âœ… Text content saved: {text_file}\")\n    \n    with open(metadata_file, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    print(f\"âœ… Metadata saved: {metadata_file}\")\n    \n    # 6. Create a comprehensive report\n    report_file = f\"{base_filename}_report.md\"\n    with open(report_file, 'w') as f:\n        f.write(f\"# Complete Paper Analysis Report\\n\\n\")\n        f.write(f\"**Paper:** {paper_title_final or 'Unknown'}\\n\")\n        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"**Mode:** {'Sample Data' if USE_SAMPLE_DATA else 'Real PDF'}\\n\\n\")\n        \n        f.write(f\"## Natural Analysis\\n\\n\")\n        f.write(f\"{complete_analysis}\\n\\n\")\n        \n        # Add citation analysis section\n        if 'citation_data' in locals() and citation_data:\n            f.write(f\"## Citation Analysis\\n\\n\")\n            f.write(f\"- **Total Citations:** {len(citation_data['inline_citations'])}\\n\")\n            f.write(f\"- **Total References:** {len(citation_data['reference_list'])}\\n\")\n            f.write(f\"- **Citation Density:** {citation_data['citation_density']:.4f} citations/word\\n\")\n            f.write(f\"- **Database Document ID:** {citation_data['paper_metadata']['document_id']}\\n\\n\")\n            \n            if citation_data['inline_citations']:\n                f.write(f\"### Sample Inline Citations\\n\\n\")\n                for i, citation in enumerate(citation_data['inline_citations'][:5], 1):\n                    f.write(f\"{i}. **[{citation['type']}]** `{citation['text']}` (Line {citation['line_number']})\\n\")\n                    f.write(f\"   - Context: ...{citation['context'][:100]}...\\n\\n\")\n            \n            if citation_data['reference_list']:\n                f.write(f\"### Sample References\\n\\n\")\n                for i, ref in enumerate(citation_data['reference_list'][:5], 1):\n                    f.write(f\"{i}. {ref['full_text'][:150]}...\\n\")\n                    if ref['year']:\n                        f.write(f\"   - Year: {ref['year']}\\n\")\n                    if ref['title']:\n                        f.write(f\"   - Title: {ref['title']}\\n\")\n                    f.write(f\"\\n\")\n        \n        f.write(f\"## Knowledge Graph Statistics\\n\\n\")\n        f.write(f\"- **Content Length:** {len(paper_content) if paper_content else 0:,} characters\\n\")\n        f.write(f\"- **Analysis Length:** {len(complete_analysis):,} characters\\n\")\n        f.write(f\"- **Graph Nodes:** {knowledge_graph['stats']['nodes']}\\n\")\n        f.write(f\"- **Graph Edges:** {knowledge_graph['stats']['edges']}\\n\")\n        f.write(f\"- **Discovery Method:** {knowledge_graph['stats'].get('discovery_method', 'natural')}\\n\\n\")\n        \n        # Show discovered concepts if available\n        G = knowledge_graph['graph']\n        if G.number_of_nodes() > 0:\n            f.write(f\"## Naturally Discovered Concepts\\n\\n\")\n            for node in G.nodes():\n                importance = G.nodes[node].get('importance', 'medium')\n                label = G.nodes[node].get('label', node)\n                f.write(f\"- **{node}**: {label} ({importance} importance)\\n\")\n            f.write(f\"\\n\")\n            \n            f.write(f\"## Natural Relationships\\n\\n\")\n            for edge in G.edges(data=True):\n                source, target, data = edge\n                relationship = data.get('relationship', 'connected to')\n                f.write(f\"- {source} **{relationship}** {target}\\n\")\n            f.write(f\"\\n\")\n        \n        f.write(f\"## Files Generated\\n\\n\")\n        f.write(f\"- `{analysis_file}` - Natural analysis in text format\\n\")\n        if 'citation_data' in locals() and citation_data:\n            f.write(f\"- `{citation_file}` - Complete citation data with locations\\n\")\n            if 'database_entry' in locals() and database_entry:\n                f.write(f\"- `{db_file}` - Database-ready entry with citations\\n\")\n        f.write(f\"- `{graph_file}` - Knowledge graph in GraphML format\\n\")\n        f.write(f\"- `{kg_file}` - Complete knowledge graph (Python pickle)\\n\")\n        f.write(f\"- `{metadata_file}` - Processing metadata\\n\")\n        if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n            f.write(f\"- `{text_file}` - Extracted text content\\n\")\n        f.write(f\"- `{report_file}` - This comprehensive report\\n\\n\")\n        \n        # Add database integration instructions\n        if 'database_entry' in locals() and database_entry:\n            f.write(f\"## Database Integration\\n\\n\")\n            f.write(f\"This analysis is ready for database storage:\\n\\n\")\n            f.write(f\"```python\\n\")\n            f.write(f\"# To store in database (requires main codebase setup):\\n\")\n            f.write(f\"from src import CitationDatabaseManager\\n\")\n            f.write(f\"import json\\n\\n\")\n            f.write(f\"# Load database entry\\n\")\n            f.write(f\"with open('{db_file}', 'r') as f:\\n\")\n            f.write(f\"    database_entry = json.load(f)\\n\\n\")\n            f.write(f\"# Store in database\\n\")\n            f.write(f\"db_manager = CitationDatabaseManager()\\n\")\n            f.write(f\"db_manager.store_paper_with_citations(database_entry)\\n\")\n            f.write(f\"```\\n\\n\")\n            f.write(f\"**Database Features:**\\n\")\n            f.write(f\"- Citation location tracking for literature reviews\\n\")\n            f.write(f\"- Cross-paper citation linking\\n\")\n            f.write(f\"- Reference verification and validation\\n\")\n            f.write(f\"- Literature corpus building for automated reviews\\n\")\n    \n    print(f\"âœ… Comprehensive report saved: {report_file}\")\n    \n    print(f\"\\nðŸ“Š SAVED FILES SUMMARY:\")\n    print(f\"ðŸ“ All files saved to: /content/\")\n    print(f\"ðŸ·ï¸ Base filename: {base_filename}\")\n    print(f\"ðŸ“„ Files created:\")\n    print(f\"   â€¢ {analysis_file} (Natural analysis)\")\n    if 'citation_data' in locals() and citation_data:\n        print(f\"   â€¢ {citation_file} (Citation data)\")\n        if 'database_entry' in locals() and database_entry:\n            print(f\"   â€¢ {db_file} (Database entry)\")\n    print(f\"   â€¢ {graph_file} (GraphML graph)\")\n    print(f\"   â€¢ {kg_file} (Python pickle)\")\n    print(f\"   â€¢ {metadata_file} (metadata)\")\n    if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n        print(f\"   â€¢ {text_file} (text content)\")\n    print(f\"   â€¢ {report_file} (comprehensive report)\")\n    \n    # 7. Download files option (Colab only)\n    if IN_COLAB:\n        print(f\"\\nðŸ“¥ DOWNLOAD FILES:\")\n        print(f\"Right-click files in the file panel to download\")\n        print(f\"Or run this code to download all at once:\")\n        print(f\"```python\")\n        print(f\"from google.colab import files\")\n        print(f\"files.download('{analysis_file}')\")\n        if 'citation_data' in locals() and citation_data:\n            print(f\"files.download('{citation_file}')\")\n            if 'database_entry' in locals() and database_entry:\n                print(f\"files.download('{db_file}')\")\n        print(f\"files.download('{graph_file}')\")\n        print(f\"files.download('{kg_file}')\")\n        print(f\"files.download('{metadata_file}')\")\n        if not USE_SAMPLE_DATA and 'text_content' in locals() and text_content:\n            print(f\"files.download('{text_file}')\")\n        print(f\"files.download('{report_file}')\")\n        print(f\"```\")\n    \n    # 8. How to reload the analysis\n    print(f\"\\nðŸ”„ TO RELOAD THIS ANALYSIS LATER:\")\n    print(f\"```python\")\n    print(f\"import pickle\")\n    print(f\"import json\")\n    print(f\"\")\n    print(f\"# Load natural analysis\")\n    print(f\"with open('{analysis_file}', 'r') as f:\")\n    print(f\"    complete_analysis = f.read()\")\n    print(f\"\")\n    if 'citation_data' in locals() and citation_data:\n        print(f\"# Load citation data\")\n        print(f\"with open('{citation_file}', 'r') as f:\")\n        print(f\"    citation_data = json.load(f)\")\n        print(f\"\")\n        if 'database_entry' in locals() and database_entry:\n            print(f\"# Load database entry\")\n            print(f\"with open('{db_file}', 'r') as f:\")\n            print(f\"    database_entry = json.load(f)\")\n            print(f\"\")\n    print(f\"# Load complete knowledge graph\")\n    print(f\"with open('{kg_file}', 'rb') as f:\")\n    print(f\"    knowledge_graph = pickle.load(f)\")\n    print(f\"\")\n    print(f\"# Load graph separately (if needed)\")\n    print(f\"import networkx as nx\")\n    print(f\"graph = nx.read_graphml('{graph_file}')\")\n    print(f\"```\")\n    \nelse:\n    print(\"âŒ No natural analysis to save\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸŽ‰ Complete Success!\n\nIf you see results above, you have successfully created a **complete natural knowledge graph system with citation tracking** using Ollama in Colab!\n\n### âœ… What You Accomplished:\n\n**Infrastructure:**\n- âœ… **Installed Ollama** in Google Colab environment\n- âœ… **Downloaded models** (llama3.1:8b + nomic-embed-text)\n- âœ… **Started server** successfully in background\n\n**Natural Knowledge Graph System:**\n- âœ… **Processed research paper** with PDF text extraction  \n- âœ… **Natural analysis** using local Ollama LLM without forced categories\n- âœ… **Citation extraction** with precise location tracking and database linking\n- âœ… **Created embeddings** with nomic-embed-text model\n- âœ… **Built vector store** with ChromaDB for semantic search\n- âœ… **Discovered knowledge graph** with natural concepts and relationships\n- âœ… **Interactive visualization** with yFiles organic layout\n- âœ… **Saved complete results** in multiple formats including citation data\n\n### ðŸ” Technical Stack Validated:\n\n**Local LLM Processing**: Ollama running on Colab T4 GPU  \n**Natural Analysis**: Open-ended paper understanding without categories  \n**Citation Tracking**: Precise location mapping with database-ready structures  \n**Vector Embeddings**: Semantic search capabilities over paper content  \n**Knowledge Graph**: Naturally discovered concepts and relationships  \n**Vector Store**: ChromaDB with persistent storage  \n**Database Integration**: Ready for PostgreSQL storage with citation linking  \n**Hybrid System**: Vector similarity + natural graph structure + citation tracking  \n**Interactive Visualization**: yFiles with organic layout and importance-based sizing  \n**Complete Save System**: Analysis, citations, GraphML, pickle, and database entries\n\n### ðŸ“š Citation System Features:\n\n**Citation Extraction:**\n- âœ… **Multiple formats** - numbered [1], author-year (Smith, 2020), superscriptÂ¹\n- âœ… **Precise locations** - character positions, line numbers, context\n- âœ… **Reference parsing** - automated reference list extraction\n- âœ… **Citation density** - statistical analysis of citation patterns\n\n**Database Integration:**\n- âœ… **Unique document IDs** for literature corpus building\n- âœ… **Cross-paper linking** ready for literature discovery\n- âœ… **Citation verification** against reference sources\n- âœ… **Literature review** preparation with precise citation tracking\n\n### ðŸš€ Next Steps:\n- Process multiple papers for cross-paper citation analysis\n- Build literature corpus with citation-linked knowledge graphs\n- Scale to literature collections with automated review generation\n- Use citation tracking for evidence-based literature synthesis\n- Connect to PostgreSQL database for persistent citation storage\n\n**You've built a complete literature analysis system!** ðŸŽ¯\n\nEach paper creates its own unique knowledge structure with precise citation tracking for literature review automation!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}