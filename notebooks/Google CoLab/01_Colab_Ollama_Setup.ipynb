{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Ollama Setup for Google Colab\n",
    "\n",
    "**Simple test to get Ollama running in Google Colab**\n",
    "\n",
    "This notebook just focuses on:\n",
    "- Installing Ollama in Colab\n",
    "- Starting the server\n",
    "- Downloading required models\n",
    "- Testing basic functionality\n",
    "\n",
    "**Requirements:** Enable GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "    \n",
    "    # Check GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected!\")\n",
    "        print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "else:\n",
    "    print(\"üè† Running locally (not Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"üì¶ Installing Ollama...\")\n",
    "    \n",
    "    # Download and install Ollama\n",
    "    !curl -fsSL https://ollama.ai/install.sh | sh\n",
    "    \n",
    "    print(\"‚úÖ Ollama installed!\")\n",
    "else:\n",
    "    print(\"üè† Assuming Ollama is already installed locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Start Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üöÄ Starting Ollama server...\")\n",
    "    \n",
    "    # Function to run Ollama serve in background\n",
    "    def run_ollama_serve():\n",
    "        os.system(\"ollama serve > /dev/null 2>&1 &\")\n",
    "    \n",
    "    # Start Ollama in a separate thread\n",
    "    ollama_thread = threading.Thread(target=run_ollama_serve, daemon=True)\n",
    "    ollama_thread.start()\n",
    "    \n",
    "    # Wait for server to start\n",
    "    print(\"‚è≥ Waiting for server to start...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Test if server is running\n",
    "    try:\n",
    "        result = !curl -s http://localhost:11434/api/version\n",
    "        if result:\n",
    "            print(\"‚úÖ Ollama server is running!\")\n",
    "            print(f\"   Version info: {result[0] if result else 'N/A'}\")\n",
    "        else:\n",
    "            print(\"‚ùå Server not responding\")\n",
    "    except:\n",
    "        print(\"‚ùå Failed to check server status\")\n",
    "        \n",
    "else:\n",
    "    print(\"üè† Assuming Ollama server is running locally\")\n",
    "    print(\"   (Run 'ollama serve' in terminal if needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"üì• Downloading models (this takes 5-10 minutes)...\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Download LLM model\n",
    "    print(\"üß† Downloading llama3.1:8b (main LLM)...\")\n",
    "    !ollama pull llama3.1:8b\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"üî§ Downloading nomic-embed-text (embeddings)...\")\n",
    "    !ollama pull nomic-embed-text\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"‚úÖ All models downloaded!\")\n",
    "    \n",
    "else:\n",
    "    print(\"üè† Check local models with: ollama list\")\n",
    "    print(\"   Download if needed: ollama pull llama3.1:8b && ollama pull nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Basic Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain for testing\n",
    "if IN_COLAB:\n",
    "    !pip install -q langchain-ollama\n",
    "\n",
    "# Test basic LLM functionality\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "    \n",
    "    print(\"üß™ Testing LLM connection...\")\n",
    "    \n",
    "    # Create LLM instance\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1:8b\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Simple test\n",
    "    response = llm.invoke(\"Say 'Hello from Colab!' and nothing else.\")\n",
    "    print(f\"‚úÖ LLM Response: {response.content}\")\n",
    "    \n",
    "    # Test embeddings\n",
    "    from langchain_ollama import OllamaEmbeddings\n",
    "    \n",
    "    print(\"üî§ Testing embeddings...\")\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    \n",
    "    test_embedding = embeddings.embed_query(\"This is a test.\")\n",
    "    print(f\"‚úÖ Embedding created: {len(test_embedding)} dimensions\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"üéâ SUCCESS! Ollama is working in Colab!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    print(\"üí° You may need to restart runtime and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "If everything worked:\n",
    "1. ‚úÖ Ollama is installed and running\n",
    "2. ‚úÖ Models are downloaded\n",
    "3. ‚úÖ LangChain can connect to Ollama\n",
    "\n",
    "**You're ready to process papers!**\n",
    "\n",
    "Now you can:\n",
    "- Run the paper processing notebook\n",
    "- Build knowledge graphs from research papers\n",
    "- Use the enhanced entity extraction system\n",
    "\n",
    "**Note:** If you restart the Colab runtime, you'll need to run this setup again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}