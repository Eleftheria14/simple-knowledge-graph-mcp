{
  "name": "DOCSGPT-NEO4J-INTEGRATION",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "docsgpt-integration",
        "responseMode": "lastNode",
        "responseData": "allEntries"
      },
      "id": "docsgpt-webhook",
      "name": "DocsGPT Integration Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [240, 300],
      "webhookId": "docsgpt-integration"
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "jsCode": "// Parse DocsGPT payload and extract document information\nconst webhookData = $input.all()[0].json;\n\n// DocsGPT sends different payload formats depending on operation\nlet documentData;\n\nif (webhookData.action === 'batch_process' && webhookData.files) {\n  // Batch processing mode\n  documentData = webhookData.files.map(file => ({\n    file_path: file.path || file.file_path,\n    filename: file.name || file.filename,\n    user_id: webhookData.user_id || 'unknown',\n    batch_id: webhookData.batch_id || `batch_${Date.now()}`,\n    processing_type: 'batch'\n  }));\n} else if (webhookData.file_path || webhookData.filename) {\n  // Single document processing\n  documentData = [{\n    file_path: webhookData.file_path || '/data/local/sample.pdf',\n    filename: webhookData.filename || 'sample.pdf',\n    user_id: webhookData.user_id || 'unknown',\n    batch_id: webhookData.batch_id || `single_${Date.now()}`,\n    processing_type: 'single'\n  }];\n} else {\n  // Test/fallback mode\n  documentData = [{\n    file_path: '/data/local/chemistry-paper.pdf/d4sc03921a.pdf',\n    filename: 'd4sc03921a.pdf',\n    user_id: 'test_user',\n    batch_id: `test_${Date.now()}`,\n    processing_type: 'test'\n  }];\n}\n\n// Process first document (extend for batch processing later)\nconst doc = documentData[0];\n\nreturn [{\n  json: {\n    file_path: doc.file_path,\n    filename: doc.filename,\n    user_id: doc.user_id,\n    batch_id: doc.batch_id,\n    processing_type: doc.processing_type,\n    processing_started: new Date().toISOString(),\n    total_documents: documentData.length,\n    current_document: 1,\n    workflow_version: 'docsgpt_neo4j_integration_v1'\n  }\n}];"
      },
      "id": "parse-docsgpt-payload",
      "name": "Parse DocsGPT Payload",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [460, 300]
    },
    {
      "parameters": {
        "filePath": "={{ $json.file_path }}",
        "resultType": "markdown",
        "parsingMode": "premium"
      },
      "id": "llamaparse-premium",
      "name": "LlamaParse Premium",
      "type": "n8n-nodes-llamacloud.llamaParse",
      "typeVersion": 1,
      "position": [680, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.groq.com/openai/v1/chat/completions",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "Bearer GROQ_API_KEY_PLACEHOLDER"
            },
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "contentType": "json",
        "jsonBody": "={{ \n  const markdownContent = $('LlamaParse Premium').first().json;\n  const fileData = $('Parse DocsGPT Payload').first().json;\n  \n  return {\n    model: \"llama-3.1-70b-versatile\",\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are an expert entity extraction system for academic papers and research documents. Extract entities, relationships, and structured data from the provided content. Focus on research concepts, methodologies, authors, institutions, and key findings. Return ONLY valid JSON format without any markdown formatting.\"\n      },\n      {\n        role: \"user\",\n        content: `Extract comprehensive knowledge from this research document:\\n\\n**DOCUMENT**: ${fileData.filename}\\n\\n**EXTRACT**:\\n1. **ENTITIES**: People (authors, researchers), Organizations (institutions, companies), Concepts (theories, methods, technologies), Materials (chemicals, compounds), Measurements (values, metrics)\\n2. **RELATIONSHIPS**: How entities connect with confidence scores (0.5-1.0)\\n3. **STRUCTURED_DATA**: Tables, figures, equations, code blocks\\n\\n**CITATION**: Create a citation entity for this document with complete bibliographic information.\\n\\nReturn JSON format:\\n{\\n  \"citation\": {\"id\": \"doc_citation\", \"title\": \"document_title\", \"authors\": [], \"year\": \"YYYY\", \"type\": \"research_paper\", \"doi\": \"\", \"abstract\": \"brief_summary\"},\\n  \"entities\": [{\"id\": \"unique_id\", \"name\": \"display_name\", \"type\": \"person|organization|concept|method|material|measurement\", \"description\": \"brief_description\", \"confidence\": 0.8, \"source_location\": \"approximate_location_in_text\"}],\\n  \"relationships\": [{\"source_entity\": \"entity_id\", \"target_entity\": \"entity_id\", \"relationship_type\": \"describes_relationship\", \"context\": \"text_snippet_showing_connection\", \"confidence\": 0.8}],\\n  \"structured_elements\": {\"tables\": [], \"figures\": [], \"equations\": [], \"key_findings\": []}\\n}\\n\\nDocument content:\\n${markdownContent.substring(0, 12000)}`\n      }\n    ],\n    temperature: 0.1,\n    max_tokens: 4000\n  };\n}}"
      },
      "id": "groq-entity-extraction",
      "name": "Groq Entity Extraction",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [900, 300]
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "jsCode": "// Process Groq response and prepare data for Neo4j storage\nconst fileData = $('Parse DocsGPT Payload').first().json;\nconst markdownContent = $('LlamaParse Premium').first().json;\nconst groqResponse = $('Groq Entity Extraction').first().json;\n\n// Parse AI extraction results\nlet extractedData = { citation: {}, entities: [], relationships: [], structured_elements: {} };\ntry {\n  if (groqResponse.choices && groqResponse.choices[0] && groqResponse.choices[0].message) {\n    const content = groqResponse.choices[0].message.content;\n    extractedData = JSON.parse(content);\n  }\n} catch (error) {\n  console.log('Error parsing Groq response:', error.message);\n  console.log('Raw response:', JSON.stringify(groqResponse, null, 2));\n}\n\n// Create systematic text chunks for vector storage\nfunction createSystematicChunks(text, chunkSize = 300, overlap = 75) {\n  if (!text || typeof text !== 'string') return [];\n  \n  const words = text.split(/\\s+/);\n  const chunks = [];\n  \n  for (let i = 0; i < words.length; i += (chunkSize - overlap)) {\n    const chunk = words.slice(i, i + chunkSize).join(' ');\n    if (chunk.trim().length > 50) {\n      chunks.push({\n        id: `${fileData.filename.replace('.pdf', '').replace(/\\s+/g, '_').toLowerCase()}_chunk_${chunks.length + 1}`,\n        content: chunk,\n        word_count: chunk.split(/\\s+/).length,\n        start_word: i,\n        end_word: Math.min(i + chunkSize, words.length),\n        document_id: fileData.filename.replace('.pdf', '').replace(/\\s+/g, '_').toLowerCase(),\n        chunk_index: chunks.length,\n        created_at: new Date().toISOString()\n      });\n    }\n  }\n  \n  return chunks;\n}\n\nconst textChunks = createSystematicChunks(markdownContent);\n\n// Prepare complete Neo4j data structure\nconst neo4jData = {\n  // Document node\n  document: {\n    id: fileData.filename.replace('.pdf', '').replace(/\\s+/g, '_').toLowerCase(),\n    title: extractedData.citation?.title || fileData.filename.replace('.pdf', ''),\n    filename: fileData.filename,\n    file_path: fileData.file_path,\n    user_id: fileData.user_id,\n    batch_id: fileData.batch_id,\n    authors: extractedData.citation?.authors || [],\n    year: extractedData.citation?.year || new Date().getFullYear(),\n    document_type: extractedData.citation?.type || 'research_paper',\n    abstract: extractedData.citation?.abstract || '',\n    doi: extractedData.citation?.doi || '',\n    processed_at: new Date().toISOString(),\n    processing_method: 'docsgpt_neo4j_integration',\n    llm_used: 'groq_llama_3_1_70b',\n    total_chunks: textChunks.length,\n    total_entities: extractedData.entities?.length || 0,\n    total_relationships: extractedData.relationships?.length || 0\n  },\n  \n  // Entity nodes\n  entities: (extractedData.entities || []).map((entity, index) => ({\n    id: entity.id || `entity_${index}`,\n    name: entity.name || 'Unknown Entity',\n    type: entity.type || 'concept',\n    description: entity.description || '',\n    confidence: entity.confidence || 0.8,\n    source_location: entity.source_location || 'unknown',\n    document_id: fileData.filename.replace('.pdf', '').replace(/\\s+/g, '_').toLowerCase(),\n    extracted_at: new Date().toISOString()\n  })),\n  \n  // Relationship edges\n  relationships: (extractedData.relationships || []).map((rel, index) => ({\n    id: `rel_${index}`,\n    source_entity: rel.source_entity || `entity_${index}_source`,\n    target_entity: rel.target_entity || `entity_${index}_target`,\n    relationship_type: rel.relationship_type || 'RELATED_TO',\n    context: rel.context || '',\n    confidence: rel.confidence || 0.8,\n    document_id: fileData.filename.replace('.pdf', '').replace(/\\s+/g, '_').toLowerCase(),\n    created_at: new Date().toISOString()\n  })),\n  \n  // Text chunks for vector storage\n  chunks: textChunks,\n  \n  // Structured elements\n  structured_elements: extractedData.structured_elements || {}\n};\n\nreturn [{ json: neo4jData }];"
      },
      "id": "process-neo4j-data",
      "name": "Process & Prepare Neo4j Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1120, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "// Create Document node with properties\nCREATE (doc:Document {\n  id: $document.id,\n  title: $document.title,\n  filename: $document.filename,\n  file_path: $document.file_path,\n  user_id: $document.user_id,\n  batch_id: $document.batch_id,\n  authors: $document.authors,\n  year: $document.year,\n  document_type: $document.document_type,\n  abstract: $document.abstract,\n  doi: $document.doi,\n  processed_at: datetime($document.processed_at),\n  processing_method: $document.processing_method,\n  llm_used: $document.llm_used,\n  total_chunks: $document.total_chunks,\n  total_entities: $document.total_entities,\n  total_relationships: $document.total_relationships\n})\n\n// Create Entity nodes\nUNWIND $entities AS entity\nCREATE (e:Entity {\n  id: entity.id,\n  name: entity.name,\n  type: entity.type,\n  description: entity.description,\n  confidence: entity.confidence,\n  source_location: entity.source_location,\n  document_id: entity.document_id,\n  extracted_at: datetime(entity.extracted_at)\n})\n\n// Create Chunk nodes\nUNWIND $chunks AS chunk\nCREATE (c:Chunk {\n  id: chunk.id,\n  content: chunk.content,\n  word_count: chunk.word_count,\n  start_word: chunk.start_word,\n  end_word: chunk.end_word,\n  document_id: chunk.document_id,\n  chunk_index: chunk.chunk_index,\n  created_at: datetime(chunk.created_at)\n})\n\n// Create Document-Chunk relationships\nMATCH (doc:Document {id: $document.id})\nMATCH (c:Chunk {document_id: $document.id})\nCREATE (doc)-[:CONTAINS]->(c)\n\n// Create Document-Entity relationships\nMATCH (doc:Document {id: $document.id})\nMATCH (e:Entity {document_id: $document.id})\nCREATE (doc)-[:EXTRACTED_FROM]->(e)\n\nRETURN doc.id as document_id, \n       count(distinct e) as entities_created, \n       count(distinct c) as chunks_created",
        "parameters": "={{ $json }}"
      },
      "id": "store-graph-structure",
      "name": "Store Graph Structure",
      "type": "n8n-nodes-neo4j",
      "typeVersion": 1,
      "position": [1340, 200],
      "credentials": {
        "neo4j": {
          "id": "neo4j-credentials",
          "name": "Neo4j Database"
        }
      }
    },
    {
      "parameters": {
        "operation": "addDocuments",
        "documents": "={{ $json.chunks.map(chunk => chunk.content) }}",
        "metadatas": "={{ $json.chunks.map(chunk => ({ chunk_id: chunk.id, document_id: chunk.document_id, chunk_index: chunk.chunk_index, word_count: chunk.word_count })) }}",
        "ids": "={{ $json.chunks.map(chunk => chunk.id) }}"
      },
      "id": "store-vector-embeddings",
      "name": "Store Vector Embeddings",
      "type": "n8n-nodes-neo4j",
      "typeVersion": 1,
      "position": [1340, 400],
      "credentials": {
        "neo4j": {
          "id": "neo4j-vector-credentials",
          "name": "Neo4j Vector Store"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "// Create Entity-Entity relationships\nUNWIND $relationships AS rel\nMATCH (source:Entity {id: rel.source_entity})\nMATCH (target:Entity {id: rel.target_entity})\nCREATE (source)-[r:RELATED_TO {\n  id: rel.id,\n  type: rel.relationship_type,\n  context: rel.context,\n  confidence: rel.confidence,\n  document_id: rel.document_id,\n  created_at: datetime(rel.created_at)\n}]->(target)\n\n// Create Chunk-Entity relationships (entities mentioned in chunks)\nMATCH (c:Chunk), (e:Entity)\nWHERE c.document_id = e.document_id \nAND toLower(c.content) CONTAINS toLower(e.name)\nCREATE (c)-[:MENTIONS {confidence: 0.7, method: 'text_matching'}]->(e)\n\nRETURN count(distinct r) as relationships_created,\n       count(distinct c) as chunk_entity_links",
        "parameters": "={{ { relationships: $json.relationships } }}"
      },
      "id": "create-graph-relationships",
      "name": "Create Graph Relationships",
      "type": "n8n-nodes-neo4j",
      "typeVersion": 1,
      "position": [1560, 300],
      "credentials": {
        "neo4j": {
          "id": "neo4j-credentials",
          "name": "Neo4j Database"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "jsCode": "// Format response for DocsGPT integration\nconst fileData = $('Parse DocsGPT Payload').first().json;\nconst neo4jData = $('Process & Prepare Neo4j Data').first().json;\nconst graphResult = $('Store Graph Structure').first().json;\nconst vectorResult = $('Store Vector Embeddings').first().json;\nconst relationshipResult = $('Create Graph Relationships').first().json;\n\n// Calculate processing statistics\nconst processingStats = {\n  entities_extracted: neo4jData.entities?.length || 0,\n  relationships_extracted: neo4jData.relationships?.length || 0,\n  text_chunks_created: neo4jData.chunks?.length || 0,\n  processing_time_seconds: Math.round((new Date() - new Date(fileData.processing_started)) / 1000),\n  storage_operations_completed: 3 // graph, vector, relationships\n};\n\n// DocsGPT expected response format\nconst docsgptResponse = {\n  success: true,\n  message: \"Document processed successfully with knowledge graph integration\",\n  \n  // Document information\n  document: {\n    filename: fileData.filename,\n    file_path: fileData.file_path,\n    batch_id: fileData.batch_id,\n    user_id: fileData.user_id,\n    processing_type: fileData.processing_type\n  },\n  \n  // Processing results\n  results: {\n    document_id: neo4jData.document.id,\n    title: neo4jData.document.title,\n    authors: neo4jData.document.authors,\n    abstract: neo4jData.document.abstract,\n    processing_method: 'neo4j_vector_integration',\n    llm_used: 'groq_llama_3_1_70b',\n    ...processingStats\n  },\n  \n  // Knowledge graph statistics\n  knowledge_graph: {\n    nodes_created: {\n      documents: 1,\n      entities: processingStats.entities_extracted,\n      chunks: processingStats.text_chunks_created\n    },\n    relationships_created: {\n      entity_relationships: processingStats.relationships_extracted,\n      document_contains_chunks: processingStats.text_chunks_created,\n      document_extracted_entities: processingStats.entities_extracted,\n      chunk_mentions_entities: relationshipResult?.chunk_entity_links || 0\n    },\n    vector_operations: {\n      documents_embedded: 1,\n      chunks_embedded: processingStats.text_chunks_created,\n      similarity_search_ready: true\n    }\n  },\n  \n  // Query capabilities\n  query_examples: {\n    semantic_search: \"Find documents similar to: 'machine learning applications'\",\n    entity_search: \"Show all papers mentioning: 'neural networks'\",\n    relationship_discovery: \"Find concepts related to: 'data analysis'\",\n    hybrid_queries: \"Combine graph traversal with vector similarity search\"\n  },\n  \n  // API endpoints for DocsGPT integration\n  api_endpoints: {\n    search_knowledge_graph: \"/api/knowledge/search\",\n    get_document_entities: `/api/knowledge/document/${neo4jData.document.id}/entities`,\n    similarity_search: \"/api/knowledge/similarity\",\n    graph_visualization: `/api/knowledge/graph/${neo4jData.document.id}`\n  },\n  \n  // Processing metadata\n  metadata: {\n    processed_at: new Date().toISOString(),\n    processing_duration: `${processingStats.processing_time_seconds}s`,\n    workflow_version: 'docsgpt_neo4j_integration_v1',\n    neo4j_integration: 'enabled',\n    vector_store: 'neo4j_native',\n    batch_processing_ready: true\n  }\n};\n\nreturn { json: docsgptResponse };"
      },
      "id": "format-docsgpt-response",
      "name": "Format DocsGPT Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1780, 300]
    }
  ],
  "connections": {
    "DocsGPT Integration Webhook": {
      "main": [[{"node": "Parse DocsGPT Payload", "type": "main", "index": 0}]]
    },
    "Parse DocsGPT Payload": {
      "main": [[{"node": "LlamaParse Premium", "type": "main", "index": 0}]]
    },
    "LlamaParse Premium": {
      "main": [[{"node": "Groq Entity Extraction", "type": "main", "index": 0}]]
    },
    "Groq Entity Extraction": {
      "main": [[{"node": "Process & Prepare Neo4j Data", "type": "main", "index": 0}]]
    },
    "Process & Prepare Neo4j Data": {
      "main": [[
        {"node": "Store Graph Structure", "type": "main", "index": 0},
        {"node": "Store Vector Embeddings", "type": "main", "index": 0}
      ]]
    },
    "Store Graph Structure": {
      "main": [[{"node": "Create Graph Relationships", "type": "main", "index": 0}]]
    },
    "Store Vector Embeddings": {
      "main": [[{"node": "Create Graph Relationships", "type": "main", "index": 1}]]
    },
    "Create Graph Relationships": {
      "main": [[{"node": "Format DocsGPT Response", "type": "main", "index": 0}]]
    }
  },
  "active": false,
  "settings": {},
  "id": "docsgpt-neo4j-integration",
  "tags": ["docsgpt", "neo4j", "vector-store", "knowledge-graph", "integration", "batch-processing", "groq", "llamaparse"]
}