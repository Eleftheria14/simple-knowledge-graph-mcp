{
  "name": "DocsGPT-Community-Nodes-Integration",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "docsgpt-process",
        "responseMode": "lastNode",
        "responseData": "allEntries"
      },
      "id": "docsgpt-webhook",
      "name": "DocsGPT Document Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [240, 300],
      "webhookId": "docsgpt-process"
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "jsCode": "// Parse DocsGPT payload and prepare for processing\nconst webhookData = $input.all()[0].json;\n\n// Handle different DocsGPT payload formats\nlet documentData;\nif (webhookData.action === 'batch_process' && webhookData.files) {\n  // Batch processing mode\n  documentData = webhookData.files.map(file => ({\n    file_path: file.path || file.file_path,\n    filename: file.name || file.filename,\n    user_id: webhookData.user_id || 'unknown',\n    batch_id: webhookData.batch_id || `batch_${Date.now()}`,\n    processing_type: 'batch'\n  }));\n} else if (webhookData.file_path || webhookData.filename) {\n  // Single document processing\n  documentData = [{\n    file_path: webhookData.file_path,\n    filename: webhookData.filename,\n    user_id: webhookData.user_id || 'unknown',\n    batch_id: webhookData.batch_id || `single_${Date.now()}`,\n    processing_type: 'single'\n  }];\n} else {\n  // Test mode with sample document\n  documentData = [{\n    file_path: '/data/local/sample-research-paper.pdf',\n    filename: 'sample-research-paper.pdf',\n    user_id: 'test_user',\n    batch_id: `test_${Date.now()}`,\n    processing_type: 'test'\n  }];\n}\n\n// Process first document (extend for batch later)\nconst doc = documentData[0];\nconst documentId = doc.filename.replace('.pdf', '').replace(/\\s+/g, '_').toLowerCase();\n\nreturn [{\n  json: {\n    file_path: doc.file_path,\n    filename: doc.filename,\n    document_id: documentId,\n    user_id: doc.user_id,\n    batch_id: doc.batch_id,\n    processing_type: doc.processing_type,\n    processing_started: new Date().toISOString(),\n    total_documents: documentData.length,\n    current_document: 1,\n    workflow_version: 'community_nodes_v1',\n    llamaparse_mode: 'premium',\n    neo4j_integration: 'vector_store_node'\n  }\n}];"
      },
      "id": "parse-payload",
      "name": "Parse DocsGPT Payload",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [460, 300]
    },
    {
      "parameters": {
        "apiKey": "={{ $vars.LLAMAPARSE_API_KEY }}",
        "file": "={{ $json.file_path }}",
        "resultType": "markdown",
        "parsingInstruction": "Extract all text, tables, figures, and document structure from this academic paper. Preserve formatting and maintain section hierarchy.",
        "premiumMode": true,
        "targetPages": "",
        "useCache": true
      },
      "id": "llamaparse-node",
      "name": "LlamaParse Community Node",
      "type": "n8n-nodes-llamacloud.llamaParse",
      "typeVersion": 1,
      "position": [680, 300],
      "credentials": {
        "llamaCloudApi": {
          "id": "llamaparse_credentials",
          "name": "LlamaParse API"
        }
      }
    },
    {
      "parameters": {
        "model": "llama-3.1-70b-versatile",
        "messages": {
          "values": [
            {
              "content": "You are an expert entity extraction system for academic papers and research documents. Extract entities, relationships, and structured data from the provided content. Focus on research concepts, methodologies, authors, institutions, and key findings. Return ONLY valid JSON format without any markdown formatting or code blocks.",
              "role": "system"
            },
            {
              "content": "=Extract comprehensive knowledge from this research document:\n\n**DOCUMENT**: {{ $('Parse DocsGPT Payload').first().json.filename }}\n\n**EXTRACT**:\n1. **ENTITIES**: People (authors, researchers), Organizations (institutions, companies), Concepts (theories, methods, technologies), Materials (chemicals, compounds), Measurements (values, metrics)\n2. **RELATIONSHIPS**: How entities connect with confidence scores (0.5-1.0)\n3. **STRUCTURED_DATA**: Tables, figures, equations, code blocks\n\n**CITATION**: Create a citation entity for this document with complete bibliographic information.\n\nReturn JSON format:\n{\n  \"citation\": {\"id\": \"doc_citation\", \"title\": \"document_title\", \"authors\": [], \"year\": \"YYYY\", \"type\": \"research_paper\", \"doi\": \"\", \"abstract\": \"brief_summary\"},\n  \"entities\": [{\"id\": \"unique_id\", \"name\": \"display_name\", \"type\": \"person|organization|concept|method|material|measurement\", \"description\": \"brief_description\", \"confidence\": 0.8, \"source_location\": \"approximate_location_in_text\"}],\n  \"relationships\": [{\"source_entity\": \"entity_id\", \"target_entity\": \"entity_id\", \"relationship_type\": \"describes_relationship\", \"context\": \"text_snippet_showing_connection\", \"confidence\": 0.8}],\n  \"structured_elements\": {\"tables\": [], \"figures\": [], \"equations\": [], \"key_findings\": []}\n}\n\nDocument content:\n{{ $('LlamaParse Community Node').first().json.text.substring(0, 12000) }}",
              "role": "user"
            }
          ]
        },
        "options": {
          "temperature": 0.1,
          "maxTokens": 4000
        }
      },
      "id": "groq-extraction",
      "name": "Groq Entity Extraction",
      "type": "n8n-nodes-base.llm",
      "typeVersion": 1,
      "position": [900, 300],
      "credentials": {
        "groqApi": {
          "id": "groq_credentials",
          "name": "Groq API"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "jsCode": "// Process LLM response and prepare data for Neo4j Vector Store\nconst fileData = $('Parse DocsGPT Payload').first().json;\nconst llamaParseResponse = $('LlamaParse Community Node').first().json;\nconst groqResponse = $('Groq Entity Extraction').first().json;\n\n// Extract markdown content from LlamaParse\nlet markdownContent = '';\nif (llamaParseResponse && llamaParseResponse.text) {\n  markdownContent = llamaParseResponse.text;\n} else if (typeof llamaParseResponse === 'string') {\n  markdownContent = llamaParseResponse;\n} else {\n  markdownContent = JSON.stringify(llamaParseResponse);\n}\n\n// Parse AI extraction results\nlet extractedData = { citation: {}, entities: [], relationships: [], structured_elements: {} };\ntry {\n  if (groqResponse && groqResponse.text) {\n    extractedData = JSON.parse(groqResponse.text);\n  } else if (typeof groqResponse === 'string') {\n    extractedData = JSON.parse(groqResponse);\n  }\n} catch (error) {\n  console.log('Error parsing Groq response:', error.message);\n  // Fallback structured data\n  extractedData = {\n    citation: {\n      id: 'doc_citation',\n      title: fileData.filename.replace('.pdf', ''),\n      authors: ['Unknown'],\n      year: new Date().getFullYear().toString(),\n      type: 'research_paper'\n    },\n    entities: [{\n      id: 'default_entity',\n      name: 'Document Content',\n      type: 'concept',\n      description: 'Fallback entity for document processing',\n      confidence: 0.7\n    }],\n    relationships: [],\n    structured_elements: { tables: [], figures: [], equations: [], key_findings: [] }\n  };\n}\n\n// Create systematic text chunks for vector storage\nfunction createSystematicChunks(text, chunkSize = 300, overlap = 75) {\n  if (!text || typeof text !== 'string') return [];\n  \n  const words = text.split(/\\s+/);\n  const chunks = [];\n  \n  for (let i = 0; i < words.length; i += (chunkSize - overlap)) {\n    const chunk = words.slice(i, i + chunkSize).join(' ');\n    if (chunk.trim().length > 50) {\n      chunks.push({\n        id: `${fileData.document_id}_chunk_${chunks.length + 1}`,\n        content: chunk,\n        word_count: chunk.split(/\\s+/).length,\n        document_id: fileData.document_id,\n        chunk_index: chunks.length,\n        metadata: {\n          document_title: extractedData.citation?.title || fileData.filename,\n          processing_method: 'community_nodes_llamaparse',\n          created_at: new Date().toISOString()\n        }\n      });\n    }\n  }\n  return chunks;\n}\n\nconst textChunks = createSystematicChunks(markdownContent);\nconst documentId = fileData.document_id;\n\n// Prepare data for Neo4j Vector Store Node\nconst neo4jData = {\n  // Document node data\n  document: {\n    id: documentId,\n    title: extractedData.citation?.title || fileData.filename.replace('.pdf', ''),\n    filename: fileData.filename,\n    file_path: fileData.file_path,\n    user_id: fileData.user_id,\n    batch_id: fileData.batch_id,\n    authors: extractedData.citation?.authors || [],\n    year: extractedData.citation?.year || new Date().getFullYear(),\n    document_type: extractedData.citation?.type || 'research_paper',\n    abstract: extractedData.citation?.abstract || '',\n    doi: extractedData.citation?.doi || '',\n    processed_at: new Date().toISOString(),\n    processing_method: 'community_nodes_integration',\n    llm_used: 'groq_llama_3_1_70b',\n    total_chunks: textChunks.length,\n    total_entities: extractedData.entities?.length || 0,\n    total_relationships: extractedData.relationships?.length || 0\n  },\n  \n  // Entities for knowledge graph\n  entities: extractedData.entities || [],\n  \n  // Relationships between entities\n  relationships: extractedData.relationships || [],\n  \n  // Text chunks for vector storage\n  text_chunks: textChunks,\n  \n  // Full document content\n  full_content: markdownContent,\n  \n  // Processing metadata\n  processing_summary: {\n    workflow_version: 'community_nodes_v1',\n    nodes_used: ['llamaparse_community', 'groq_llm', 'neo4j_vector_store'],\n    entities_extracted: extractedData.entities?.length || 0,\n    relationships_extracted: extractedData.relationships?.length || 0,\n    chunks_created: textChunks.length,\n    processing_completed_at: new Date().toISOString(),\n    reliability: 'high_community_nodes'\n  }\n};\n\nreturn [{ json: neo4jData }];"
      },
      "id": "prepare-neo4j-data",
      "name": "Prepare Neo4j Vector Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1120, 300]
    },
    {
      "parameters": {
        "operation": "insert",
        "connectionUri": "bolt://localhost:7687",
        "database": "neo4j",
        "documents": "={{ $json.text_chunks }}",
        "indexName": "document_vectors",
        "textKey": "content",
        "metadataKeys": "metadata,document_id,chunk_index",
        "embeddingDimension": 384
      },
      "id": "neo4j-vector-store",
      "name": "Neo4j Vector Store",
      "type": "n8n-nodes-neo4j.vectorStore",
      "typeVersion": 1,
      "position": [1340, 300],
      "credentials": {
        "neo4j": {
          "id": "neo4j_credentials",
          "name": "Neo4j Database"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "// Create Document node\nCREATE (doc:Document {\n  id: $document.id,\n  title: $document.title,\n  filename: $document.filename,\n  file_path: $document.file_path,\n  user_id: $document.user_id,\n  batch_id: $document.batch_id,\n  authors: $document.authors,\n  year: $document.year,\n  document_type: $document.document_type,\n  abstract: $document.abstract,\n  doi: $document.doi,\n  processed_at: datetime($document.processed_at),\n  processing_method: $document.processing_method,\n  llm_used: $document.llm_used,\n  total_chunks: $document.total_chunks,\n  total_entities: $document.total_entities,\n  total_relationships: $document.total_relationships\n})\n\n// Create Entity nodes\nFOREACH (entity IN $entities |\n  CREATE (e:Entity {\n    id: entity.id,\n    name: entity.name,\n    type: entity.type,\n    description: entity.description,\n    confidence: entity.confidence,\n    source_location: entity.source_location,\n    document_id: $document.id,\n    extracted_at: datetime()\n  })\n)\n\n// Create relationships between entities\nFOREACH (rel IN $relationships |\n  MERGE (source:Entity {id: rel.source_entity})\n  MERGE (target:Entity {id: rel.target_entity})\n  CREATE (source)-[r:RELATED_TO {\n    type: rel.relationship_type,\n    context: rel.context,\n    confidence: rel.confidence,\n    document_id: $document.id,\n    created_at: datetime()\n  }]->(target)\n)\n\n// Link document to entities\nMATCH (doc:Document {id: $document.id})\nMATCH (e:Entity {document_id: $document.id})\nCREATE (doc)-[:EXTRACTED]->(e)\n\nRETURN doc.id as document_id, \n       count(e) as entities_created,\n       $document.total_chunks as chunks_stored",
        "parameters": {
          "document": "={{ $json.document }}",
          "entities": "={{ $json.entities }}",
          "relationships": "={{ $json.relationships }}"
        }
      },
      "id": "neo4j-graph-structure",
      "name": "Create Graph Structure",
      "type": "n8n-nodes-neo4j.neo4j",
      "typeVersion": 1,
      "position": [1560, 300],
      "credentials": {
        "neo4j": {
          "id": "neo4j_credentials",
          "name": "Neo4j Database"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForAllItems",
        "jsCode": "// Format comprehensive DocsGPT response using community nodes results\nconst preparedData = $('Prepare Neo4j Vector Data').first().json;\nconst vectorStoreResult = $('Neo4j Vector Store').first().json;\nconst graphResult = $('Create Graph Structure').first().json;\n\n// Calculate processing statistics\nconst processingStats = {\n  entities_extracted: preparedData.processing_summary.entities_extracted,\n  relationships_extracted: preparedData.processing_summary.relationships_extracted,\n  text_chunks_created: preparedData.processing_summary.chunks_created,\n  processing_time_seconds: Math.round((new Date() - new Date(preparedData.document.processed_at)) / 1000),\n  neo4j_operations_completed: 2, // vector store + graph structure\n  community_nodes_used: 3 // llamaparse + groq + neo4j\n};\n\n// DocsGPT compatible response format\nconst docsgptResponse = {\n  success: true,\n  message: \"Document processed successfully using n8n community nodes integration\",\n  \n  // Document information\n  document: {\n    filename: preparedData.document.filename,\n    file_path: preparedData.document.file_path,\n    batch_id: preparedData.document.batch_id,\n    user_id: preparedData.document.user_id,\n    document_id: preparedData.document.id,\n    title: preparedData.document.title\n  },\n  \n  // Processing results\n  results: {\n    processing_method: 'n8n_community_nodes_integration',\n    llamaparse_method: 'community_node_premium',\n    llm_used: 'groq_llama_3_1_70b_community_node',\n    neo4j_method: 'vector_store_community_node',\n    ...processingStats\n  },\n  \n  // Knowledge graph statistics\n  knowledge_graph: {\n    nodes_created: {\n      documents: 1,\n      entities: processingStats.entities_extracted,\n      text_chunks: processingStats.text_chunks_created\n    },\n    relationships_created: {\n      entity_relationships: processingStats.relationships_extracted,\n      document_entity_links: processingStats.entities_extracted,\n      vector_embeddings: processingStats.text_chunks_created\n    },\n    storage_method: 'neo4j_community_nodes',\n    vector_store_index: 'document_vectors',\n    embedding_dimension: 384\n  },\n  \n  // Community nodes architecture\n  architecture: {\n    workflow_approach: 'n8n_community_nodes_integration',\n    community_nodes_used: [\n      'n8n-nodes-llamacloud.llamaParse (LlamaParse Community Node)',\n      'n8n-nodes-base.llm (Groq LLM Integration)',\n      'n8n-nodes-neo4j.vectorStore (Neo4j Vector Store)',\n      'n8n-nodes-neo4j.neo4j (Neo4j Graph Operations)'\n    ],\n    http_requests_eliminated: [\n      'Direct LlamaParse API calls',\n      'Direct Groq API calls', \n      'Direct Neo4j HTTP API calls'\n    ],\n    reliability: 'maximum_community_node_integration',\n    maintenance_complexity: 'minimal',\n    error_handling: 'built_in_community_nodes'\n  },\n  \n  // Query capabilities\n  query_examples: {\n    vector_search: `CALL db.index.vector.queryNodes('document_vectors', 5, [0.1, 0.2, ...]) YIELD node, score`,\n    entity_search: `MATCH (e:Entity {document_id: '${preparedData.document.id}'}) RETURN e`,\n    relationship_discovery: `MATCH (e1:Entity)-[r:RELATED_TO]->(e2:Entity) WHERE r.document_id = '${preparedData.document.id}' RETURN e1.name, r.type, e2.name`,\n    document_overview: `MATCH (doc:Document {id: '${preparedData.document.id}'}) RETURN doc`\n  },\n  \n  // API endpoints for DocsGPT integration\n  api_endpoints: {\n    neo4j_browser: \"http://localhost:7474\",\n    n8n_workflow: \"http://localhost:5678\",\n    search_knowledge_graph: \"/api/knowledge/search\",\n    get_document_entities: `/api/knowledge/document/${preparedData.document.id}/entities`,\n    vector_search_endpoint: `/api/knowledge/vector-search/${preparedData.document.id}`\n  },\n  \n  // Processing metadata\n  metadata: {\n    processed_at: new Date().toISOString(),\n    processing_duration: `${processingStats.processing_time_seconds}s`,\n    workflow_version: 'community_nodes_v1',\n    llamaparse_integration: 'community_node',\n    groq_integration: 'community_node', \n    neo4j_integration: 'vector_store_community_node',\n    community_node_dependencies: 3,\n    reliability_score: 'maximum',\n    maintenance_required: 'minimal',\n    upgrade_path: 'automatic_with_n8n_updates'\n  }\n};\n\nreturn { json: docsgptResponse };"
      },
      "id": "format-response",
      "name": "Format DocsGPT Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1780, 300]
    }
  ],
  "connections": {
    "DocsGPT Document Webhook": {
      "main": [[{"node": "Parse DocsGPT Payload", "type": "main", "index": 0}]]
    },
    "Parse DocsGPT Payload": {
      "main": [[{"node": "LlamaParse Community Node", "type": "main", "index": 0}]]
    },
    "LlamaParse Community Node": {
      "main": [[{"node": "Groq Entity Extraction", "type": "main", "index": 0}]]
    },
    "Groq Entity Extraction": {
      "main": [[{"node": "Prepare Neo4j Vector Data", "type": "main", "index": 0}]]
    },
    "Prepare Neo4j Vector Data": {
      "main": [[{"node": "Neo4j Vector Store", "type": "main", "index": 0}]]
    },
    "Neo4j Vector Store": {
      "main": [[{"node": "Create Graph Structure", "type": "main", "index": 0}]]
    },
    "Create Graph Structure": {
      "main": [[{"node": "Format DocsGPT Response", "type": "main", "index": 0}]]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "id": "docsgpt-community-nodes-integration",
  "tags": ["docsgpt", "community-nodes", "llamaparse", "neo4j", "groq", "vector-store", "knowledge-graph"],
  "meta": {
    "instanceId": "community-nodes-workflow"
  }
}