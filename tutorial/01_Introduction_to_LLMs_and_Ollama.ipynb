{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Tutorial 1: Introduction to LLMs and Ollama\n",
    "\n",
    "**Welcome to your AI journey!** This tutorial teaches you the very basics of Large Language Models (LLMs) and how to use them locally with Ollama.\n",
    "\n",
    "## 🎯 What You'll Learn:\n",
    "- What is an LLM (Large Language Model)?\n",
    "- What is Ollama and why use it?\n",
    "- How to check if Ollama is working\n",
    "- Your first conversation with an AI\n",
    "- Understanding how AI responses work\n",
    "\n",
    "## ⏱️ Time: 15-20 minutes\n",
    "## 📚 Level: Complete Beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 What is an LLM?\n",
    "\n",
    "**LLM = Large Language Model**\n",
    "\n",
    "Think of an LLM as a very smart computer program that:\n",
    "- **Reads and understands text** (like a human)\n",
    "- **Writes responses** that make sense\n",
    "- **Answers questions** about topics it has learned\n",
    "- **Helps with tasks** like writing, summarizing, and explaining\n",
    "\n",
    "### 🌟 Popular LLMs:\n",
    "- **ChatGPT** (by OpenAI) - you might have used this\n",
    "- **Claude** (by Anthropic) - that's me!\n",
    "- **Llama** (by Meta) - this is what we'll use locally\n",
    "\n",
    "### 🔒 Why Use Local LLMs?\n",
    "- **Privacy**: Your data never leaves your computer\n",
    "- **Free**: No monthly subscription fees\n",
    "- **Always Available**: Works without internet\n",
    "- **Full Control**: You decide what data to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🦙 What is Ollama?\n",
    "\n",
    "**Ollama** is like a \"personal AI assistant\" that runs on your computer.\n",
    "\n",
    "### 🎯 Think of it like this:\n",
    "- **ChatGPT** = Using AI on the internet (like Gmail)\n",
    "- **Ollama** = Having AI on your computer (like having Microsoft Word)\n",
    "\n",
    "### 🔧 What Ollama Does:\n",
    "1. **Downloads AI models** to your computer\n",
    "2. **Runs them locally** without internet\n",
    "3. **Provides an easy way** to chat with AI\n",
    "4. **Manages everything** so you don't have to worry about technical details\n",
    "\n",
    "### 📦 Models We'll Use:\n",
    "- **llama3.1:8b** - The \"brain\" that understands and responds\n",
    "- **nomic-embed-text** - Helps find relevant information in documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Step 1: Check if Ollama is Running\n",
    "\n",
    "Before we start, let's make sure Ollama is working on your computer.\n",
    "\n",
    "**🚨 Important:** If this doesn't work, you need to:\n",
    "1. Make sure Ollama is installed\n",
    "2. Run `ollama serve` in your terminal\n",
    "3. Make sure you have the models downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if Ollama is running\n",
    "import requests\n",
    "\n",
    "print(\"🔍 Checking if Ollama is running...\")\n",
    "\n",
    "try:\n",
    "    # Try to connect to Ollama\n",
    "    response = requests.get(\"http://127.0.0.1:11434\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Great! Ollama is running and ready to use!\")\n",
    "        print(\"🎉 You're ready to continue with the tutorial\")\n",
    "    else:\n",
    "        print(\"❌ Ollama is not responding properly\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Can't connect to Ollama\")\n",
    "    print(\"🔧 Make sure you've run 'ollama serve' in your terminal\")\n",
    "    print(f\"   Error details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Step 2: Import LangChain\n",
    "\n",
    "**LangChain** is a tool that makes it easy to work with LLMs. Think of it as a \"translator\" that helps your Python code talk to Ollama.\n",
    "\n",
    "We'll import the parts we need step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain - this helps us talk to Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(\"📚 LangChain imported successfully!\")\n",
    "print(\"🔗 Now we can connect to Ollama through Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Step 3: Create Your First AI Assistant\n",
    "\n",
    "Now let's create an AI assistant that can answer questions. This is like \"waking up\" the AI and telling it how to behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AI assistant using Ollama\n",
    "print(\"🤖 Creating your AI assistant...\")\n",
    "\n",
    "# This creates a connection to the llama3.1:8b model\n",
    "ai_assistant = ChatOllama(\n",
    "    model=\"llama3.1:8b\",           # Which AI brain to use\n",
    "    temperature=0.7,              # How creative (0 = boring, 1 = very creative)\n",
    "    num_ctx=4096                  # How much text it can remember at once\n",
    ")\n",
    "\n",
    "print(\"✅ AI assistant created!\")\n",
    "print(\"🧠 Using llama3.1:8b model\")\n",
    "print(\"🌡️ Temperature: 0.7 (moderately creative)\")\n",
    "print(\"💭 Context window: 4096 tokens (about 3000 words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💬 Step 4: Your First Conversation\n",
    "\n",
    "Time for the exciting part - let's actually talk to the AI! We'll start with a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the AI a simple question\n",
    "print(\"💬 Having your first conversation with AI...\")\n",
    "print(\"⏳ This might take 5-15 seconds...\")\n",
    "print()\n",
    "\n",
    "# Your question to the AI\n",
    "question = \"Hello! Can you explain what you are in simple terms?\"\n",
    "\n",
    "print(f\"👤 You: {question}\")\n",
    "print(\"🤖 AI:\")\n",
    "\n",
    "# Send the question to AI and get response\n",
    "response = ai_assistant.invoke(question)\n",
    "\n",
    "# Print the AI's response\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n🎉 Congratulations! You just had your first local AI conversation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Step 5: Try Different Questions\n",
    "\n",
    "Let's experiment with different types of questions to see how the AI responds. Each cell below asks a different type of question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a factual question\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "print(f\"👤 You: {question}\")\n",
    "print(\"🤖 AI:\")\n",
    "\n",
    "response = ai_assistant.invoke(question)\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n💡 This was a factual question - the AI gave a direct answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an explanation question\n",
    "question = \"Can you explain how bicycles work in simple terms?\"\n",
    "\n",
    "print(f\"👤 You: {question}\")\n",
    "print(\"🤖 AI:\")\n",
    "\n",
    "response = ai_assistant.invoke(question)\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n💡 This was an explanation question - the AI broke down a complex topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a creative question\n",
    "question = \"Write a short poem about computers and humans working together\"\n",
    "\n",
    "print(f\"👤 You: {question}\")\n",
    "print(\"🤖 AI:\")\n",
    "\n",
    "response = ai_assistant.invoke(question)\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n💡 This was a creative question - the AI generated original content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎛️ Step 6: Understanding AI Settings\n",
    "\n",
    "Let's learn about the settings we used when creating the AI assistant and what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create AIs with different \"personalities\" by changing the temperature\n",
    "\n",
    "print(\"🧪 EXPERIMENT: Different AI Personalities\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a \"boring\" AI (temperature = 0)\n",
    "boring_ai = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.0,    # Very predictable, factual\n",
    "    num_ctx=4096\n",
    ")\n",
    "\n",
    "# Create a \"creative\" AI (temperature = 1.0)\n",
    "creative_ai = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=1.0,    # Very creative, unpredictable\n",
    "    num_ctx=4096\n",
    ")\n",
    "\n",
    "question = \"Describe a sunny day\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n🤖 BORING AI (temperature=0.0):\")\n",
    "response1 = boring_ai.invoke(question)\n",
    "print(response1.content)\n",
    "\n",
    "print(\"\\n🎨 CREATIVE AI (temperature=1.0):\")\n",
    "response2 = creative_ai.invoke(question)\n",
    "print(response2.content)\n",
    "\n",
    "print(\"\\n💡 Notice the difference? Lower temperature = more predictable, higher = more creative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 7: Your Turn to Experiment\n",
    "\n",
    "Now it's your turn! Try asking the AI different questions. Change the question in the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 YOUR TURN: Change this question to anything you want!\n",
    "your_question = \"Tell me a fun fact about space\"  # 👈 Change this!\n",
    "\n",
    "print(f\"👤 You: {your_question}\")\n",
    "print(\"🤖 AI:\")\n",
    "\n",
    "response = ai_assistant.invoke(your_question)\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n🎉 Great job! You're now experimenting with AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 8: Understanding What Just Happened\n",
    "\n",
    "Let's break down what actually happened when you talked to the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what information we can get about our AI responses\n",
    "print(\"🔍 BEHIND THE SCENES: How AI Responses Work\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "question = \"What is machine learning?\"\n",
    "response = ai_assistant.invoke(question)\n",
    "\n",
    "print(f\"📝 Your Question: {question}\")\n",
    "print(f\"🤖 AI Response: {response.content}\")\n",
    "print(f\"\\n🔧 Technical Details:\")\n",
    "print(f\"   • Response Type: {type(response).__name__}\")\n",
    "print(f\"   • Response Length: {len(response.content)} characters\")\n",
    "print(f\"   • Word Count: {len(response.content.split())} words\")\n",
    "\n",
    "print(\"\\n💡 The AI:\")\n",
    "print(\"   1. Read your question\")\n",
    "print(\"   2. Thought about what you were asking\")\n",
    "print(\"   3. Generated a response word by word\")\n",
    "print(\"   4. Sent the complete answer back to you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 What You've Learned\n",
    "\n",
    "**Congratulations!** You've just completed your first AI tutorial. Here's what you now know:\n",
    "\n",
    "### ✅ **Key Concepts:**\n",
    "- **LLMs** are AI programs that understand and generate text\n",
    "- **Ollama** runs AI models locally on your computer\n",
    "- **LangChain** helps Python code talk to AI models\n",
    "- **Temperature** controls how creative vs predictable the AI is\n",
    "\n",
    "### ✅ **Skills You've Gained:**\n",
    "- How to check if Ollama is running\n",
    "- How to create an AI assistant in Python\n",
    "- How to ask questions and get responses\n",
    "- How to experiment with different AI settings\n",
    "\n",
    "### 🚀 **What's Next:**\n",
    "In **Tutorial 2**, you'll learn:\n",
    "- How to work with documents (like PDF files)\n",
    "- What LangChain can do beyond simple chat\n",
    "- How to process and analyze text files\n",
    "- Building blocks for more complex AI applications\n",
    "\n",
    "### 🎯 **Practice Ideas:**\n",
    "- Try asking the AI to explain different topics\n",
    "- Experiment with different temperature settings\n",
    "- Ask the AI to help you learn new subjects\n",
    "- See how the AI responds to different types of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 Final Challenge\n",
    "\n",
    "Before moving to Tutorial 2, try this fun challenge to test your understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏆 CHALLENGE: Create your own AI with custom settings\n",
    "print(\"🏆 FINAL CHALLENGE: Create Your Custom AI\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# TODO: Create an AI assistant with:\n",
    "# - Temperature of 0.5 (balanced creativity)\n",
    "# - Context window of 2048\n",
    "# Then ask it to explain what an AI assistant is\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "my_custom_ai = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.5,     # 👈 Balanced creativity\n",
    "    num_ctx=2048         # 👈 Smaller context window\n",
    ")\n",
    "\n",
    "challenge_question = \"Explain what an AI assistant is to a 10-year-old\"\n",
    "\n",
    "print(f\"🎯 Challenge Question: {challenge_question}\")\n",
    "print(\"\\n🤖 Your Custom AI:\")\n",
    "\n",
    "response = my_custom_ai.invoke(challenge_question)\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n🎉 Challenge Complete! You've successfully created and used a custom AI assistant!\")\n",
    "print(\"🚀 You're ready for Tutorial 2: LangChain Fundamentals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}