{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Tutorial 3: Understanding RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**Welcome to the exciting world of RAG!** This is where AI gets really smart about finding and using information.\n",
    "\n",
    "## 🎯 What You'll Learn:\n",
    "- What RAG means in simple terms\n",
    "- How AI \"understands\" text with embeddings\n",
    "- How to find the most relevant information\n",
    "- Building a smart question-answering system\n",
    "- Why RAG is better than just asking AI questions\n",
    "\n",
    "## ⏱️ Time: 30-35 minutes\n",
    "## 📚 Level: Beginner\n",
    "## 📋 Prerequisites: Tutorials 1 & 2 completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 What is RAG?\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "Sounds complicated? Let's break it down:\n",
    "\n",
    "### 🧩 The Three Parts:\n",
    "1. **Retrieval**: Finding the right information\n",
    "2. **Augmented**: Adding that information to help the AI\n",
    "3. **Generation**: AI creates an answer using that information\n",
    "\n",
    "### 🏠 Real-World Analogy:\n",
    "Imagine you're helping a friend with homework:\n",
    "- **Without RAG**: \"What's the capital of France?\" → Friend guesses from memory\n",
    "- **With RAG**: Friend first looks it up in a textbook, then answers confidently\n",
    "\n",
    "### 🎯 Why RAG is Amazing:\n",
    "- **Up-to-date**: Can use latest information\n",
    "- **Accurate**: Based on real documents, not just AI memory\n",
    "- **Specific**: Can answer questions about your specific documents\n",
    "- **Trustworthy**: You can see where the answer came from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Step 1: Setup for RAG\n",
    "\n",
    "Let's import everything we need to build a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools for RAG\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "print(\"🔍 RAG tools imported!\")\n",
    "print(\"📊 Ready to build an intelligent retrieval system\")\n",
    "print(\"🧠 This will combine document search with AI understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Step 2: Understanding Embeddings\n",
    "\n",
    "**Embeddings** are how AI \"understands\" text. Think of them as AI's way of describing text with numbers.\n",
    "\n",
    "### 🎨 Analogy:\n",
    "- **Human**: \"This text is about cats\" \n",
    "- **AI Embedding**: [0.2, -0.8, 0.5, 0.1, ...] (hundreds of numbers)\n",
    "\n",
    "### 🤝 Similar Meanings = Similar Numbers:\n",
    "- \"Dog\" and \"Puppy\" have similar embeddings\n",
    "- \"Car\" and \"Airplane\" are less similar\n",
    "- \"Happy\" and \"Joyful\" are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding model\n",
    "print(\"🧠 Creating an embedding model...\")\n",
    "print(\"⏳ This connects to Ollama's nomic-embed-text model...\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "print(\"✅ Embedding model ready!\")\n",
    "print(\"🔢 This model converts text into numbers that AI can understand\")\n",
    "\n",
    "# Let's see embeddings in action\n",
    "print(\"\\n🧪 EXPERIMENT: How AI Sees Text\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create embeddings for different words\n",
    "word1 = \"cat\"\n",
    "word2 = \"dog\"\n",
    "word3 = \"car\"\n",
    "\n",
    "print(\"Converting words to AI numbers...\")\n",
    "embedding1 = embeddings.embed_query(word1)\n",
    "embedding2 = embeddings.embed_query(word2)\n",
    "embedding3 = embeddings.embed_query(word3)\n",
    "\n",
    "print(\"\\n📊 Results:\")\n",
    "print(f\"'{word1}' → {len(embedding1)} numbers (first 5: {embedding1[:5]})\")\n",
    "print(f\"'{word2}' → {len(embedding2)} numbers (first 5: {embedding2[:5]})\")\n",
    "print(f\"'{word3}' → {len(embedding3)} numbers (first 5: {embedding3[:5]})\")\n",
    "\n",
    "print(\"\\n💡 Each word becomes a list of numbers that capture its meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Step 3: Prepare Documents for RAG\n",
    "\n",
    "Let's load and prepare our research paper for RAG. We'll convert all the text chunks into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the document (like Tutorial 2)\n",
    "print(\"📄 Loading research paper...\")\n",
    "\n",
    "pdf_path = \"../examples/d4sc03921a.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Paper loaded and split into {len(chunks)} chunks\")\n",
    "print(\"📝 Each chunk is about 1000 characters\")\n",
    "print(\"🔄 200 character overlap between chunks for continuity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗃️ Step 4: Create a Vector Database\n",
    "\n",
    "A **vector database** stores all our text chunks as embeddings. It's like a super-smart library that can find similar content instantly.\n",
    "\n",
    "### 🏗️ What We're Building:\n",
    "1. Take each text chunk\n",
    "2. Convert it to embeddings (numbers)\n",
    "3. Store in a searchable database\n",
    "4. When asked a question, find the most similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database\n",
    "print(\"🗃️ Creating vector database...\")\n",
    "print(\"⏳ Converting all text chunks to embeddings... (this takes 30-60 seconds)\")\n",
    "\n",
    "# This creates embeddings for all chunks and stores them\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"../tutorial/chroma_db\"  # Save to disk\n",
    ")\n",
    "\n",
    "print(\"✅ Vector database created!\")\n",
    "print(f\"📊 Stored {len(chunks)} chunks as searchable embeddings\")\n",
    "print(\"🔍 Ready for intelligent document search!\")\n",
    "\n",
    "# Create a retriever (the search engine)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar chunks\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 Search engine configured:\")\n",
    "print(\"   • Type: Similarity search\")\n",
    "print(\"   • Returns: Top 3 most relevant chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 5: Test the Retrieval System\n",
    "\n",
    "Let's see how well our system can find relevant information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our retrieval system\n",
    "print(\"🔍 Testing the retrieval system...\")\n",
    "\n",
    "# Ask a question\n",
    "test_question = \"What are the main findings of this research?\"\n",
    "\n",
    "print(f\"❓ Question: {test_question}\")\n",
    "print(\"🔍 Searching for relevant chunks...\")\n",
    "\n",
    "# Find relevant chunks\n",
    "relevant_chunks = retriever.invoke(test_question)\n",
    "\n",
    "print(f\"\\n✅ Found {len(relevant_chunks)} relevant chunks!\")\n",
    "\n",
    "# Show what we found\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"\\n📄 Chunk {i+1}:\")\n",
    "    print(f\"Page: {chunk.metadata.get('page', 'Unknown')}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "\n",
    "print(\"\\n💡 The system found the most relevant parts of the document for your question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Step 6: Build the Complete RAG System\n",
    "\n",
    "Now let's combine retrieval with AI generation to create a complete RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AI assistant\n",
    "ai_assistant = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1  # Low temperature for factual answers\n",
    ")\n",
    "\n",
    "# Create a RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant that answers questions based on provided context.\n",
    "\n",
    "Context from the document:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a clear, accurate answer based on the context above. \n",
    "If the context doesn't contain enough information, say so.\n",
    "\"\"\")\n",
    "\n",
    "print(\"🤖 RAG system components created!\")\n",
    "print(\"🔧 Components: Retriever + AI + Smart Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Combine multiple document chunks into one context\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain\n",
    "print(\"🔗 Building the complete RAG chain...\")\n",
    "\n",
    "# This chain: Question → Retrieve docs → Format → AI → Answer\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Get and format relevant docs\n",
    "        \"question\": RunnablePassthrough()    # Pass question through\n",
    "    }\n",
    "    | rag_prompt          # Create prompt with context and question\n",
    "    | ai_assistant        # Generate answer\n",
    "    | StrOutputParser()   # Clean output\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain created!\")\n",
    "print(\"🎯 Flow: Question → Retrieve → Context → AI → Answer\")\n",
    "print(\"🚀 Ready for intelligent question answering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 7: Test Your RAG System\n",
    "\n",
    "Time for the exciting part - let's ask our RAG system questions about the research paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system\n",
    "print(\"🎯 Testing the complete RAG system!\")\n",
    "print(\"⏳ This might take 15-30 seconds...\")\n",
    "\n",
    "question = \"What is this research paper about? What are the main topics?\"\n",
    "\n",
    "print(f\"\\n❓ Question: {question}\")\n",
    "print(\"\\n🔍 RAG Process:\")\n",
    "print(\"   1. Finding relevant document chunks...\")\n",
    "print(\"   2. Providing context to AI...\")\n",
    "print(\"   3. Generating informed answer...\")\n",
    "\n",
    "# Get the answer\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(\"\\n🤖 RAG Answer:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n✨ This answer is based on the actual content of the document!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "question2 = \"Who are the authors of this paper and what did they study?\"\n",
    "\n",
    "print(f\"❓ Question: {question2}\")\n",
    "print(\"\\n🤖 RAG Answer:\")\n",
    "\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "print(answer2)\n",
    "\n",
    "print(\"\\n💡 RAG found the specific information about authors in the document!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a more specific question\n",
    "question3 = \"What methods or techniques were used in this research?\"\n",
    "\n",
    "print(f\"❓ Question: {question3}\")\n",
    "print(\"\\n🤖 RAG Answer:\")\n",
    "\n",
    "answer3 = rag_chain.invoke(question3)\n",
    "print(answer3)\n",
    "\n",
    "print(\"\\n🎯 RAG can find specific technical details in the document!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Step 8: Compare RAG vs Non-RAG\n",
    "\n",
    "Let's see the difference between asking AI with and without document context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG vs regular AI\n",
    "print(\"🔬 EXPERIMENT: RAG vs Regular AI\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "comparison_question = \"What are the main conclusions of the paper by Mayk Caldas Ramos?\"\n",
    "\n",
    "print(f\"❓ Question: {comparison_question}\")\n",
    "\n",
    "# Get answer WITHOUT RAG (just AI memory)\n",
    "print(\"\\n🤖 Regular AI (without document):\")\n",
    "regular_answer = ai_assistant.invoke(comparison_question)\n",
    "print(regular_answer.content)\n",
    "\n",
    "# Get answer WITH RAG (using document)\n",
    "print(\"\\n🔍 RAG AI (with document context):\")\n",
    "rag_answer = rag_chain.invoke(comparison_question)\n",
    "print(rag_answer)\n",
    "\n",
    "print(\"\\n💡 NOTICE THE DIFFERENCE:\")\n",
    "print(\"   • Regular AI: Might guess or say it doesn't know\")\n",
    "print(\"   • RAG AI: Uses actual content from the document\")\n",
    "print(\"   • RAG is more accurate and specific!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎮 Step 9: Interactive RAG Session\n",
    "\n",
    "Now it's your turn to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 YOUR TURN: Ask the RAG system anything!\n",
    "\n",
    "your_question = \"What are the key findings or results mentioned in this paper?\"  # 👈 Change this!\n",
    "\n",
    "print(f\"❓ Your Question: {your_question}\")\n",
    "print(\"\\n🔍 RAG is searching and thinking...\")\n",
    "\n",
    "your_answer = rag_chain.invoke(your_question)\n",
    "\n",
    "print(\"\\n🤖 RAG Answer:\")\n",
    "print(your_answer)\n",
    "\n",
    "print(\"\\n🎉 Great! You're now using RAG to get intelligent answers from documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 10: Understanding RAG Parameters\n",
    "\n",
    "Let's learn how to tune RAG for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different retrieval settings\n",
    "print(\"🔧 TUNING RAG: Different Settings\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "question = \"What methods were used?\"\n",
    "\n",
    "# Try retrieving more chunks\n",
    "more_chunks_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5}  # Get 5 chunks instead of 3\n",
    ")\n",
    "\n",
    "# Build new chain with more context\n",
    "more_context_chain = (\n",
    "    {\n",
    "        \"context\": more_chunks_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | ai_assistant\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(f\"❓ Question: {question}\")\n",
    "\n",
    "print(\"\\n🔍 RAG with 3 chunks:\")\n",
    "answer_3 = rag_chain.invoke(question)\n",
    "print(answer_3)\n",
    "\n",
    "print(\"\\n📚 RAG with 5 chunks (more context):\")\n",
    "answer_5 = more_context_chain.invoke(question)\n",
    "print(answer_5)\n",
    "\n",
    "print(\"\\n💡 More chunks = more context, but also more processing time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 What You've Learned\n",
    "\n",
    "**Fantastic work!** You've built a complete RAG system and understand how it works.\n",
    "\n",
    "### ✅ **Key Concepts:**\n",
    "- **RAG**: Combines document retrieval with AI generation\n",
    "- **Embeddings**: How AI converts text to searchable numbers\n",
    "- **Vector Database**: Stores and searches document embeddings\n",
    "- **Retrieval**: Finding relevant document chunks\n",
    "- **Context**: Providing AI with relevant information\n",
    "\n",
    "### ✅ **Skills You've Gained:**\n",
    "- Creating embeddings from text\n",
    "- Building vector databases for document search\n",
    "- Implementing similarity-based retrieval\n",
    "- Combining retrieval with AI generation\n",
    "- Tuning RAG parameters for better results\n",
    "\n",
    "### 🚀 **What's Next:**\n",
    "In **Tutorial 4**, you'll learn about **Knowledge Graphs**:\n",
    "- What knowledge graphs are and how they work\n",
    "- Extracting entities (people, places, concepts) from text\n",
    "- Finding relationships between entities\n",
    "- Visualizing knowledge networks\n",
    "- Combining knowledge graphs with RAG\n",
    "\n",
    "### 🎯 **Practice Ideas:**\n",
    "- Try RAG with your own documents\n",
    "- Experiment with different chunk sizes\n",
    "- Test various numbers of retrieved chunks\n",
    "- Compare answers with and without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 Final Challenge\n",
    "\n",
    "Build your own specialized RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏆 CHALLENGE: Build a Summary RAG System\n",
    "print(\"🏆 FINAL CHALLENGE: Specialized RAG for Summaries\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create a special prompt for generating summaries\n",
    "summary_rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert at creating clear, concise summaries of research papers.\n",
    "\n",
    "Based on this context from a research paper:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear summary that:\n",
    "1. Highlights the main points\n",
    "2. Uses simple language\n",
    "3. Is 2-3 sentences maximum\n",
    "\"\"\")\n",
    "\n",
    "# Build the summary RAG chain\n",
    "summary_rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | summary_rag_prompt\n",
    "    | ai_assistant\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test it\n",
    "question = \"Give me a brief summary of what this paper is about\"\n",
    "summary = summary_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"❓ Question: {question}\")\n",
    "print(\"\\n📝 Summary RAG Answer:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n🎉 Challenge Complete! You've built a specialized RAG system!\")\n",
    "print(\"🚀 Ready for Tutorial 4: Building Knowledge Graphs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}