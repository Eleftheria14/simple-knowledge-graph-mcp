{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” Tutorial 3: Understanding RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**Welcome to the exciting world of RAG!** This is where AI gets really smart about finding and using information.\n",
    "\n",
    "## ğŸ¯ What You'll Learn:\n",
    "- What RAG means in simple terms\n",
    "- How AI \"understands\" text with embeddings\n",
    "- How to find the most relevant information\n",
    "- Building a smart question-answering system\n",
    "- Why RAG is better than just asking AI questions\n",
    "\n",
    "## â±ï¸ Time: 30-35 minutes\n",
    "## ğŸ“š Level: Beginner\n",
    "## ğŸ“‹ Prerequisites: Tutorials 1 & 2 completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” What is RAG?\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "Sounds complicated? Let's break it down:\n",
    "\n",
    "### ğŸ§© The Three Parts:\n",
    "1. **Retrieval**: Finding the right information\n",
    "2. **Augmented**: Adding that information to help the AI\n",
    "3. **Generation**: AI creates an answer using that information\n",
    "\n",
    "### ğŸ  Real-World Analogy:\n",
    "Imagine you're helping a friend with homework:\n",
    "- **Without RAG**: \"What's the capital of France?\" â†’ Friend guesses from memory\n",
    "- **With RAG**: Friend first looks it up in a textbook, then answers confidently\n",
    "\n",
    "### ğŸ¯ Why RAG is Amazing:\n",
    "- **Up-to-date**: Can use latest information\n",
    "- **Accurate**: Based on real documents, not just AI memory\n",
    "- **Specific**: Can answer questions about your specific documents\n",
    "- **Trustworthy**: You can see where the answer came from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 1: Setup for RAG\n",
    "\n",
    "Let's import everything we need to build a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools for RAG\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "print(\"ğŸ” RAG tools imported!\")\n",
    "print(\"ğŸ“Š Ready to build an intelligent retrieval system\")\n",
    "print(\"ğŸ§  This will combine document search with AI understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 2: Understanding Embeddings\n",
    "\n",
    "**Embeddings** are how AI \"understands\" text. Think of them as AI's way of describing text with numbers.\n",
    "\n",
    "### ğŸ¨ Analogy:\n",
    "- **Human**: \"This text is about cats\" \n",
    "- **AI Embedding**: [0.2, -0.8, 0.5, 0.1, ...] (hundreds of numbers)\n",
    "\n",
    "### ğŸ¤ Similar Meanings = Similar Numbers:\n",
    "- \"Dog\" and \"Puppy\" have similar embeddings\n",
    "- \"Car\" and \"Airplane\" are less similar\n",
    "- \"Happy\" and \"Joyful\" are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding model\n",
    "print(\"ğŸ§  Creating an embedding model...\")\n",
    "print(\"â³ This connects to Ollama's nomic-embed-text model...\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Embedding model ready!\")\n",
    "print(\"ğŸ”¢ This model converts text into numbers that AI can understand\")\n",
    "\n",
    "# Let's see embeddings in action\n",
    "print(\"\\nğŸ§ª EXPERIMENT: How AI Sees Text\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create embeddings for different words\n",
    "word1 = \"cat\"\n",
    "word2 = \"dog\"\n",
    "word3 = \"car\"\n",
    "\n",
    "print(\"Converting words to AI numbers...\")\n",
    "embedding1 = embeddings.embed_query(word1)\n",
    "embedding2 = embeddings.embed_query(word2)\n",
    "embedding3 = embeddings.embed_query(word3)\n",
    "\n",
    "print(\"\\nğŸ“Š Results:\")\n",
    "print(f\"'{word1}' â†’ {len(embedding1)} numbers (first 5: {embedding1[:5]})\")\n",
    "print(f\"'{word2}' â†’ {len(embedding2)} numbers (first 5: {embedding2[:5]})\")\n",
    "print(f\"'{word3}' â†’ {len(embedding3)} numbers (first 5: {embedding3[:5]})\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Each word becomes a list of numbers that capture its meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 3: Prepare Documents for RAG\n",
    "\n",
    "Let's load and prepare our research paper for RAG. We'll convert all the text chunks into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the document (like Tutorial 2)\n",
    "print(\"ğŸ“„ Loading research paper...\")\n",
    "\n",
    "pdf_path = \"../examples/d4sc03921a.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ… Paper loaded and split into {len(chunks)} chunks\")\n",
    "print(\"ğŸ“ Each chunk is about 1000 characters\")\n",
    "print(\"ğŸ”„ 200 character overlap between chunks for continuity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ƒï¸ Step 4: Create a Vector Database\n",
    "\n",
    "A **vector database** stores all our text chunks as embeddings. It's like a super-smart library that can find similar content instantly.\n",
    "\n",
    "### ğŸ—ï¸ What We're Building:\n",
    "1. Take each text chunk\n",
    "2. Convert it to embeddings (numbers)\n",
    "3. Store in a searchable database\n",
    "4. When asked a question, find the most similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database\n",
    "print(\"ğŸ—ƒï¸ Creating vector database...\")\n",
    "print(\"â³ Converting all text chunks to embeddings... (this takes 30-60 seconds)\")\n",
    "\n",
    "# This creates embeddings for all chunks and stores them\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"../tutorial/chroma_db\"  # Save to disk\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector database created!\")\n",
    "print(f\"ğŸ“Š Stored {len(chunks)} chunks as searchable embeddings\")\n",
    "print(\"ğŸ” Ready for intelligent document search!\")\n",
    "\n",
    "# Create a retriever (the search engine)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar chunks\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ¯ Search engine configured:\")\n",
    "print(\"   â€¢ Type: Similarity search\")\n",
    "print(\"   â€¢ Returns: Top 3 most relevant chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 5: Test the Retrieval System\n",
    "\n",
    "Let's see how well our system can find relevant information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our retrieval system\n",
    "print(\"ğŸ” Testing the retrieval system...\")\n",
    "\n",
    "# Ask a question\n",
    "test_question = \"What are the main findings of this research?\"\n",
    "\n",
    "print(f\"â“ Question: {test_question}\")\n",
    "print(\"ğŸ” Searching for relevant chunks...\")\n",
    "\n",
    "# Find relevant chunks\n",
    "relevant_chunks = retriever.invoke(test_question)\n",
    "\n",
    "print(f\"\\nâœ… Found {len(relevant_chunks)} relevant chunks!\")\n",
    "\n",
    "# Show what we found\n",
    "for i, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"\\nğŸ“„ Chunk {i+1}:\")\n",
    "    print(f\"Page: {chunk.metadata.get('page', 'Unknown')}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "\n",
    "print(\"\\nğŸ’¡ The system found the most relevant parts of the document for your question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 6: Build the Complete RAG System\n",
    "\n",
    "Now let's combine retrieval with AI generation to create a complete RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AI assistant\n",
    "ai_assistant = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1  # Low temperature for factual answers\n",
    ")\n",
    "\n",
    "# Create a RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant that answers questions based on provided context.\n",
    "\n",
    "Context from the document:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a clear, accurate answer based on the context above. \n",
    "If the context doesn't contain enough information, say so.\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ¤– RAG system components created!\")\n",
    "print(\"ğŸ”§ Components: Retriever + AI + Smart Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Combine multiple document chunks into one context\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain\n",
    "print(\"ğŸ”— Building the complete RAG chain...\")\n",
    "\n",
    "# This chain: Question â†’ Retrieve docs â†’ Format â†’ AI â†’ Answer\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Get and format relevant docs\n",
    "        \"question\": RunnablePassthrough()    # Pass question through\n",
    "    }\n",
    "    | rag_prompt          # Create prompt with context and question\n",
    "    | ai_assistant        # Generate answer\n",
    "    | StrOutputParser()   # Clean output\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG chain created!\")\n",
    "print(\"ğŸ¯ Flow: Question â†’ Retrieve â†’ Context â†’ AI â†’ Answer\")\n",
    "print(\"ğŸš€ Ready for intelligent question answering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 7: Test Your RAG System\n",
    "\n",
    "Time for the exciting part - let's ask our RAG system questions about the research paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system\n",
    "print(\"ğŸ¯ Testing the complete RAG system!\")\n",
    "print(\"â³ This might take 15-30 seconds...\")\n",
    "\n",
    "question = \"What is this research paper about? What are the main topics?\"\n",
    "\n",
    "print(f\"\\nâ“ Question: {question}\")\n",
    "print(\"\\nğŸ” RAG Process:\")\n",
    "print(\"   1. Finding relevant document chunks...\")\n",
    "print(\"   2. Providing context to AI...\")\n",
    "print(\"   3. Generating informed answer...\")\n",
    "\n",
    "# Get the answer\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(\"\\nğŸ¤– RAG Answer:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nâœ¨ This answer is based on the actual content of the document!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "question2 = \"Who are the authors of this paper and what did they study?\"\n",
    "\n",
    "print(f\"â“ Question: {question2}\")\n",
    "print(\"\\nğŸ¤– RAG Answer:\")\n",
    "\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "print(answer2)\n",
    "\n",
    "print(\"\\nğŸ’¡ RAG found the specific information about authors in the document!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a more specific question\n",
    "question3 = \"What methods or techniques were used in this research?\"\n",
    "\n",
    "print(f\"â“ Question: {question3}\")\n",
    "print(\"\\nğŸ¤– RAG Answer:\")\n",
    "\n",
    "answer3 = rag_chain.invoke(question3)\n",
    "print(answer3)\n",
    "\n",
    "print(\"\\nğŸ¯ RAG can find specific technical details in the document!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Step 8: Compare RAG vs Non-RAG\n",
    "\n",
    "Let's see the difference between asking AI with and without document context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG vs regular AI\n",
    "print(\"ğŸ”¬ EXPERIMENT: RAG vs Regular AI\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "comparison_question = \"What are the main conclusions of the paper by Mayk Caldas Ramos?\"\n",
    "\n",
    "print(f\"â“ Question: {comparison_question}\")\n",
    "\n",
    "# Get answer WITHOUT RAG (just AI memory)\n",
    "print(\"\\nğŸ¤– Regular AI (without document):\")\n",
    "regular_answer = ai_assistant.invoke(comparison_question)\n",
    "print(regular_answer.content)\n",
    "\n",
    "# Get answer WITH RAG (using document)\n",
    "print(\"\\nğŸ” RAG AI (with document context):\")\n",
    "rag_answer = rag_chain.invoke(comparison_question)\n",
    "print(rag_answer)\n",
    "\n",
    "print(\"\\nğŸ’¡ NOTICE THE DIFFERENCE:\")\n",
    "print(\"   â€¢ Regular AI: Might guess or say it doesn't know\")\n",
    "print(\"   â€¢ RAG AI: Uses actual content from the document\")\n",
    "print(\"   â€¢ RAG is more accurate and specific!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Step 9: Interactive RAG Session\n",
    "\n",
    "Now it's your turn to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ YOUR TURN: Ask the RAG system anything!\n",
    "\n",
    "your_question = \"What are the key findings or results mentioned in this paper?\"  # ğŸ‘ˆ Change this!\n",
    "\n",
    "print(f\"â“ Your Question: {your_question}\")\n",
    "print(\"\\nğŸ” RAG is searching and thinking...\")\n",
    "\n",
    "your_answer = rag_chain.invoke(your_question)\n",
    "\n",
    "print(\"\\nğŸ¤– RAG Answer:\")\n",
    "print(your_answer)\n",
    "\n",
    "print(\"\\nğŸ‰ Great! You're now using RAG to get intelligent answers from documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 10: Understanding RAG Parameters\n",
    "\n",
    "Let's learn how to tune RAG for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different retrieval settings\n",
    "print(\"ğŸ”§ TUNING RAG: Different Settings\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "question = \"What methods were used?\"\n",
    "\n",
    "# Try retrieving more chunks\n",
    "more_chunks_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5}  # Get 5 chunks instead of 3\n",
    ")\n",
    "\n",
    "# Build new chain with more context\n",
    "more_context_chain = (\n",
    "    {\n",
    "        \"context\": more_chunks_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | ai_assistant\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(f\"â“ Question: {question}\")\n",
    "\n",
    "print(\"\\nğŸ” RAG with 3 chunks:\")\n",
    "answer_3 = rag_chain.invoke(question)\n",
    "print(answer_3)\n",
    "\n",
    "print(\"\\nğŸ“š RAG with 5 chunks (more context):\")\n",
    "answer_5 = more_context_chain.invoke(question)\n",
    "print(answer_5)\n",
    "\n",
    "print(\"\\nğŸ’¡ More chunks = more context, but also more processing time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ What You've Learned\n",
    "\n",
    "**Fantastic work!** You've built a complete RAG system and understand how it works.\n",
    "\n",
    "### âœ… **Key Concepts:**\n",
    "- **RAG**: Combines document retrieval with AI generation\n",
    "- **Embeddings**: How AI converts text to searchable numbers\n",
    "- **Vector Database**: Stores and searches document embeddings\n",
    "- **Retrieval**: Finding relevant document chunks\n",
    "- **Context**: Providing AI with relevant information\n",
    "\n",
    "### âœ… **Skills You've Gained:**\n",
    "- Creating embeddings from text\n",
    "- Building vector databases for document search\n",
    "- Implementing similarity-based retrieval\n",
    "- Combining retrieval with AI generation\n",
    "- Tuning RAG parameters for better results\n",
    "\n",
    "### ğŸš€ **What's Next:**\n",
    "In **Tutorial 4**, you'll learn about **Knowledge Graphs**:\n",
    "- What knowledge graphs are and how they work\n",
    "- Extracting entities (people, places, concepts) from text\n",
    "- Finding relationships between entities\n",
    "- Visualizing knowledge networks\n",
    "- Combining knowledge graphs with RAG\n",
    "\n",
    "### ğŸ¯ **Practice Ideas:**\n",
    "- Try RAG with your own documents\n",
    "- Experiment with different chunk sizes\n",
    "- Test various numbers of retrieved chunks\n",
    "- Compare answers with and without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ† Final Challenge\n",
    "\n",
    "Build your own specialized RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ† CHALLENGE: Build a Summary RAG System\n",
    "print(\"ğŸ† FINAL CHALLENGE: Specialized RAG for Summaries\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create a special prompt for generating summaries\n",
    "summary_rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert at creating clear, concise summaries of research papers.\n",
    "\n",
    "Based on this context from a research paper:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear summary that:\n",
    "1. Highlights the main points\n",
    "2. Uses simple language\n",
    "3. Is 2-3 sentences maximum\n",
    "\"\"\")\n",
    "\n",
    "# Build the summary RAG chain\n",
    "summary_rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | summary_rag_prompt\n",
    "    | ai_assistant\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test it\n",
    "question = \"Give me a brief summary of what this paper is about\"\n",
    "summary = summary_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"â“ Question: {question}\")\n",
    "print(\"\\nğŸ“ Summary RAG Answer:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nğŸ‰ Challenge Complete! You've built a specialized RAG system!\")\n",
    "print(\"ğŸš€ Ready for Tutorial 4: Building Knowledge Graphs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}