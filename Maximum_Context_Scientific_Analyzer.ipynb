{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Context Scientific Paper Analyzer\n",
    "\n",
    "**Strategy**: Preserve maximum context while staying within Ollama limits\n",
    "\n",
    "**Approach**: Extract key sections ‚Üí Detailed summarization ‚Üí Single comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from citation_extractor import display_citation_info, get_acs_citation\n",
    "import textwrap\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# Configuration\n",
    "pdf_path = \"/Users/aimiegarces/Agents/d4sc03921a.pdf\"\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Paper Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display citation information\n",
    "citation_result = display_citation_info(pdf_path, show_metadata=True, show_all_formats=False)\n",
    "acs_citation = get_acs_citation(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Ollama for maximum context and detailed responses\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1,      # Low temp for analytical consistency\n",
    "    num_ctx=32768,        # Maximum context window\n",
    "    num_predict=4096      # Longer responses for detailed analysis\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Ollama configured for maximum context (32K tokens)\")\n",
    "print(\"üìä Response length: up to 4K tokens for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Paper Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_sections(pdf_path):\n",
    "    \"\"\"Extract the most information-dense sections for maximum context preservation\"\"\"\n",
    "    \n",
    "    # Load full paper\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    print(f\"üìÑ Original paper: {len(full_text):,} characters\")\n",
    "    \n",
    "    # Extract abstract\n",
    "    abstract_patterns = [\n",
    "        r'Abstract\\s*[:\\-]?\\s*\\n(.*?)(?=\\n\\s*\\n|\\nIntroduction|\\n1\\s+Introduction|\\nKeywords|\\n\\d+\\.)',\n",
    "        r'ABSTRACT\\s*[:\\-]?\\s*\\n(.*?)(?=\\n\\s*\\n|\\nINTRODUCTION|\\n1\\s+INTRODUCTION|\\nKEYWORDS|\\n\\d+\\.)',\n",
    "    ]\n",
    "    \n",
    "    abstract = \"\"\n",
    "    for pattern in abstract_patterns:\n",
    "        match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            abstract = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # If no abstract header, extract from beginning\n",
    "    if not abstract or len(abstract) < 100:\n",
    "        intro_pattern = r'(.*?)(?=\\n\\s*1\\s+Introduction|\\n\\s*Introduction)'\n",
    "        match = re.search(intro_pattern, full_text, re.DOTALL)\n",
    "        if match:\n",
    "            pre_intro = match.group(1)\n",
    "            lines = pre_intro.split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if len(line.strip()) > 50 and any(word in line.lower() for word in ['large language', 'models', 'emerged', 'review']):\n",
    "                    abstract_lines = []\n",
    "                    for line in lines[i:]:\n",
    "                        if line.strip() and not line.startswith('1 ') and 'Introduction' not in line:\n",
    "                            abstract_lines.append(line.strip())\n",
    "                        elif len(abstract_lines) > 0:\n",
    "                            break\n",
    "                    abstract = ' '.join(abstract_lines)\n",
    "                    break\n",
    "    \n",
    "    # Extract major sections\n",
    "    def extract_section(section_name, end_markers=None):\n",
    "        if end_markers is None:\n",
    "            end_markers = [r'\\n\\d+\\s+[A-Z]', r'\\n[A-Z][a-z]+\\s*\\n', r'\\nReferences', r'\\nConclusion']\n",
    "        \n",
    "        # Create patterns using string concatenation to avoid f-string issues\n",
    "        end_pattern = \"|\".join(end_markers)\n",
    "        pattern1 = r'\\n\\d+\\s+' + section_name + r'\\s*\\n(.*?)(?=' + end_pattern + ')'\n",
    "        pattern2 = r'\\n' + section_name + r'\\s*\\n(.*?)(?=' + end_pattern + ')'\n",
    "        pattern3 = section_name + r'\\s*[:\\-]?\\s*\\n(.*?)(?=' + end_pattern + ')'\n",
    "        patterns = [pattern1, pattern2, pattern3]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                content = match.group(1).strip()\n",
    "                if len(content) > 200:  # Meaningful content\n",
    "                    return content\n",
    "        return \"\"\n",
    "    \n",
    "    # Extract key sections\n",
    "    sections = {\n",
    "        'abstract': abstract,\n",
    "        'introduction': extract_section(\"Introduction\"),\n",
    "        'methods': extract_section(\"Methods\") or extract_section(\"Methodology\") or extract_section(\"Approach\"),\n",
    "        'results': extract_section(\"Results\") or extract_section(\"Findings\"),\n",
    "        'discussion': extract_section(\"Discussion\") or extract_section(\"Analysis\"),\n",
    "        'conclusion': extract_section(\"Conclusion\") or extract_section(\"Conclusions\")\n",
    "    }\n",
    "    \n",
    "    # Report extraction results\n",
    "    print(\"\\nüìä SECTION EXTRACTION RESULTS:\")\n",
    "    total_extracted = 0\n",
    "    for name, content in sections.items():\n",
    "        length = len(content)\n",
    "        total_extracted += length\n",
    "        status = \"‚úÖ\" if length > 100 else \"‚ö†Ô∏è\" if length > 0 else \"‚ùå\"\n",
    "        print(f\"{status} {name.capitalize()}: {length:,} characters\")\n",
    "    \n",
    "    print(f\"\\nüìà Total extracted: {total_extracted:,} characters ({total_extracted/len(full_text)*100:.1f}% of original)\")\n",
    "    print(f\"üß† Estimated tokens: {total_extracted//4:,} (target: <30K for Ollama)\")\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Extract sections from paper\n",
    "paper_sections = extract_key_sections(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Context-Preserving Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed summaries that preserve maximum context\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Create a detailed, comprehensive summary of this research paper section. \n",
    "PRESERVE ALL IMPORTANT INFORMATION including:\n",
    "- Technical details and specifications\n",
    "- Numerical results and performance metrics\n",
    "- Methodology and experimental setup\n",
    "- Key findings and insights\n",
    "- Comparisons with other approaches\n",
    "- Limitations and challenges mentioned\n",
    "\n",
    "Maintain technical accuracy while condensing length. This summary will be used for downstream scientific analysis.\n",
    "\n",
    "SECTION: {section_name}\n",
    "\n",
    "CONTENT:\n",
    "{content}\n",
    "\n",
    "DETAILED SUMMARY:\n",
    "\"\"\")\n",
    "\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Process each section that has substantial content\n",
    "section_summaries = {}\n",
    "print(\"üîÑ Creating context-preserving summaries...\\n\")\n",
    "\n",
    "for section_name, content in paper_sections.items():\n",
    "    if len(content) > 500:  # Only summarize substantial sections\n",
    "        print(f\"üìù Summarizing {section_name}... ({len(content):,} chars)\")\n",
    "        try:\n",
    "            summary = summary_chain.invoke({\n",
    "                \"section_name\": section_name.upper(),\n",
    "                \"content\": content\n",
    "            })\n",
    "            section_summaries[section_name] = summary\n",
    "            print(f\"‚úÖ {section_name} summarized to {len(summary):,} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error summarizing {section_name}: {str(e)}\")\n",
    "            section_summaries[section_name] = content[:2000] + \"...[truncated]\"  # Fallback\n",
    "    \n",
    "    elif len(content) > 100:  # Keep shorter sections as-is\n",
    "        section_summaries[section_name] = content\n",
    "        print(f\"üìã Keeping {section_name} as-is ({len(content):,} chars)\")\n",
    "\n",
    "# Calculate final context size\n",
    "total_summary_length = sum(len(summary) for summary in section_summaries.values())\n",
    "estimated_tokens = total_summary_length // 4\n",
    "\n",
    "print(f\"\\nüéØ CONTEXT PREPARATION COMPLETE\")\n",
    "print(f\"üìä Total summarized content: {total_summary_length:,} characters\")\n",
    "print(f\"üß† Estimated tokens: {estimated_tokens:,}\")\n",
    "print(f\"‚úÖ Fits in Ollama context: {estimated_tokens < 30000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Analysis Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis prompt with detailed requirements\n",
    "comprehensive_analysis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert in AI applications for scientific research. Analyze this complete research paper using the provided comprehensive content. Generate a HIGHLY DETAILED and thorough analysis focused on scientific applications and R&D implications.\n",
    "\n",
    "INSTRUCTION: Provide comprehensive, detailed responses for each section. Include specific examples, quantitative details, technical specifications, and actionable insights.\n",
    "\n",
    "COMPLETE PAPER CONTENT:\n",
    "\n",
    "ABSTRACT:\n",
    "{abstract}\n",
    "\n",
    "INTRODUCTION:\n",
    "{introduction}\n",
    "\n",
    "METHODS/APPROACH:\n",
    "{methods}\n",
    "\n",
    "RESULTS/FINDINGS:\n",
    "{results}\n",
    "\n",
    "DISCUSSION/ANALYSIS:\n",
    "{discussion}\n",
    "\n",
    "CONCLUSION:\n",
    "{conclusion}\n",
    "\n",
    "Using this complete context, provide a comprehensive analysis with the following structure:\n",
    "\n",
    "**Executive Summary** (4-5 detailed sentences)\n",
    "- Core LLM capability/advancement and its specific scientific impact\n",
    "- Primary scientific domain(s) addressed with specific applications\n",
    "- Key quantitative improvements or breakthroughs demonstrated\n",
    "\n",
    "**Technical Architecture & Training**\n",
    "- Model architecture details (transformer variants, layer count, attention mechanisms)\n",
    "- Model sizes, parameter counts, and computational requirements\n",
    "- Training datasets used (PubMed, arXiv, domain-specific corpora)\n",
    "- Fine-tuning approaches, RLHF implementation, or domain adaptation methods\n",
    "- Benchmark performance on scientific tasks with specific metrics\n",
    "\n",
    "**Scientific Applications Demonstrated**\n",
    "- Use cases: literature review, hypothesis generation, experimental design, data analysis\n",
    "- Performance metrics on scientific reasoning and predictions\n",
    "- Comparison with domain-specific tools (ChemBERTa, BioGPT, etc.)\n",
    "\n",
    "**Research Acceleration Potential**\n",
    "- Time savings demonstrated for research workflows\n",
    "- Novel scientific insights generated by the model\n",
    "- Integration capabilities with existing scientific software/databases\n",
    "- Productivity improvements measured\n",
    "\n",
    "**Implementation & Deployment Considerations**\n",
    "- Computational requirements (GPU/CPU needs, memory, storage)\n",
    "- Scalability considerations and deployment architectures\n",
    "- API availability, pricing models, on-premise deployment options\n",
    "- Data privacy considerations for proprietary research\n",
    "\n",
    "**Strategic R&D Implications**\n",
    "- Impact on research productivity and methodology changes\n",
    "- Skills/training implications for research teams\n",
    "- Competitive advantages for R&D organizations\n",
    "- Regulatory or validation challenges in scientific contexts\n",
    "\n",
    "**Future Research Directions**\n",
    "- Limitations in current scientific reasoning capabilities\n",
    "- Multimodal capabilities needed (chemical structures, spectra, etc.)\n",
    "- Opportunities for domain-specific fine-tuning\n",
    "- Emerging research areas where these technologies could be applied\n",
    "\n",
    "Focus on actionable insights for implementing LLMs in industrial R&D environments.\n",
    "\n",
    "COMPREHENSIVE SCIENTIFIC ANALYSIS:\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìã Analysis prompt created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the final comprehensive analysis with citation\n",
    "analysis_chain = comprehensive_analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"üî¨ Generating comprehensive scientific analysis with maximum context...\\n\")\n",
    "\n",
    "try:\n",
    "    # Invoke analysis with all preserved context\n",
    "    final_analysis = analysis_chain.invoke({\n",
    "        \"abstract\": section_summaries.get('abstract', 'Not available'),\n",
    "        \"introduction\": section_summaries.get('introduction', 'Not available'),\n",
    "        \"methods\": section_summaries.get('methods', 'Not available'),\n",
    "        \"results\": section_summaries.get('results', 'Not available'),\n",
    "        \"discussion\": section_summaries.get('discussion', 'Not available'),\n",
    "        \"conclusion\": section_summaries.get('conclusion', 'Not available')\n",
    "    })\n",
    "    \n",
    "    # Display the comprehensive analysis with citation header\n",
    "    print(\"üî¨ COMPREHENSIVE SCIENTIFIC ANALYSIS REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Citation header\n",
    "    print(\"\\nüìñ PAPER CITATION (ACS Style):\")\n",
    "    print(\"-\" * 50)\n",
    "    wrapped_citation = textwrap.fill(acs_citation, width=75, initial_indent=\"\", subsequent_indent=\"\")\n",
    "    print(wrapped_citation)\n",
    "    \n",
    "    # Analysis metadata\n",
    "    print(f\"\\nüìä ANALYSIS METADATA:\")\n",
    "    print(f\"üß† Context used: {total_summary_length:,} characters (~{estimated_tokens:,} tokens)\")\n",
    "    print(f\"üìÑ Sections analyzed: {len([s for s in section_summaries.values() if s])}\")\n",
    "    print(f\"‚è±Ô∏è  Analysis date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Format and display analysis\n",
    "    print(final_analysis)\n",
    "    print()\n",
    "    print(\"‚úÖ COMPREHENSIVE ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Quick copy citation\n",
    "    print(f\"\\nüìã QUICK COPY - ACS CITATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(acs_citation)\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating analysis: {str(e)}\")\n",
    "    print(f\"üìä Context size might be too large: {estimated_tokens:,} tokens\")\n",
    "    print(\"üí° Try reducing section content or increasing model context window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess how much context we preserved\n",
    "print(\"üìä CONTEXT PRESERVATION ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sections_found = sum(1 for content in section_summaries.values() if len(content) > 100)\n",
    "total_sections = len(section_summaries)\n",
    "\n",
    "print(f\"üìö Sections successfully extracted: {sections_found}/{total_sections}\")\n",
    "print(f\"üìÑ Total content for analysis: {total_summary_length:,} characters\")\n",
    "print(f\"üß† Token efficiency: {estimated_tokens:,}/32K tokens ({estimated_tokens/32000*100:.1f}% of max)\")\n",
    "\n",
    "print(\"\\nüìã Section breakdown:\")\n",
    "for name, content in section_summaries.items():\n",
    "    if content:\n",
    "        print(f\"  ‚Ä¢ {name.capitalize()}: {len(content):,} chars\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ {name.capitalize()}: ‚ùå Not found\")\n",
    "\n",
    "print(f\"\\nüí° Context quality: {'Excellent' if sections_found >= 4 else 'Good' if sections_found >= 3 else 'Limited'}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}