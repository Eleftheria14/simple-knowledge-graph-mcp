{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Research Paper Analyzer\n",
    "\n",
    "**Goal**: Extract full research papers and generate comprehensive scientific analysis\n",
    "\n",
    "**Output**: Structured analysis focused on R&D applications and scientific impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Complete Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_paper(pdf_path, chunk_size=4000, chunk_overlap=500):\n",
    "    \"\"\"Load and chunk the entire research paper for comprehensive analysis\"\"\"\n",
    "    # Load PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Get full text\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Split into manageable chunks for analysis\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    \n",
    "    return full_text, chunks\n",
    "\n",
    "# Load your research paper\n",
    "pdf_path = \"/Users/aimiegarces/Agents/d4sc03921a.pdf\"\n",
    "full_text, text_chunks = load_full_paper(pdf_path)\n",
    "\n",
    "print(f\"üìÑ Paper loaded successfully!\")\n",
    "print(f\"üìä Total length: {len(full_text):,} characters\")\n",
    "print(f\"üìö Split into: {len(text_chunks)} chunks\")\n",
    "print(f\"üìù Average chunk size: {len(full_text)//len(text_chunks):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Ollama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama with settings optimized for analysis\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.1,  # Low temperature for analytical consistency\n",
    "    num_ctx=4096      # Large context window for comprehensive analysis\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Ollama model configured for scientific analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scientific Analysis Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive scientific analysis prompt\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert in AI applications for scientific research. Analyze this research paper section and extract information relevant to the following analysis structure. Focus on scientific applications and R&D implications.\n",
    "\n",
    "PAPER SECTION:\n",
    "{text_chunk}\n",
    "\n",
    "ANALYSIS FRAMEWORK:\n",
    "**Executive Summary** (2-3 sentences)\n",
    "- Core LLM capability/advancement and its scientific impact\n",
    "- Primary scientific domain(s) addressed\n",
    "\n",
    "**Technical Architecture & Training**\n",
    "- Model architecture, size, and training specifics\n",
    "- Scientific datasets used (PubMed, arXiv, domain-specific corpora)\n",
    "- Fine-tuning approaches, RLHF, or domain adaptation methods\n",
    "- Benchmark performance on scientific tasks\n",
    "\n",
    "**Scientific Applications Demonstrated**\n",
    "- Specific use cases: literature review, hypothesis generation, experimental design, data analysis\n",
    "- Performance on scientific reasoning, chemical/biological predictions, or research workflows\n",
    "- Comparison with domain-specific tools (ChemBERTa, BioGPT, etc.)\n",
    "\n",
    "**Experimental Validation**\n",
    "- How scientific accuracy was evaluated\n",
    "- Expert validation studies or blind comparisons\n",
    "- Error analysis and failure modes in scientific contexts\n",
    "\n",
    "**Research Acceleration Potential**\n",
    "- Time savings demonstrated for research workflows\n",
    "- Novel scientific insights generated by the model\n",
    "- Integration capabilities with existing scientific software/databases\n",
    "\n",
    "**Implementation & Deployment Considerations**\n",
    "- Computational requirements and scalability\n",
    "- API availability, on-premise deployment options\n",
    "- Integration with lab automation, LIMS, or research platforms\n",
    "- Data privacy considerations for proprietary research\n",
    "\n",
    "**Strategic R&D Implications**\n",
    "- Impact on research productivity and methodology\n",
    "- Skills/training implications for research teams\n",
    "- Competitive advantages for R&D organizations\n",
    "- Regulatory or validation challenges in scientific contexts\n",
    "\n",
    "**Future Research Directions**\n",
    "- Identified limitations in current scientific reasoning\n",
    "- Multimodal capabilities needed (chemical structures, spectra, etc.)\n",
    "- Opportunities for domain-specific fine-tuning\n",
    "\n",
    "Extract and organize relevant information from this section. If a section doesn't contain information for a particular category, write \"Not covered in this section\" for that category. Focus on actionable insights for implementing LLMs in industrial R&D environments.\n",
    "\n",
    "ANALYSIS:\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìã Scientific analysis prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Paper in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis chain\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Process chunks and collect analyses\n",
    "chunk_analyses = []\n",
    "print(\"üîÑ Processing paper chunks for comprehensive analysis...\\n\")\n",
    "\n",
    "for i, chunk in enumerate(text_chunks[:3]):  # Process first 3 chunks for demo\n",
    "    print(f\"üìñ Analyzing chunk {i+1}/{min(3, len(text_chunks))}...\")\n",
    "    \n",
    "    try:\n",
    "        analysis = analysis_chain.invoke({\"text_chunk\": chunk})\n",
    "        chunk_analyses.append({\n",
    "            \"chunk_id\": i+1,\n",
    "            \"analysis\": analysis,\n",
    "            \"chunk_preview\": chunk[:200] + \"...\"\n",
    "        })\n",
    "        print(f\"‚úÖ Chunk {i+1} analyzed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing chunk {i+1}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéØ Completed analysis of {len(chunk_analyses)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthesis Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis prompt to combine all chunk analyses\n",
    "synthesis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert in AI applications for scientific research. Synthesize the following chunk analyses into a comprehensive, coherent report about this LLM research paper. Focus on scientific applications and R&D implications.\n",
    "\n",
    "CHUNK ANALYSES:\n",
    "{combined_analyses}\n",
    "\n",
    "Create a final comprehensive report using this structure:\n",
    "\n",
    "**Executive Summary** (2-3 sentences)\n",
    "**Technical Architecture & Training**\n",
    "**Scientific Applications Demonstrated**\n",
    "**Experimental Validation**\n",
    "**Research Acceleration Potential**\n",
    "**Implementation & Deployment Considerations**\n",
    "**Strategic R&D Implications**\n",
    "**Future Research Directions**\n",
    "\n",
    "Synthesize information across all chunks, eliminating redundancy and creating a coherent narrative. Prioritize actionable insights for implementing LLMs in industrial R&D environments. Assess both opportunities and risks for scientific research acceleration.\n",
    "\n",
    "COMPREHENSIVE SCIENTIFIC ANALYSIS:\n",
    "\"\"\")\n",
    "\n",
    "synthesis_chain = synthesis_prompt | llm | StrOutputParser()\n",
    "print(\"üîó Synthesis prompt created for final report generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Final Scientific Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunk_analyses:\n",
    "    # Combine all chunk analyses\n",
    "    combined_analyses = \"\\n\\n\".join([\n",
    "        f\"=== CHUNK {item['chunk_id']} ANALYSIS ===\\n{item['analysis']}\"\n",
    "        for item in chunk_analyses\n",
    "    ])\n",
    "    \n",
    "    print(\"üîÑ Synthesizing comprehensive scientific analysis...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Generate final synthesis\n",
    "        final_analysis = synthesis_chain.invoke({\"combined_analyses\": combined_analyses})\n",
    "        \n",
    "        # Format and display the final report\n",
    "        print(\"üî¨ COMPREHENSIVE SCIENTIFIC RESEARCH ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìÑ Paper: LLMs and Autonomous Agents in Chemistry\")\n",
    "        print(f\"üìä Analysis based on {len(chunk_analyses)} text chunks\")\n",
    "        print(f\"üìà Total content analyzed: {sum(len(item['analysis']) for item in chunk_analyses):,} characters\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Display formatted analysis\n",
    "        formatted_analysis = textwrap.fill(final_analysis, width=75)\n",
    "        print(formatted_analysis)\n",
    "        print()\n",
    "        print(\"‚úÖ SCIENTIFIC ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating final synthesis: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No chunk analyses available for synthesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: View Individual Chunk Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display individual chunk analyses for detailed review\n",
    "print(\"üìö INDIVIDUAL CHUNK ANALYSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for item in chunk_analyses:\n",
    "    print(f\"\\nüìñ CHUNK {item['chunk_id']} ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Content preview: {item['chunk_preview']}\")\n",
    "    print(\"\\nAnalysis:\")\n",
    "    formatted_chunk_analysis = textwrap.fill(item['analysis'], width=70)\n",
    "    print(formatted_chunk_analysis)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Process Different Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze any scientific paper with the same framework\n",
    "def analyze_scientific_paper(pdf_path, max_chunks=5):\n",
    "    \"\"\"Complete scientific analysis pipeline for any research paper\"\"\"\n",
    "    print(f\"üî¨ Starting scientific analysis of: {pdf_path.split('/')[-1]}\")\n",
    "    \n",
    "    # Load and process\n",
    "    full_text, chunks = load_full_paper(pdf_path)\n",
    "    \n",
    "    # Analyze chunks\n",
    "    analyses = []\n",
    "    for i, chunk in enumerate(chunks[:max_chunks]):\n",
    "        try:\n",
    "            analysis = analysis_chain.invoke({\"text_chunk\": chunk})\n",
    "            analyses.append({\"chunk_id\": i+1, \"analysis\": analysis})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped chunk {i+1}: {str(e)}\")\n",
    "    \n",
    "    # Synthesize\n",
    "    if analyses:\n",
    "        combined = \"\\n\\n\".join([f\"=== CHUNK {item['chunk_id']} ===\\n{item['analysis']}\" for item in analyses])\n",
    "        final_report = synthesis_chain.invoke({\"combined_analyses\": combined})\n",
    "        return final_report\n",
    "    \n",
    "    return \"Analysis failed - no chunks processed successfully\"\n",
    "\n",
    "# Example usage:\n",
    "# new_analysis = analyze_scientific_paper(\"/path/to/another/paper.pdf\")\n",
    "# print(new_analysis)\n",
    "\n",
    "print(\"üõ†Ô∏è Scientific paper analysis function ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}