{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Paper Summarizer\n",
    "\n",
    "**Goal**: Extract abstracts from PDFs and summarize them with local AI\n",
    "\n",
    "**Flow**: PDF ‚Üí Extract Abstract ‚Üí Summarize ‚Üí Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import re\n",
    "\n",
    "print (\"Imported Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF and Extract Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract extracted: 1269 characters\n",
      "First 200 chars: Large language models (LLMs) have emerged as powerful tools in chemistry, signiÔ¨Åcantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabiliti...\n"
     ]
    }
   ],
   "source": [
    "def extract_abstract(pdf_path):\n",
    "    # Load PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # First try to find explicit \"Abstract\" sections\n",
    "    abstract_patterns = [\n",
    "        r'Abstract\\s*[:\\-]?\\s*\\n(.*?)(?=\\n\\s*\\n|\\nIntroduction|\\n1\\s+Introduction|\\nKeywords|\\n\\d+\\.)',\n",
    "        r'ABSTRACT\\s*[:\\-]?\\s*\\n(.*?)(?=\\n\\s*\\n|\\nINTRODUCTION|\\n1\\s+INTRODUCTION|\\nKEYWORDS|\\n\\d+\\.)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in abstract_patterns:\n",
    "        match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            abstract = match.group(1).strip()\n",
    "            if len(abstract) > 100:\n",
    "                return abstract\n",
    "    \n",
    "    # If no explicit \"Abstract\" header found, extract text after authors but before \"Introduction\"\n",
    "    # This PDF has format: Title + Authors + Abstract (no header) + Introduction\n",
    "    intro_pattern = r'(.*?)(?=\\n\\s*1\\s+Introduction|\\n\\s*Introduction)'\n",
    "    match = re.search(intro_pattern, full_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        pre_intro_text = match.group(1)\n",
    "        \n",
    "        # Remove title and author info - get the paragraph that's clearly the abstract\n",
    "        # Look for the substantial paragraph after the author line\n",
    "        lines = pre_intro_text.split('\\n')\n",
    "        \n",
    "        # Find where the abstract likely starts (after author names)\n",
    "        abstract_start = -1\n",
    "        for i, line in enumerate(lines):\n",
    "            # Look for a line that seems like the start of abstract content\n",
    "            if len(line.strip()) > 50 and any(word in line.lower() for word in ['large language', 'models', 'emerged', 'review']):\n",
    "                abstract_start = i\n",
    "                break\n",
    "        \n",
    "        if abstract_start >= 0:\n",
    "            # Take from abstract start until we hit a clear section break\n",
    "            abstract_lines = []\n",
    "            for line in lines[abstract_start:]:\n",
    "                if line.strip() and not line.startswith('1 ') and 'Introduction' not in line:\n",
    "                    abstract_lines.append(line.strip())\n",
    "                elif len(abstract_lines) > 0 and (line.strip() == '' or 'Introduction' in line):\n",
    "                    break\n",
    "            \n",
    "            if abstract_lines:\n",
    "                return ' '.join(abstract_lines)\n",
    "    \n",
    "    # Final fallback\n",
    "    return \" \".join(full_text.split()[:500])\n",
    "\n",
    "# Load your paper\n",
    "pdf_path = \"/Users/aimiegarces/Agents/d4sc03921a.pdf\"\n",
    "abstract = extract_abstract(pdf_path)\n",
    "\n",
    "print(f\"Abstract extracted: {len(abstract)} characters\")\n",
    "print(f\"First 200 chars: {abstract[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Local AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama model\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Summarize this research abstract in 2-3 clear sentences:\n",
    "\n",
    "{abstract}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "# Build the chain: prompt ‚Üí model ‚Üí parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Chain created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run the chain\nsummary = chain.invoke({\"abstract\": abstract})\n\n# Clean and professional output formatting\nprint(\"üìÑ RESEARCH PAPER ANALYSIS\")\nprint(\"=\" * 80)\n\n# Show paper info if available\nprint(f\"üìä Abstract Length: {len(abstract)} characters\")\nprint(f\"üìù Summary Length: {len(summary)} characters\")\nprint()\n\nprint(\"üîç ORIGINAL ABSTRACT\")\nprint(\"-\" * 50)\n# Format abstract nicely with line breaks\nimport textwrap\nformatted_abstract = textwrap.fill(abstract, width=75)\nprint(formatted_abstract)\nprint()\n\nprint(\"ü§ñ AI SUMMARY\")\nprint(\"-\" * 50)\nformatted_summary = textwrap.fill(summary, width=75)\nprint(formatted_summary)\nprint()\n\nprint(\"‚úÖ Analysis Complete!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST 1500 CHARACTERS OF YOUR PDF ===\n",
      "'A review of large language models and\\nautonomous agents in chemistry\\nMayk Caldas Ramos, ab Christopher J. Collison c and Andrew D. White *ab\\nLarge language models (LLMs) have emerged as powerful tools in chemistry, signiÔ¨Åcantly impacting\\nmolecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities\\nin these domains and their potential to accelerate scientiÔ¨Åc discovery through automation. We also\\nreview LLM-based autonomous agents: LLMs with a broader set of tools to interact with their\\nsurrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with\\nautomated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope\\nof our review of agents beyond chemistry and discuss across any scientiÔ¨Åc domains. This review covers\\nthe recent history, current capabilities, and design of LLMs and autonomous agents, addressing speciÔ¨Åc\\nchallenges, opportunities, and future directions in chemistry. Key challenges include data quality and\\nintegration, model interpretability, and the need for standard benchmarks, while future directions point\\ntowards more sophisticated multi-modal agents and enhanced collaboration between agents and\\nexperimental methods. Due to the quick pace of thisÔ¨Åeld, a repository has been built to keep track of\\nthe latest studies:https://github.com/ur-whitelab/LLMs-in-science.\\n1 Introduction\\nThe integration of Machine Learning (ML) and Arti\\ue103cial Intelli-\\ngence (AI)'\n",
      "=== END ===\n",
      "\n",
      "Found 17 instances of 'abstract':\n",
      "\n",
      "--- Instance 1 at position 44543 ---\n",
      "'22.09\\nMaterialsBERT204 110M 2.4M material science\\nabstracts + 750 annotated\\nabstract for NER\\nBERT NER and property\\nextraction\\n2022.09 (2023.04)\\nSolvBERT\\n205 b 1M SMILES of solute‚Äìsolvent\\npairs from CombiSolv-QM and\\nLogS from Boobieret al.\\n206\\nBERT Property p'\n",
      "\n",
      "--- Instance 2 at position 44569 ---\n",
      "'M 2.4M material science\\nabstracts + 750 annotated\\nabstract for NER\\nBERT NER and property\\nextraction\\n2022.09 (2023.04)\\nSolvBERT\\n205 b 1M SMILES of solute‚Äìsolvent\\npairs from CombiSolv-QM and\\nLogS from Boobieret al.\\n206\\nBERT Property prediction 2022.07 (2023.01'\n",
      "\n",
      "--- Instance 3 at position 44993 ---\n",
      "'T Document classi \\ue103cation 2022.05\\nMatBERT209 110M Abstracts from solid state\\narticles and abstracts and\\nmethods from gold nanoparticle\\narticles\\nBERT NER 2022.04\\nMatSciBERT\\n210 110M ‚àº150k material science paper\\ndownloaded from Elsevier\\nBERT NER and text\\nclass'\n",
      "\n",
      "--- Instance 4 at position 45033 ---\n",
      "'T209 110M Abstracts from solid state\\narticles and abstracts and\\nmethods from gold nanoparticle\\narticles\\nBERT NER 2022.04\\nMatSciBERT\\n210 110M ‚àº150k material science paper\\ndownloaded from Elsevier\\nBERT NER and text\\nclassi\\ue103cation\\n2021.09 (2022.05)\\nMol-BERT118 1'\n",
      "\n",
      "--- Instance 5 at position 45731 ---\n",
      "'ron-LM NER and QA 2020-10\\nPubMedBERT\\n215 110M 14M abstracts from PubMed BERT NER, QA, and document\\nclassi\\ue103cation\\n2020.07 (2021.10)\\nMolecule attention\\ntransformer216\\nb ZINC15 Encoder with\\nGCN features\\nProperty prediction 2020.02\\nSMILES-BERT217 b ‚àº18M SMILES f'\n",
      "\n",
      "--- Instance 6 at position 62033 ---\n",
      "'nd\\nretrosynthesis\\n2024.06\\nBioMedLM272 2.7B PubMed abstracts and full articles GPT QA 2024.03\\nLlasMol273 ‚àº7B SMolInstruct Galactica,\\nLLaMa, Mistral\\nProperty prediction,\\nmolecule captioning,\\nmolecule generation,\\nretrosynthesis, name\\nconversion\\n2024.02\\n(2024.08'\n",
      "\n",
      "--- Instance 7 at position 63430 ---\n",
      "'ediction\\n2022.11\\nBioGPT\\n282 355M 15M of title and abstract from\\nPubMed\\nGPT-2 QA, NER, and document\\nclassi\\ue103cation\\n2022-09\\n(2023.04)\\nSMILES-to-\\nproperties-\\ntransformer\\n265\\n6.5M Synthetic data generated with the\\nthermodynamic model COSMO-\\nRS\\nGPT-3 Property pred'\n",
      "\n",
      "--- Instance 8 at position 64173 ---\n",
      "'number of parameters.‚ÄúPubMed‚Äù refer to the PubMed abstracts dataset, while PMC (PubMed Corpus) refers to the\\nfull-text corpus dataset.b The total number of parameters was not reported.\\n¬© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. S'\n",
      "\n",
      "--- Instance 9 at position 93287 ---\n",
      "'ated data for\\nease of access and uniformity, they abstract away from real-world\\nchemical properties. We again emphasize the importance of\\nincorporating experimentally derived data into Chemistry LLM\\nresearch to create more robust and relevant models. Continu'\n",
      "\n",
      "--- Instance 10 at position 110689 ---\n",
      "'t al.\\n385 \\ue103ne-tuned PubMedBERT 215 on 2.4 million\\nabstracts, showing incremental precision improvements in NER\\ntasks. BatteryBERT\\n208 also followed this strategy, outperforming\\nbaseline BERT models in battery-related tasks.\\n2532 | Chem. Sci.,2 0 2 5 ,16,2 5 '\n",
      "\n",
      "--- Instance 11 at position 111696 ---\n",
      "'f either 30k or\\n50k tokens, were pretrained using abstracts from the PubMed\\ndataset and full-text scienti\\ue103c articles from PubMed Central\\n(PMC), similar to BioBERT.\\n221\\nSurprisingly, the largest 1.2B model did not perform better\\nthan the smaller ones, with th'\n",
      "\n",
      "--- Instance 12 at position 112741 ---\n",
      "'ed using data from Wikipedia, text-\\nbooks, PubMed abstracts, and the PMC full-text corpus, out-\\nperformed the original BERT in all tested benchmarks, and in\\nsome cases even achieved state-of-the-art (SOTA) performance\\nin benchmarks such as NCBI disease, 2010'\n",
      "\n",
      "--- Instance 13 at position 113211 ---\n",
      "'benchmark.218 BlueBERT was pre-\\ntrained on PubMed abstracts and MIMIC-III,376 and \\ue103ne-tuned\\non various BLUE tasks, showing performance similar to Bio-\\nBERT across multiple benchmarks.\\nPubMedBERT,\\n215 following the approach adopted in Sci-\\nBERT, created a dom'\n",
      "\n",
      "--- Instance 14 at position 113453 ---\n",
      "'T, created a domain-speci \\ue103c vocabulary using 14M\\nabstracts from PubChem for pretraining. In addition to pre-\\ntraining, the team curated and grouped biomedical datasets to\\ndevelop BLURB, a comprehensive benchmark for biomedical\\nnatural language processing (N'\n",
      "\n",
      "--- Instance 15 at position 117171 ---\n",
      "'2 pretrained a GPT-2 model architecture\\nusing 15M abstracts from PubChem corpus. BioGPT was eval-\\nuated across four tasks and\\ue103ve benchmarks, including end-to-\\nend relation extraction on BC5CDR, KD-DTI, and DDI, question-\\nanswering on PubMedQA, document class'\n",
      "\n",
      "--- Instance 16 at position 326216 ---\n",
      "' Y. Qin, Z. Liu and H. Ji,\\nCREATOR: Disentangling abstract and concrete\\nreasonings of large language models through tool\\ncreation, arXiv, 2023, preprint, arXiv:2305.14318, DOI:\\n10.48550/arXiv.2305.14318, http://arxiv.org/abs/\\n2305.14318.\\n487 L. Yuan, Y. Chen'\n",
      "\n",
      "--- Instance 17 at position 341394 ---\n",
      "'rmer-Based molecular property prediction\\n(student abstract), Proceedings of the AAAI Conference on\\nArti\\ue103cial Intelligence , 2022, 36(11), 12949 ‚Äì12950, DOI:\\n10.1609/aaai.v36i11.21611.\\n544 G. Landrum, Rdkit documentation, Release, 1(1-79):4,\\n2013, pp. 1533 ‚Äì3'\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Show what's actually in the PDF\n",
    "loader = PyPDFLoader(\"/Users/aimiegarces/Agents/d4sc03921a.pdf\")\n",
    "docs = loader.load()\n",
    "full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "print(\"=== FIRST 1500 CHARACTERS OF YOUR PDF ===\")\n",
    "print(repr(full_text[:1500]))  # Using repr to see whitespace/newlines\n",
    "print(\"=== END ===\")\n",
    "\n",
    "# Look for \"Abstract\" anywhere in the text\n",
    "import re\n",
    "abstract_positions = []\n",
    "for match in re.finditer(r'abstract', full_text, re.IGNORECASE):\n",
    "    start = max(0, match.start() - 50)\n",
    "    end = min(len(full_text), match.end() + 200)\n",
    "    abstract_positions.append((match.start(), full_text[start:end]))\n",
    "\n",
    "print(f\"\\nFound {len(abstract_positions)} instances of 'abstract':\")\n",
    "for i, (pos, context) in enumerate(abstract_positions):\n",
    "    print(f\"\\n--- Instance {i+1} at position {pos} ---\")\n",
    "    print(repr(context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}